{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Standard Python Data Science imports\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partly taken from https://github.com/prakashpandey9/Text-Classification-Pytorch\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "        \n",
    "        --------\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.weights = weights\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        self.dropout = 0.8\n",
    "        self.bilstm = nn.LSTM(embedding_length, hidden_size, dropout=self.dropout, bidirectional=True)\n",
    "        # We will use da = 350, r = 30 & penalization_coeff = 1 \n",
    "        # as per given in the self-attention original ICLR paper\n",
    "        self.W_s1 = nn.Linear(2*hidden_size, 350)\n",
    "        self.W_s2 = nn.Linear(350, 30)\n",
    "        self.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n",
    "        self.label = nn.Linear(2000, output_size)\n",
    "\n",
    "    def attention_net(self, lstm_output):\n",
    "\n",
    "        \"\"\"\n",
    "        Now we will use self attention mechanism to produce a matrix \n",
    "        embedding of the input sentence in which every row represents an\n",
    "        encoding of the input sentence but giving an attention to a \n",
    "        specific part of the sentence. We will use 30 such embedding of \n",
    "        the input sentence and then finally we will concatenate all the 30 \n",
    "        sentence embedding vectors and connect it to a fully connected layer \n",
    "        of size 2000 which will be connected to the output layer of size 2 \n",
    "        returning logits for our two classes i.e., pos & neg.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        lstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
    "        ---------\n",
    "        Returns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
    "                  attention to different parts of the input sentence.\n",
    "        Tensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "                      attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
    "        \"\"\"\n",
    "        attn_weight_matrix = self.W_s2(torch.tanh(self.W_s1(lstm_output)))\n",
    "        attn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
    "        attn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
    "\n",
    "        return attn_weight_matrix\n",
    "\n",
    "    def forward(self, input_sentences, batch_size=None, return_attn=False):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. \n",
    "        Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "        return_attn : bool determining whether to return attention layer activation \n",
    "                      default = False\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for pos & neg class.\n",
    "        Attention layer if the return_attn is set\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        if batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "\n",
    "        output, (h_n, c_n) = self.bilstm(input, (h_0, c_0))\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "        # h_n.size() = (1, batch_size, hidden_size)\n",
    "        # c_n.size() = (1, batch_size, hidden_size)\n",
    "        attn_weight_matrix = self.attention_net(output)\n",
    "        # attn_weight_matrix.size() = (batch_size, r, num_seq)\n",
    "        # output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "        hidden_matrix = torch.bmm(attn_weight_matrix, output)\n",
    "        # hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n",
    "        # Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n",
    "        fc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
    "        logits = self.label(fc_out)\n",
    "        # logits.size() = (batch_size, output_size)\n",
    "        if not return_attn:\n",
    "            return logits\n",
    "        else:\n",
    "            return logits, attn_weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(filename, vocab_size=10000, compute_avg=True):\n",
    "    \"\"\"\n",
    "    Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
    "  \n",
    "    Arguments:\n",
    "      - filename:     path to file\n",
    "                      automatically infers correct embedding dimension from filename\n",
    "      - vocab_size:   maximum number of embeddings to load\n",
    "      - compute_avg:  bool to decide whether to comnpute the average word embedding,\n",
    "                      which can be used as a <unk> embedding\n",
    "\n",
    "      Returns \n",
    "      - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
    "      - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # get the embedding size from the first embedding\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
    "\n",
    "    vocab = {}\n",
    "    inv_vocab = {}\n",
    "    \n",
    "    if compute_avg:\n",
    "        # Add extra embedding for <unk> and <pad>\n",
    "        # last index is <unk>, first index is <pad>\n",
    "        embeddings = np.zeros((vocab_size + 2, word_embedding_dim))\n",
    "    else:\n",
    "         # Only add extra embedding for <pad>\n",
    "         # first index is <pad>\n",
    "        embeddings = np.zeros((vocab_size + 1, word_embedding_dim))\n",
    "        \n",
    "\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "\n",
    "            if idx >= vocab_size:\n",
    "                break\n",
    "            \n",
    "            cols = line.rstrip().split(\" \")\n",
    "            val = np.array(cols[1:])\n",
    "            word = cols[0]\n",
    "            embeddings[idx + 1] = val\n",
    "            vocab[word] = idx + 1\n",
    "            inv_vocab[idx + 1] = word\n",
    "    \n",
    "    # Set <unk> embedding to the average of all other embedding vects in vocab\n",
    "    if compute_avg:\n",
    "        embeddings[-1] = embeddings[:-1].mean(axis=0)\n",
    "    \n",
    "    return torch.FloatTensor(embeddings), vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_embedd(sent, model, vocab_dict, tokenizer, max_length, device, unk_embedd=True):\n",
    "    '''\n",
    "    Helper function to extract the embedding for an input sentence.\n",
    "    '''\n",
    "    idxs = [0 for i in range(max_length)]\n",
    "    i = 0\n",
    "    for word in tokenizer(sent):\n",
    "        if i < max_length:\n",
    "            if word in vocab_dict:\n",
    "                idxs[i] = vocab_dict[word]\n",
    "            else:\n",
    "                # If using <unk> embedding, append \n",
    "                # the final index where that embedding is stored\n",
    "                if unk_embedd:\n",
    "                    idxs[i] = len(vocab_dict)\n",
    "            i += 1\n",
    "    return torch.LongTensor([idxs]).to(device)\n",
    "            \n",
    "def sent_pred(sent, model, vocab_dict, tokenizer, max_length, device, batch_size):\n",
    "    '''\n",
    "    Runs the model on an input sentence.\n",
    "    \n",
    "    Arguments: \n",
    "    \n",
    "      sent : str. The input sentence.\n",
    "      model : the pytorch model to be used.\n",
    "      vocab_dict : dict. A dictionary with words as keys and their indices as values\n",
    "     \n",
    "    Returns:\n",
    "      pred : np array. The prediction, wich is a normalized array with a value for \\\n",
    "             each class, representing the predicted probability for that class\n",
    "      attns : the attention matrix\n",
    "    '''\n",
    "    input_tensor = sent_embedd(sent, model, vocab_dict, tokenizer, max_length, device)\n",
    "    input_tensor = torch.cat(batch_size * [input_tensor])\n",
    "    pred, attns = model(input_tensor, return_attn=True)\n",
    "    return pred.detach().cpu().numpy(), attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single(input_tensor_batch, target_tensor_batch, model, \n",
    "          model_optimizer, criterion):\n",
    "    '''\n",
    "    A single forward and backward pass of the neural net on a single training batch.\n",
    "    '''\n",
    "    target_tensor = torch.stack(target_tensor_batch).reshape(len(input_tensor_batch))\n",
    "    input_tensor = torch.stack(input_tensor_batch)\n",
    "    input_tensor = input_tensor.reshape(len(input_tensor_batch), input_tensor.shape[2])\n",
    "    output = model(input_tensor, return_attn=False)\n",
    "    loss = criterion(output, target_tensor)\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train(input_tensors, target_tensors, input_val_tensors, target_val_tensors,\n",
    "          model, model_optimizer, criterion, n_epochs):\n",
    "    '''\n",
    "    Train the attention classfier for a given number of epochs on the whole training set.\n",
    "    '''\n",
    "    losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    # Iterate over given num of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = 0\n",
    "        # Iterate over batches\n",
    "        for i in range(len(input_tensors)):\n",
    "            input_tensor = input_tensors[i]\n",
    "            target_tensor = target_tensors[i]\n",
    "            loss += train_single(input_tensor, target_tensor, model, \n",
    "                                 model_optimizer, criterion)\n",
    "        train_accuracy = get_accuracy(input_tensors, target_tensors, model)\n",
    "        val_accuracy = get_accuracy(input_val_tensors, target_val_tensors, model)\n",
    "        print(f\"Epoch {epoch} :\") \n",
    "        print(f\"\\tLoss {loss/len(input_tensors):.4f}\")\n",
    "        print(f\"\\tTraining Accuracy {train_accuracy:.4f}\")\n",
    "        print(f\"\\tValidation Accuracy {val_accuracy:.4f}\")\n",
    "        losses.append(loss/len(input_tensors))\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "    return losses, train_accs, val_accs\n",
    "\n",
    "def get_accuracy(input_tensors, target_tensors, model):\n",
    "    '''\n",
    "    Get model accuracy.\n",
    "    '''\n",
    "    accs = []\n",
    "    # Iterate over batches\n",
    "    for i in range(len(input_tensors)):\n",
    "        input_tensor_batch = input_tensors[i]\n",
    "        target_tensor_batch = target_tensors[i]\n",
    "        target_tensor = torch.stack(target_tensor_batch).reshape(len(input_tensor_batch))\n",
    "        input_tensor = torch.stack(input_tensor_batch)\n",
    "        input_tensor = input_tensor.reshape(len(input_tensor_batch), input_tensor.shape[2])\n",
    "        output = model(input_tensor, return_attn=False)\n",
    "        # Get classification prediction\n",
    "        preds = output.argmax(axis=1)\n",
    "        # Get accuracy of given batch\n",
    "        batch_acc = ((preds == target_tensor).sum()/target_tensor.shape[0]).item()\n",
    "        accs.append(batch_acc)\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_embedd = True # Bool for whether to create an embedding for the <unk> token\n",
    "# Get initial word vector embedings from disk\n",
    "glove_filename = './GloveEmbeddings/glove.6B.100d.txt' \n",
    "glove_embeddings, vocab_dict, inv_vocab_dict = read_embeddings(glove_filename, \n",
    "                                                               vocab_size=100000,\n",
    "                                                               compute_avg=unk_embedd)\n",
    "# Create tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False # Whether to save model or not\n",
    "load_model = False # Whether to load model or not\n",
    "train_model = True # Whether to train model or not\n",
    "checkpoint_path = \"../models/attn_test.pth\"\n",
    "train_epochs = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmac/Documents/MIMS Coursework/Capstone/env_marvin/lib/python3.8/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "if not load_model:\n",
    "    # Set dimensions and hyperparameters\n",
    "    batch_size = 32\n",
    "    output_size = 2\n",
    "    hidden_size = 100\n",
    "    vocab_size = glove_embeddings.shape[0]\n",
    "    embedding_length = glove_embeddings.shape[1]\n",
    "    learning_rate = 5e-5\n",
    "    max_length = 40 # max sentence length (in tokens)\n",
    "    # Initialize embedding weights\n",
    "    weights = glove_embeddings\n",
    "    # Create model\n",
    "    model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, \n",
    "                          embedding_length, weights)\n",
    "    #Define model optimizer\n",
    "    model_optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    # Use cross entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Detect if we have a GPU available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, \n",
    "                      embedding_length, weights)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model_test.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Senti tree bank\n",
    "train_file = \"../../../cross_style_transfer_internal/data/xslue/SentiTreeBank/train.tsv\"\n",
    "dev_file = \"../../../cross_style_transfer_internal/data/xslue/SentiTreeBank/dev.tsv\"\n",
    "train_data = pd.read_csv(train_file, sep='\\t')\n",
    "val_data = pd.read_csv(dev_file, sep='\\t')\n",
    "\n",
    "\n",
    "# Short Humor Dataset:\n",
    "\n",
    "# train_file = \"../../../cross_style_transfer_internal/data/xslue/ShortHumor/train.tsv\"\n",
    "# dev_file = \"../../../cross_style_transfer_internal/data/xslue/ShortHumor/dev.tsv\"\n",
    "# train_data = pd.read_csv(train_file, names=['domain', 'score', 'text'], sep='\\t', error_bad_lines=False)\n",
    "# val_data = pd.read_csv(dev_file, names=['domain', 'score', 'text'], sep='\\t', \n",
    "#                        quoting=3, error_bad_lines=False)\n",
    "\n",
    "# Politeness dataset:\n",
    "\n",
    "# train_file = \"../../../cross_style_transfer_internal/data/xslue/StanfordPoliteness/train.tsv\"\n",
    "# dev_file = \"../../../cross_style_transfer_internal/data/xslue/StanfordPoliteness/dev.tsv\"\n",
    "# train_data = pd.read_csv(train_file, names=['domain', 'id', 'text', 'score'], sep='\\t')\n",
    "# val_data = pd.read_csv(dev_file, names=['domain', 'id', 'text', 'score'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    236076.000000\n",
       "mean          0.513022\n",
       "std           0.174044\n",
       "min           0.000000\n",
       "25%           0.416670\n",
       "50%           0.500000\n",
       "75%           0.611110\n",
       "max           1.000000\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['sentiment'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentibank(senti_df):\n",
    "    '''\n",
    "    Parse SentiBank dataframe into the format we need for classification.\n",
    "    '''\n",
    "    input_df = pd.DataFrame()\n",
    "    input_df['text'] = senti_df['phrase']\n",
    "    # Map scores >= 0.5 (positive) to label 1 and scores < 0 (negative) to label 0.\n",
    "    input_df['label'] = senti_df['sentiment'].apply(lambda x : int(x >= 0.5))\n",
    "    return input_df\n",
    "\n",
    "\n",
    "def parse_stanford_politeness(polite_df):\n",
    "    '''\n",
    "    Parse stanford politeness dataframe into the format we need for classification.\n",
    "    '''\n",
    "    input_df = pd.DataFrame()\n",
    "    input_df['text'] = polite_df['text']\n",
    "    # Map scores >= 0 (polite) to label 1 and scores < 0 (impolite) to label 0.\n",
    "    input_df['label'] = polite_df['score'].apply(lambda x : int(x >= 0))\n",
    "    return input_df\n",
    "\n",
    "def df_to_training_pairs(df, batch_size, max_length):\n",
    "    input_tensors = df['text'].apply(lambda x : sent_embedd(x, model, vocab_dict, tokenizer, max_length, device))\n",
    "    target_tensors = df['label'].apply(lambda x : torch.LongTensor([x]).to(device))\n",
    "    return input_tensors.values.reshape(-1, batch_size).tolist(), target_tensors.values.reshape(-1, batch_size).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = parse_sentibank(train_data)\n",
    "val_df = parse_sentibank(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>! '</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>! ''</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>! Alas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>! Brilliant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>! Brilliant !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text  label\n",
       "0            ! '      1\n",
       "1           ! ''      1\n",
       "2         ! Alas      0\n",
       "3    ! Brilliant      1\n",
       "4  ! Brilliant !      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing a single pass on a single input\n",
    "\n",
    "# sent = input_df['text'].iloc[0]\n",
    "# target_tensor = torch.LongTensor([input_df['label'].iloc[0]]).to(device)\n",
    "# input_tensor = sent_embedd(sent, model, vocab_dict)\n",
    "# train_single(input_tensor, target_tensor, model, \n",
    "#           model_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensors, target_tensors = df_to_training_pairs(train_df.head((len(train_df)//batch_size)*batch_size), \n",
    "                                                                   batch_size, max_length)\n",
    "\n",
    "input_val_tensors, target_val_tensors = df_to_training_pairs(val_df.head((len(val_df)//batch_size)*batch_size), \n",
    "                                                                   batch_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "\tLoss 0.6848\n",
      "\tTraining Accuracy 0.6348\n",
      "\tValidation Accuracy 0.5156\n",
      "Epoch 1 :\n",
      "\tLoss 0.8102\n",
      "\tTraining Accuracy 0.6346\n",
      "\tValidation Accuracy 0.5107\n",
      "Epoch 2 :\n",
      "\tLoss 1.4141\n",
      "\tTraining Accuracy 0.3681\n",
      "\tValidation Accuracy 0.4941\n",
      "Epoch 3 :\n",
      "\tLoss 2.7998\n",
      "\tTraining Accuracy 0.6351\n",
      "\tValidation Accuracy 0.5098\n",
      "Epoch 4 :\n",
      "\tLoss 6.4531\n",
      "\tTraining Accuracy 0.6336\n",
      "\tValidation Accuracy 0.5078\n",
      "Epoch 5 :\n",
      "\tLoss 24.4944\n",
      "\tTraining Accuracy 0.5512\n",
      "\tValidation Accuracy 0.5342\n",
      "Epoch 6 :\n",
      "\tLoss 371.6418\n",
      "\tTraining Accuracy 0.6319\n",
      "\tValidation Accuracy 0.5117\n",
      "Epoch 7 :\n",
      "\tLoss 563400.5259\n",
      "\tTraining Accuracy 0.5828\n",
      "\tValidation Accuracy 0.5049\n",
      "Epoch 8 :\n",
      "\tLoss 922142530.7605\n",
      "\tTraining Accuracy 0.3669\n",
      "\tValidation Accuracy 0.4902\n",
      "Epoch 9 :\n",
      "\tLoss 43427386306833.8359\n",
      "\tTraining Accuracy 0.6160\n",
      "\tValidation Accuracy 0.5215\n",
      "Epoch 10 :\n",
      "\tLoss 659193539149840128.0000\n",
      "\tTraining Accuracy 0.6134\n",
      "\tValidation Accuracy 0.5264\n",
      "Epoch 11 :\n",
      "\tLoss nan\n",
      "\tTraining Accuracy 0.3654\n",
      "\tValidation Accuracy 0.4902\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6cc6c4485cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     train(input_tensors, target_tensors, input_val_tensors, target_val_tensors, \n\u001b[0m\u001b[1;32m      3\u001b[0m           model, model_optimizer, criterion, train_epochs)\n",
      "\u001b[0;32m<ipython-input-5-52dc2c900251>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensors, target_tensors, input_val_tensors, target_val_tensors, model, model_optimizer, criterion, n_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             loss += train_single(input_tensor, target_tensor, model, \n\u001b[0m\u001b[1;32m     31\u001b[0m                                  model_optimizer, criterion)\n\u001b[1;32m     32\u001b[0m         \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-52dc2c900251>\u001b[0m in \u001b[0;36mtrain_single\u001b[0;34m(input_tensor_batch, target_tensor_batch, model, model_optimizer, criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Capstone/env_marvin/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Capstone/env_marvin/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if train_model:\n",
    "    train(input_tensors, target_tensors, input_val_tensors, target_val_tensors, \n",
    "          model, model_optimizer, criterion, train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model:\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    with open('../models/vocab_dict.pickle', 'wb') as pickleFile:\n",
    "        pickle.dump(vocab_dict, pickleFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- [X] handle unk words better\n",
    "    - currently initalizing to the average of all word embeddings like suggested [here](https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt)\n",
    "- [X] make work with batches\n",
    "- [ ] use different, better embeddings\n",
    "- [X] use better tokenizer, like spacy or some huggingface transformer model\n",
    "- [ ] train and save a good model\n",
    "- [ ] visualize attentions\n",
    "- [ ] make work with other datasets\n",
    "- [ ] convert to .py script that runs with input file that determines which data and parameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_batch = input_tensors[0]\n",
    "target_tensor_batch = target_tensors[0]\n",
    "target_tensor = torch.stack(target_tensor_batch).reshape(len(input_tensor_batch))\n",
    "input_tensor = torch.stack(input_tensor_batch)\n",
    "input_tensor = input_tensor.reshape(len(input_tensor_batch), input_tensor.shape[2])\n",
    "output, attention = model(input_tensor, return_attn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensors[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(train_df['text'].iloc[0])[attention[0].sum(axis=0).argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = attention[0].sum(axis=0).cpu().detach().numpy()\n",
    "plt.matshow([attn], cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"It is raining.\"\n",
    "tokens = tokenizer(sent)\n",
    "output, attns = sent_pred(sent, model, vocab_dict, tokenizer, max_length, device, batch_size)\n",
    "softmax = torch.nn.Softmax(dim=0)\n",
    "pred_scores = softmax(torch.tensor(output[0]))\n",
    "print(f\"humorous : {pred_scores[0]*100:.2f}%, not : {pred_scores[1]*100:.2f}%\")\n",
    "attn, _ = attns[0].max(axis=0)\n",
    "attn = attn.cpu().detach().numpy()\n",
    "print(tokens)\n",
    "plt.matshow([attn[:len(tokens)]], cmap='Reds');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = attn[:len(tokens)]\n",
    "x = np.arange(len(tokens))\n",
    "labels = tokens\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "plt.plot(x, y, 'bo')\n",
    "ax.set_ylabel(\"Attention\")\n",
    "ax.set_xlabel(\"Token Position\")\n",
    "\n",
    "for i, txt in enumerate(labels):\n",
    "    ax.annotate(txt, (x[i], y[i]), xytext=(x[i] + 0.015, y[i] + 0.015))\n",
    "plt.savefig('Attention_funny.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_marvin)",
   "language": "python",
   "name": "env_marvin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
