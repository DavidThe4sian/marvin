{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Standard Python Data Science imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partly taken from https://github.com/prakashpandey9/Text-Classification-Pytorch\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "        \n",
    "        --------\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.weights = weights\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        self.dropout = 0.8\n",
    "        self.bilstm = nn.LSTM(embedding_length, hidden_size, dropout=self.dropout, bidirectional=True)\n",
    "        # We will use da = 350, r = 30 & penalization_coeff = 1 \n",
    "        # as per given in the self-attention original ICLR paper\n",
    "        self.W_s1 = nn.Linear(2*hidden_size, 350)\n",
    "        self.W_s2 = nn.Linear(350, 30)\n",
    "        self.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n",
    "        self.label = nn.Linear(2000, output_size)\n",
    "\n",
    "    def attention_net(self, lstm_output):\n",
    "\n",
    "        \"\"\"\n",
    "        Now we will use self attention mechanism to produce a matrix \n",
    "        embedding of the input sentence in which every row represents an\n",
    "        encoding of the input sentence but giving an attention to a \n",
    "        specific part of the sentence. We will use 30 such embedding of \n",
    "        the input sentence and then finally we will concatenate all the 30 \n",
    "        sentence embedding vectors and connect it to a fully connected layer \n",
    "        of size 2000 which will be connected to the output layer of size 2 \n",
    "        returning logits for our two classes i.e., pos & neg.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        lstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
    "        ---------\n",
    "        Returns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
    "                  attention to different parts of the input sentence.\n",
    "        Tensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "                      attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
    "        \"\"\"\n",
    "        attn_weight_matrix = self.W_s2(F.tanh(self.W_s1(lstm_output)))\n",
    "        attn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
    "        attn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
    "\n",
    "        return attn_weight_matrix\n",
    "\n",
    "    def forward(self, input_sentences, batch_size=None, return_attn=False):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. \n",
    "        Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "        return_attn : bool determining whether to return attention layer activation \n",
    "                      default = False\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for pos & neg class.\n",
    "        Attention layer if the return_attn is set\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        if batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "\n",
    "        output, (h_n, c_n) = self.bilstm(input, (h_0, c_0))\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "        # h_n.size() = (1, batch_size, hidden_size)\n",
    "        # c_n.size() = (1, batch_size, hidden_size)\n",
    "        attn_weight_matrix = self.attention_net(output)\n",
    "        # attn_weight_matrix.size() = (batch_size, r, num_seq)\n",
    "        # output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "        hidden_matrix = torch.bmm(attn_weight_matrix, output)\n",
    "        # hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n",
    "        # Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n",
    "        fc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
    "        logits = self.label(fc_out)\n",
    "        # logits.size() = (batch_size, output_size)\n",
    "        if not return_attn:\n",
    "            return logits\n",
    "        else:\n",
    "            return logits, attn_weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(filename, vocab_size=10000, compute_avg=True):\n",
    "    \"\"\"\n",
    "    Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
    "  \n",
    "    Arguments:\n",
    "      - filename:     path to file\n",
    "                      automatically infers correct embedding dimension from filename\n",
    "      - vocab_size:   maximum number of embeddings to load\n",
    "      - compute_avg:  bool to decide whether to comnpute the average word embedding,\n",
    "                      which can be used as a <unk> embedding\n",
    "\n",
    "      Returns \n",
    "      - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
    "      - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # get the embedding size from the first embedding\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
    "\n",
    "    vocab = {}\n",
    "    inv_vocab = {}\n",
    "    \n",
    "    if compute_avg:\n",
    "        # Add extra embedding for <unk> and <pad>\n",
    "        # last index is <unk>, first index is <pad>\n",
    "        embeddings = np.zeros((vocab_size + 2, word_embedding_dim))\n",
    "    else:\n",
    "         # Only add extra embedding for <pad>\n",
    "         # first index is <pad>\n",
    "        embeddings = np.zeros((vocab_size + 1, word_embedding_dim))\n",
    "        \n",
    "\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "\n",
    "            if idx >= vocab_size:\n",
    "                break\n",
    "            \n",
    "            cols = line.rstrip().split(\" \")\n",
    "            val = np.array(cols[1:])\n",
    "            word = cols[0]\n",
    "            embeddings[idx + 1] = val\n",
    "            vocab[word] = idx + 1\n",
    "            inv_vocab[idx + 1] = word\n",
    "    \n",
    "    # Set <unk> embedding to the average of all other embedding vects in vocab\n",
    "    if compute_avg:\n",
    "        embeddings[-1] = embeddings[:-1].mean(axis=0)\n",
    "    \n",
    "    return torch.FloatTensor(embeddings), vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_embedd(sent, model, vocab_dict, tokenizer, max_length, unk_embedd=True):\n",
    "    '''\n",
    "    Helper function to extract the embedding for an input sentence.\n",
    "    '''\n",
    "    idxs = [0 for i in range(max_length)]\n",
    "    i = 0\n",
    "    for word in tokenizer(sent):\n",
    "        if i < max_length:\n",
    "            if word in vocab_dict:\n",
    "                idxs[i] = vocab_dict[word]\n",
    "            else:\n",
    "                # If using <unk> embedding, append \n",
    "                # the final index where that embedding is stored\n",
    "                if unk_embedd:\n",
    "                    idxs[i] = len(vocab_dict)\n",
    "            i += 1\n",
    "    return torch.LongTensor([idxs]).to(device)\n",
    "            \n",
    "def sent_pred(sent, model, vocab_dict, max_length):\n",
    "    '''\n",
    "    Runs the model on an input sentence.\n",
    "    \n",
    "    Arguments: \n",
    "    \n",
    "      sent : str. The input sentence.\n",
    "      model : the pytorch model to be used.\n",
    "      vocab_dict : dict. A dictionary with words as keys and their indices as values\n",
    "     \n",
    "    Returns:\n",
    "      pred : np array. The prediction, wich is a normalized array with a value for \\\n",
    "             each class, representing the predicted probability for that class\n",
    "      attns : the attention matrix\n",
    "    '''\n",
    "    input_tensor = sent_embedd(sent, model, vocab_dict, max_length)\n",
    "    pred, attns = model(input_tensor, return_attn=True)\n",
    "    return pred.detach().cpu().numpy(), attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single(input_tensor_batch, target_tensor_batch, model, \n",
    "          model_optimizer, criterion):\n",
    "    '''\n",
    "    A single forward and backward pass of the neural net on a single training batch.\n",
    "    '''\n",
    "    target_tensor = torch.stack(target_tensor_batch).reshape(len(input_tensor_batch))\n",
    "    input_tensor = torch.stack(input_tensor_batch)\n",
    "    input_tensor = input_tensor.reshape(len(input_tensor_batch), input_tensor.shape[2])\n",
    "    output = model(input_tensor, return_attn=False)\n",
    "    loss = criterion(output, target_tensor)\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train(input_tensors, target_tensors, model, model_optimizer, criterion, n_epochs):\n",
    "    '''\n",
    "    Train the attention classfier for a given number of epochs on the whole training set.\n",
    "    '''\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = 0\n",
    "        for i in range(len(input_tensors)):\n",
    "            input_tensor = input_tensors[i]\n",
    "            target_tensor = target_tensors[i]\n",
    "            loss += train_single(input_tensor, target_tensor, model, \n",
    "                                 model_optimizer, criterion)\n",
    "        print(f\"Epoch {epoch} : Loss {loss/len(input_tensors):.4f}\")\n",
    "        losses.append(loss/len(input_tensors))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_embedd = True # Bool for whether to create an embedding for the <unk> token\n",
    "# Get initial word vector embedings from disk\n",
    "glove_filename = './GloveEmbeddings/glove.6B.100d.txt' \n",
    "glove_embeddings, vocab_dict, inv_vocab_dict = read_embeddings(glove_filename, \n",
    "                                                               vocab_size=50000,\n",
    "                                                               compute_avg=unk_embedd)\n",
    "# Create tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention(\n",
       "  (word_embeddings): Embedding(50002, 100)\n",
       "  (bilstm): LSTM(100, 100, dropout=0.8, bidirectional=True)\n",
       "  (W_s1): Linear(in_features=200, out_features=350, bias=True)\n",
       "  (W_s2): Linear(in_features=350, out_features=30, bias=True)\n",
       "  (fc_layer): Linear(in_features=6000, out_features=2000, bias=True)\n",
       "  (label): Linear(in_features=2000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dimensions and hyperparameters\n",
    "batch_size = 16\n",
    "output_size = 2\n",
    "hidden_size = 100\n",
    "vocab_size = glove_embeddings.shape[0]\n",
    "embedding_length = glove_embeddings.shape[1]\n",
    "learning_rate = 10e-5\n",
    "max_length = 40 # max sentence length (in tokens)\n",
    "# Initialize embedding weights\n",
    "weights = glove_embeddings\n",
    "# Create model\n",
    "model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, \n",
    "                      embedding_length, weights)\n",
    "#Define model optimizer\n",
    "model_optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# Use cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred, attns = sent_pred('the world is red', model, vocab_dict)\n",
    "# print(pred)\n",
    "# print(attns.sum(axis=1).reshape(attns.shape[2]).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"../cross_style_transfer_internal/data/xslue/StanfordPoliteness/train.tsv\"\n",
    "data = pd.read_csv(train_file, names=['domain', 'id', 'text', 'score'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stanford_politeness(polite_df):\n",
    "    '''\n",
    "    Parse stanford politeness dataframe into the format we need for classification.\n",
    "    '''\n",
    "    input_df = pd.DataFrame()\n",
    "    input_df['text'] = polite_df['text']\n",
    "    # Map scores >= 0 (polite) to label 1 and scores < 0 (impolite) to label 0.\n",
    "    input_df['label'] = polite_df['score'].apply(lambda x : int(x >= 0))\n",
    "    return input_df\n",
    "\n",
    "def df_to_training_pairs(df, batch_size, max_length):\n",
    "    input_tensors = df['text'].apply(lambda x : sent_embedd(x, model, vocab_dict, tokenizer, max_length))\n",
    "    target_tensors = df['label'].apply(lambda x : torch.LongTensor([x]).to(device))\n",
    "    return input_tensors.values.reshape(-1, batch_size).tolist(), target_tensors.values.reshape(-1, batch_size).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = parse_stanford_politeness(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where did you learn English? How come you're t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thanks very much for your edit to the &lt;url&gt; ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sir i think u hav many friends on wiki who can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't find it.  Maybe I didn't manage to gue...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I can't spend too much time, and I'm no specia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Where did you learn English? How come you're t...      0\n",
       "1  Thanks very much for your edit to the <url> ar...      1\n",
       "2  Sir i think u hav many friends on wiki who can...      0\n",
       "3  I can't find it.  Maybe I didn't manage to gue...      1\n",
       "4  I can't spend too much time, and I'm no specia...      1"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing a single pass on a single input\n",
    "\n",
    "# sent = input_df['text'].iloc[0]\n",
    "# target_tensor = torch.LongTensor([input_df['label'].iloc[0]]).to(device)\n",
    "# input_tensor = sent_embedd(sent, model, vocab_dict)\n",
    "# train_single(input_tensor, target_tensor, model, \n",
    "#           model_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensors, target_tensors = df_to_training_pairs(input_df.head((len(input_df)//batch_size)*batch_size), \n",
    "                                                                   batch_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Loss 0.7273\n",
      "Epoch 1 : Loss 0.7626\n",
      "Epoch 2 : Loss 0.7861\n",
      "Epoch 3 : Loss 0.7671\n",
      "Epoch 4 : Loss 0.9012\n",
      "Epoch 5 : Loss 0.7876\n",
      "Epoch 6 : Loss 0.7040\n",
      "Epoch 7 : Loss 0.7569\n",
      "Epoch 8 : Loss 0.7684\n",
      "Epoch 9 : Loss 0.8150\n",
      "Epoch 10 : Loss 0.8577\n",
      "Epoch 11 : Loss 0.9584\n",
      "Epoch 12 : Loss 0.9595\n",
      "Epoch 13 : Loss 0.9125\n",
      "Epoch 14 : Loss 0.8147\n",
      "Epoch 15 : Loss 0.7696\n",
      "Epoch 16 : Loss 0.7491\n",
      "Epoch 17 : Loss 0.6014\n",
      "Epoch 18 : Loss 0.5949\n",
      "Epoch 19 : Loss 0.5755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7272814550957123,\n",
       " 0.7625551431306771,\n",
       " 0.7861400538353951,\n",
       " 0.7671344040759972,\n",
       " 0.9012486181379139,\n",
       " 0.7876155763477474,\n",
       " 0.7039959943526751,\n",
       " 0.7568828998157731,\n",
       " 0.7684298430557375,\n",
       " 0.8150115689480459,\n",
       " 0.8576596030941257,\n",
       " 0.9583921643917437,\n",
       " 0.9594507070814634,\n",
       " 0.912521401589567,\n",
       " 0.8147297311912883,\n",
       " 0.7696443870905545,\n",
       " 0.7490790940385748,\n",
       " 0.6014387190535471,\n",
       " 0.5948914579031143,\n",
       " 0.5755344770074665]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(input_tensors, target_tensors, model, model_optimizer, criterion, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- [X] handle unk words better\n",
    "    - currently initalizing to the average of all word embeddings like suggested [here](https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt)\n",
    "- [X] make work with batches\n",
    "- [ ] use different, better embeddings\n",
    "- [X] use better tokenizer, like spacy or some huggingface transformer model\n",
    "- [ ] train and save a good model\n",
    "- [ ] visualize attentions\n",
    "- [ ] make work with other datasets\n",
    "- [ ] convert to .py script that runs with input file that determines which data and parameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
