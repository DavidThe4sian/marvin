{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Standard Python Data Science imports\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partly taken from https://github.com/prakashpandey9/Text-Classification-Pytorch\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "        \n",
    "        --------\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.weights = weights\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        self.dropout = 0.8\n",
    "        self.bilstm = nn.LSTM(embedding_length, hidden_size, dropout=self.dropout, bidirectional=True)\n",
    "        # We will use da = 350, r = 30 & penalization_coeff = 1 \n",
    "        # as per given in the self-attention original ICLR paper\n",
    "        self.W_s1 = nn.Linear(2*hidden_size, 350)\n",
    "        self.W_s2 = nn.Linear(350, 30)\n",
    "        self.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n",
    "        self.label = nn.Linear(2000, output_size)\n",
    "\n",
    "    def attention_net(self, lstm_output):\n",
    "\n",
    "        \"\"\"\n",
    "        Now we will use self attention mechanism to produce a matrix \n",
    "        embedding of the input sentence in which every row represents an\n",
    "        encoding of the input sentence but giving an attention to a \n",
    "        specific part of the sentence. We will use 30 such embedding of \n",
    "        the input sentence and then finally we will concatenate all the 30 \n",
    "        sentence embedding vectors and connect it to a fully connected layer \n",
    "        of size 2000 which will be connected to the output layer of size 2 \n",
    "        returning logits for our two classes i.e., pos & neg.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        lstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
    "        ---------\n",
    "        Returns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
    "                  attention to different parts of the input sentence.\n",
    "        Tensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "                      attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
    "        \"\"\"\n",
    "        attn_weight_matrix = self.W_s2(F.tanh(self.W_s1(lstm_output)))\n",
    "        attn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
    "        attn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
    "\n",
    "        return attn_weight_matrix\n",
    "\n",
    "    def forward(self, input_sentences, batch_size=None, return_attn=False):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. \n",
    "        Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "        return_attn : bool determining whether to return attention layer activation \n",
    "                      default = False\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for pos & neg class.\n",
    "        Attention layer if the return_attn is set\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        if batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "\n",
    "        output, (h_n, c_n) = self.bilstm(input, (h_0, c_0))\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "        # h_n.size() = (1, batch_size, hidden_size)\n",
    "        # c_n.size() = (1, batch_size, hidden_size)\n",
    "        attn_weight_matrix = self.attention_net(output)\n",
    "        # attn_weight_matrix.size() = (batch_size, r, num_seq)\n",
    "        # output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "        hidden_matrix = torch.bmm(attn_weight_matrix, output)\n",
    "        # hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n",
    "        # Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n",
    "        fc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
    "        logits = self.label(fc_out)\n",
    "        # logits.size() = (batch_size, output_size)\n",
    "        if not return_attn:\n",
    "            return logits\n",
    "        else:\n",
    "            return logits, attn_weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(filename, vocab_size=10000, compute_avg=True):\n",
    "    \"\"\"\n",
    "    Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
    "  \n",
    "    Arguments:\n",
    "      - filename:     path to file\n",
    "                      automatically infers correct embedding dimension from filename\n",
    "      - vocab_size:   maximum number of embeddings to load\n",
    "      - compute_avg:  bool to decide whether to comnpute the average word embedding,\n",
    "                      which can be used as a <unk> embedding\n",
    "\n",
    "      Returns \n",
    "      - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
    "      - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # get the embedding size from the first embedding\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
    "\n",
    "    vocab = {}\n",
    "    inv_vocab = {}\n",
    "    \n",
    "    if compute_avg:\n",
    "        # Add extra embedding for <unk> and <pad>\n",
    "        # last index is <unk>, first index is <pad>\n",
    "        embeddings = np.zeros((vocab_size + 2, word_embedding_dim))\n",
    "    else:\n",
    "         # Only add extra embedding for <pad>\n",
    "         # first index is <pad>\n",
    "        embeddings = np.zeros((vocab_size + 1, word_embedding_dim))\n",
    "        \n",
    "\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "\n",
    "            if idx >= vocab_size:\n",
    "                break\n",
    "            \n",
    "            cols = line.rstrip().split(\" \")\n",
    "            val = np.array(cols[1:])\n",
    "            word = cols[0]\n",
    "            embeddings[idx + 1] = val\n",
    "            vocab[word] = idx + 1\n",
    "            inv_vocab[idx + 1] = word\n",
    "    \n",
    "    # Set <unk> embedding to the average of all other embedding vects in vocab\n",
    "    if compute_avg:\n",
    "        embeddings[-1] = embeddings[:-1].mean(axis=0)\n",
    "    \n",
    "    return torch.FloatTensor(embeddings), vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_embedd(sent, model, vocab_dict, tokenizer, max_length, device, unk_embedd=True):\n",
    "    '''\n",
    "    Helper function to extract the embedding for an input sentence.\n",
    "    '''\n",
    "    idxs = [0 for i in range(max_length)]\n",
    "    i = 0\n",
    "    for word in tokenizer(sent):\n",
    "        if i < max_length:\n",
    "            if word in vocab_dict:\n",
    "                idxs[i] = vocab_dict[word]\n",
    "            else:\n",
    "                # If using <unk> embedding, append \n",
    "                # the final index where that embedding is stored\n",
    "                if unk_embedd:\n",
    "                    idxs[i] = len(vocab_dict)\n",
    "            i += 1\n",
    "    return torch.LongTensor([idxs]).to(device)\n",
    "            \n",
    "def sent_pred(sent, model, vocab_dict, tokenizer, max_length, device, batch_size):\n",
    "    '''\n",
    "    Runs the model on an input sentence.\n",
    "    \n",
    "    Arguments: \n",
    "    \n",
    "      sent : str. The input sentence.\n",
    "      model : the pytorch model to be used.\n",
    "      vocab_dict : dict. A dictionary with words as keys and their indices as values\n",
    "     \n",
    "    Returns:\n",
    "      pred : np array. The prediction, wich is a normalized array with a value for \\\n",
    "             each class, representing the predicted probability for that class\n",
    "      attns : the attention matrix\n",
    "    '''\n",
    "    input_tensor = sent_embedd(sent, model, vocab_dict, tokenizer, max_length, device)\n",
    "    input_tensor = torch.cat(batch_size * [input_tensor])\n",
    "    pred, attns = model(input_tensor, return_attn=True)\n",
    "    return pred.detach().cpu().numpy(), attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single(input_tensor_batch, target_tensor_batch, model, \n",
    "          model_optimizer, criterion):\n",
    "    '''\n",
    "    A single forward and backward pass of the neural net on a single training batch.\n",
    "    '''\n",
    "    target_tensor = torch.stack(target_tensor_batch).reshape(len(input_tensor_batch))\n",
    "    input_tensor = torch.stack(input_tensor_batch)\n",
    "    input_tensor = input_tensor.reshape(len(input_tensor_batch), input_tensor.shape[2])\n",
    "    output = model(input_tensor, return_attn=False)\n",
    "    loss = criterion(output, target_tensor)\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train(input_tensors, target_tensors, input_val_tensors, target_val_tensors,\n",
    "          model, model_optimizer, criterion, n_epochs):\n",
    "    '''\n",
    "    Train the attention classfier for a given number of epochs on the whole training set.\n",
    "    '''\n",
    "    losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    # Iterate over given num of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = 0\n",
    "        # Iterate over batches\n",
    "        for i in range(len(input_tensors)):\n",
    "            input_tensor = input_tensors[i]\n",
    "            target_tensor = target_tensors[i]\n",
    "            loss += train_single(input_tensor, target_tensor, model, \n",
    "                                 model_optimizer, criterion)\n",
    "        train_accuracy = get_accuracy(input_tensors, target_tensors, model)\n",
    "        val_accuracy = get_accuracy(input_val_tensors, target_val_tensors, model)\n",
    "        print(f\"Epoch {epoch} :\") \n",
    "        print(f\"\\tLoss {loss/len(input_tensors):.4f}\")\n",
    "        print(f\"\\tTraining Accuracy {train_accuracy:.4f}\")\n",
    "        print(f\"\\tValidation Accuracy {val_accuracy:.4f}\")\n",
    "        losses.append(loss/len(input_tensors))\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "    return losses, train_accs, val_accs\n",
    "\n",
    "def get_accuracy(input_tensors, target_tensors, model):\n",
    "    '''\n",
    "    Get model accuracy.\n",
    "    '''\n",
    "    accs = []\n",
    "    # Iterate over batches\n",
    "    for i in range(len(input_tensors)):\n",
    "        input_tensor_batch = input_tensors[i]\n",
    "        target_tensor_batch = target_tensors[i]\n",
    "        target_tensor = torch.stack(target_tensor_batch).reshape(len(input_tensor_batch))\n",
    "        input_tensor = torch.stack(input_tensor_batch)\n",
    "        input_tensor = input_tensor.reshape(len(input_tensor_batch), input_tensor.shape[2])\n",
    "        output = model(input_tensor, return_attn=False)\n",
    "        # Get classification prediction\n",
    "        preds = output.argmax(axis=1)\n",
    "        # Get accuracy of given batch\n",
    "        batch_acc = ((preds == target_tensor).sum()/target_tensor.shape[0]).item()\n",
    "        accs.append(batch_acc)\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_embedd = True # Bool for whether to create an embedding for the <unk> token\n",
    "# Get initial word vector embedings from disk\n",
    "glove_filename = './GloveEmbeddings/glove.6B.100d.txt' \n",
    "glove_embeddings, vocab_dict, inv_vocab_dict = read_embeddings(glove_filename, \n",
    "                                                               vocab_size=100000,\n",
    "                                                               compute_avg=unk_embedd)\n",
    "# Create tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False # Whether to save model or not\n",
    "load_model = False # Whether to load model or not\n",
    "train_model = True # Whether to train model or not\n",
    "checkpoint_path = \"../models/attn_test.pth\"\n",
    "train_epochs = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmac/Documents/MIMS Coursework/Capstone/env_marvin/lib/python3.8/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "if not load_model:\n",
    "    # Set dimensions and hyperparameters\n",
    "    batch_size = 32\n",
    "    output_size = 2\n",
    "    hidden_size = 100\n",
    "    vocab_size = glove_embeddings.shape[0]\n",
    "    embedding_length = glove_embeddings.shape[1]\n",
    "    learning_rate = 5e-5\n",
    "    max_length = 40 # max sentence length (in tokens)\n",
    "    # Initialize embedding weights\n",
    "    weights = glove_embeddings\n",
    "    # Create model\n",
    "    model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, \n",
    "                          embedding_length, weights)\n",
    "    #Define model optimizer\n",
    "    model_optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    # Use cross entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Detect if we have a GPU available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    model = SelfAttention(batch_size, output_size, hidden_size, vocab_size, \n",
    "                      embedding_length, weights)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model_test.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"../../../cross_style_transfer_internal/data/xslue/ShortHumor/train.tsv\"\n",
    "dev_file = \"../../../cross_style_transfer_internal/data/xslue/ShortHumor/dev.tsv\"\n",
    "train_data = pd.read_csv(train_file, names=['domain', 'score', 'text'], sep='\\t', error_bad_lines=False)\n",
    "val_data = pd.read_csv(dev_file, names=['domain', 'score', 'text'], sep='\\t', \n",
    "                       quoting=3, error_bad_lines=False)\n",
    "\n",
    "\n",
    "# train_file = \"../../../cross_style_transfer_internal/data/xslue/StanfordPoliteness/train.tsv\"\n",
    "# dev_file = \"../../../cross_style_transfer_internal/data/xslue/StanfordPoliteness/dev.tsv\"\n",
    "# train_data = pd.read_csv(train_file, names=['domain', 'id', 'text', 'score'], sep='\\t')\n",
    "# val_data = pd.read_csv(dev_file, names=['domain', 'id', 'text', 'score'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stanford_politeness(polite_df):\n",
    "    '''\n",
    "    Parse stanford politeness dataframe into the format we need for classification.\n",
    "    '''\n",
    "    input_df = pd.DataFrame()\n",
    "    input_df['text'] = polite_df['text']\n",
    "    # Map scores >= 0 (polite) to label 1 and scores < 0 (impolite) to label 0.\n",
    "    input_df['label'] = polite_df['score'].apply(lambda x : int(x > 0))\n",
    "    return input_df\n",
    "\n",
    "def df_to_training_pairs(df, batch_size, max_length):\n",
    "    input_tensors = df['text'].apply(lambda x : sent_embedd(x, model, vocab_dict, tokenizer, max_length, device))\n",
    "    target_tensors = df['label'].apply(lambda x : torch.LongTensor([x]).to(device))\n",
    "    return input_tensors.values.reshape(-1, batch_size).tolist(), target_tensors.values.reshape(-1, batch_size).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = parse_stanford_politeness(train_data)\n",
    "val_df = parse_stanford_politeness(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They wouldn't just lie about something like th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asia shares mostly firm, China sets yuan higher</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Get 'im, Browns!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>norman virgil osbourne !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Republican Marco Rubio winning Wall Street fun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  They wouldn't just lie about something like th...      0\n",
       "1    Asia shares mostly firm, China sets yuan higher      1\n",
       "2                                   Get 'im, Browns!      0\n",
       "3                           norman virgil osbourne !      0\n",
       "4  Republican Marco Rubio winning Wall Street fun...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing a single pass on a single input\n",
    "\n",
    "# sent = input_df['text'].iloc[0]\n",
    "# target_tensor = torch.LongTensor([input_df['label'].iloc[0]]).to(device)\n",
    "# input_tensor = sent_embedd(sent, model, vocab_dict)\n",
    "# train_single(input_tensor, target_tensor, model, \n",
    "#           model_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensors, target_tensors = df_to_training_pairs(train_df.head((len(train_df)//batch_size)*batch_size), \n",
    "                                                                   batch_size, max_length)\n",
    "\n",
    "input_val_tensors, target_val_tensors = df_to_training_pairs(val_df.head((len(val_df)//batch_size)*batch_size), \n",
    "                                                                   batch_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmac/Documents/MIMS Coursework/Capstone/env_marvin/lib/python3.8/site-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "\tLoss 0.6683\n",
      "\tTraining Accuracy 0.5117\n",
      "\tValidation Accuracy 0.5115\n",
      "Epoch 1 :\n",
      "\tLoss 0.6033\n",
      "\tTraining Accuracy 0.6973\n",
      "\tValidation Accuracy 0.7005\n",
      "Epoch 2 :\n",
      "\tLoss 0.5232\n",
      "\tTraining Accuracy 0.7862\n",
      "\tValidation Accuracy 0.7788\n",
      "Epoch 3 :\n",
      "\tLoss 0.4379\n",
      "\tTraining Accuracy 0.8197\n",
      "\tValidation Accuracy 0.8077\n",
      "Epoch 4 :\n",
      "\tLoss 0.3776\n",
      "\tTraining Accuracy 0.8456\n",
      "\tValidation Accuracy 0.8457\n",
      "Epoch 5 :\n",
      "\tLoss 0.3619\n",
      "\tTraining Accuracy 0.8590\n",
      "\tValidation Accuracy 0.8452\n",
      "Epoch 6 :\n",
      "\tLoss 0.3116\n",
      "\tTraining Accuracy 0.8803\n",
      "\tValidation Accuracy 0.8558\n",
      "Epoch 7 :\n",
      "\tLoss 0.2949\n",
      "\tTraining Accuracy 0.8881\n",
      "\tValidation Accuracy 0.8538\n",
      "Epoch 8 :\n",
      "\tLoss 0.2796\n",
      "\tTraining Accuracy 0.9043\n",
      "\tValidation Accuracy 0.8654\n",
      "Epoch 9 :\n",
      "\tLoss 0.2624\n",
      "\tTraining Accuracy 0.9248\n",
      "\tValidation Accuracy 0.8788\n",
      "Epoch 10 :\n",
      "\tLoss 0.2522\n",
      "\tTraining Accuracy 0.9078\n",
      "\tValidation Accuracy 0.8572\n",
      "Epoch 11 :\n",
      "\tLoss 0.2486\n",
      "\tTraining Accuracy 0.9032\n",
      "\tValidation Accuracy 0.8529\n",
      "Epoch 12 :\n",
      "\tLoss 0.2337\n",
      "\tTraining Accuracy 0.9327\n",
      "\tValidation Accuracy 0.8697\n",
      "Epoch 13 :\n",
      "\tLoss 0.2290\n",
      "\tTraining Accuracy 0.9390\n",
      "\tValidation Accuracy 0.8740\n"
     ]
    }
   ],
   "source": [
    "if train_model:\n",
    "    train(input_tensors, target_tensors, input_val_tensors, target_val_tensors, \n",
    "          model, model_optimizer, criterion, train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model:\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    with open('../models/vocab_dict.pickle', 'wb') as pickleFile:\n",
    "        pickle.dump(vocab_dict, pickleFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- [X] handle unk words better\n",
    "    - currently initalizing to the average of all word embeddings like suggested [here](https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt)\n",
    "- [X] make work with batches\n",
    "- [ ] use different, better embeddings\n",
    "- [X] use better tokenizer, like spacy or some huggingface transformer model\n",
    "- [ ] train and save a good model\n",
    "- [ ] visualize attentions\n",
    "- [ ] make work with other datasets\n",
    "- [ ] convert to .py script that runs with input file that determines which data and parameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_batch = input_tensors[0]\n",
    "target_tensor_batch = target_tensors[0]\n",
    "target_tensor = torch.stack(target_tensor_batch).reshape(len(input_tensor_batch))\n",
    "input_tensor = torch.stack(input_tensor_batch)\n",
    "input_tensor = input_tensor.reshape(len(input_tensor_batch), input_tensor.shape[2])\n",
    "output, attention = model(input_tensor, return_attn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     They wouldn't just lie about something like th...\n",
       "label                                                    0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    40, 100000,     58,   2160,    121,   4622,     60,    646,    118,\n",
       "             13,     55,     40,    189,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensors[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 40])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lie'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_df['text'].iloc[0])[attention[0].sum(axis=0).argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc1ac766b20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn = attention[0].sum(axis=0).cpu().detach().numpy()\n",
    "plt.matshow([attn], cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humorous : 99.95%, not : 0.05%\n",
      "['looks', 'like', 'rigged', 'polls', 'are', 'back', 'on', 'the', 'menu', 'boys', '!']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6oAAAB0CAYAAACMoM/cAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAM3klEQVR4nO3df6zd9V3H8eerrWV2hQLtTVfaQjFrXOqiA28Ic3ES6ZISDSVRJ8RpMcz+sTHxV0yVBBP2D/MHYiIxNgypYxlineHGVRnrZvaPELpBkILYygYUCi3gmNgw1u3tH/eLuVzO7V17Dvf7Pec8H0lzvj8+OZ9X0k/a+7rf7/ecVBWSJEmSJHXForYDSJIkSZI0k0VVkiRJktQpFlVJkiRJUqdYVCVJkiRJnWJRlSRJkiR1ikVVkiRJktQpFtVTlGRLkieSHEyyo+08Gj9J1if5SpLHkuxPcl3bmTS+kixO8lCSf2o7i8ZXkjOT7E7yH0keT/L+tjNp/CT57eb/5UeTfC7JO9rOpNGX5PYkR5I8OuPY2UnuS3KgeT2rzYwny6J6CpIsBm4FLgM2AVcl2dRuKo2h48DvVtUm4GLg465Dteg64PG2Q2js/QXwL1X1HuAncE1qgSVZC/wmMFlV7wUWA1e2m0pj4g5gy6xjO4C9VbUR2NvsDw2L6qm5CDhYVU9W1evAXcDWljNpzFTV4ar6erP9P0z/QLa23VQaR0nWAT8H3NZ2Fo2vJCuADwKfBqiq16vqW62G0rhaAvxwkiXAMuC5lvNoDFTVV4GXZx3eCuxqtncBVyxkpn5ZVE/NWuCZGfuHsCCoRUk2ABcAD7QcRePpFuD3ge+3nEPj7XzgKPA3zW3otyV5Z9uhNF6q6lngT4GngcPAK1X1xXZTaYytrqrDzfbzwOo2w5wsi6o05JIsB/4B+K2q+nbbeTRekvw8cKSqvtZ2Fo29JcCFwF9V1QXA/zJkt7lp+DXPAG5l+hcn5wDvTPKRdlNJUFUFVNs5ToZF9dQ8C6yfsb+uOSYtqCQ/xHRJ/WxVfb7tPBpLHwAuT/JNph+D+Nkkd7YbSWPqEHCoqt64s2Q308VVWkibgW9U1dGq+i7weeCnWs6k8fVCkjUAzeuRlvOcFIvqqXkQ2Jjk/CRLmX5IfqrlTBozScL0s1iPV9XNbefReKqqP6iqdVW1gel/C79cVV490IKrqueBZ5L8aHPoUuCxFiNpPD0NXJxkWfP/9KX4oV5qzxSwrdneBtzTYpaTtqTtAMOoqo4nuRa4l+lPc7u9qva3HEvj5wPArwL/nuTh5tgfVtWe9iJJUqs+AXy2+SXyk8Cvt5xHY6aqHkiyG/g605/O/xCws91UGgdJPgdcAqxKcgj4I+Am4O4k1wBPAR9uL+HJy/TtypIkSZIkdYO3/kqSJEmSOsWiKkmSJEnqFIuqJEmSJKlTLKqSJEmSpE6xqEqSJEmSOsWi2ock29vOILkO1RWuRXWB61Bd4DpUVwzzWrSo9mdo/+I1UlyH6grXorrAdagucB2qK4Z2LVpUJUmSJEmdkqpqO0NPq05fVhtWntl2jBM6+uoxJpYvazvGnJ566oW2Iwy9897zI21HmNfRb73CxJkr2o4xt+9+p+0EWiBHv/0qE2csbzvG3Jad3naC4bdkadsJ5nX0xZeYWLWy7RhzquefbjvC8Ovmj65v8uKx11i17B1tx5jborSdYCRk1Zq2I8zr6EsvM7Hy7LZjzOlrjzz6YlVN9Dq3ZKHD/KA2rDyTB274jbZjDLWPX3Nz2xGG3q27bmk7wvB77httJ5AAWHThJW1HGHpZtb7tCEPv+E2faDvC8Pve99pOMPxOO63tBCNh8fbr244w9BatefdTc55byCCSJEmSJM3HoipJkiRJ6pS+imqSs5Pcl+RA83rWCcaekeRQkr/sZ05JkiRJ0mjr94rqDmBvVW0E9jb7c/kk8NU+55MkSZIkjbh+i+pWYFezvQu4otegJD8JrAa+2Od8kiRJkqQR129RXV1Vh5vt55kuo2+SZBHwZ8Dv9TmXJEmSJGkMzPv1NEm+BLyrx6k3fR5zVVWSXt9s9TFgT1UdSk78nU1JtgPbAc5d2eHvhZQkSZIkvW3mLapVtXmuc0leSLKmqg4nWQMc6THs/cBPJ/kYsBxYmuTVqnrL86xVtRPYCTC54Zwh+DpnSZIkSdKgzVtU5zEFbANual7vmT2gqn7lje0kVwOTvUqqJEmSJEnQ/zOqNwEfSnIA2Nzsk2QyyW39hpMkSZIkjZ++rqhW1UvApT2O7wM+2uP4HcAd/cwpSZIkSRpt/V5RlSRJkiRpoCyqkiRJkqROsahKkiRJkjrFoipJkiRJ6hSLqiRJkiSpUyyqkiRJkqROsahKkiRJkjrFoipJkiRJ6pS+imqSs5Pcl+RA83pWjzHvS/JvSfYneSTJL/czpyRJkiRptPV7RXUHsLeqNgJ7m/3ZjgG/VlU/BmwBbklyZp/zSpIkSZJGVL9FdSuwq9neBVwxe0BV/WdVHWi2nwOOABN9zitJkiRJGlH9FtXVVXW42X4eWH2iwUkuApYC/zXH+e1J9iXZd/TVY31GkyRJkiQNoyXzDUjyJeBdPU5dP3OnqipJneB91gCfAbZV1fd7jamqncBOgMkN58z5XpIkSZKk0TVvUa2qzXOdS/JCkjVVdbgpokfmGHcG8AXg+qq6/5TTSpIkSZJGXr+3/k4B25rtbcA9swckWQr8I/C3VbW7z/kkSZIkSSOu36J6E/ChJAeAzc0+SSaT3NaM+TDwQeDqJA83f97X57ySJEmSpBE1762/J1JVLwGX9ji+D/hos30ncGc/80iSJEmSxke/V1QlSZIkSRooi6okSZIkqVMsqpIkSZKkTrGoSpIkSZI6xaIqSZIkSeoUi6okSZIkqVMsqpIkSZKkThlIUU2yJckTSQ4m2dHj/GlJ/q45/0CSDYOYV5IkSZI0evouqkkWA7cClwGbgKuSbJo17Brgv6vq3cCfA5/qd15JkiRJ0mgaxBXVi4CDVfVkVb0O3AVsnTVmK7Cr2d4NXJokA5hbkiRJkjRiBlFU1wLPzNg/1BzrOaaqjgOvACsHMLckSZIkacR06sOUkmxPsi/JvqOvHms7jiRJkiSpBYMoqs8C62fsr2uO9RyTZAmwAnhp9htV1c6qmqyqyYnlywYQTZIkSZI0bAZRVB8ENiY5P8lS4EpgataYKWBbs/2LwJerqgYwtyRJkiRpxCzp9w2q6niSa4F7gcXA7VW1P8mNwL6qmgI+DXwmyUHgZabLrCRJkiRJb9F3UQWoqj3AnlnHbpix/RrwS4OYS5IkSZI02jr1YUqSJEmSJFlUJUmSJEmdYlGVJEmSJHWKRVWSJEmS1CkWVUmSJElSp1hUJUmSJEmdYlGVJEmSJHXKQIpqki1JnkhyMMmOHud/J8ljSR5JsjfJeYOYV5IkSZI0evouqkkWA7cClwGbgKuSbJo17CFgsqp+HNgN/HG/80qSJEmSRtMgrqheBBysqier6nXgLmDrzAFV9ZWqOtbs3g+sG8C8kiRJkqQRNIiiuhZ4Zsb+oebYXK4B/nkA80qSJEmSRtCShZwsyUeASeBn5ji/HdgOcO7KFQuYTJIkSZLUFYO4ovossH7G/rrm2Jsk2QxcD1xeVd/p9UZVtbOqJqtqcmL5sgFEkyRJkiQNm0EU1QeBjUnOT7IUuBKYmjkgyQXAXzNdUo8MYE5JkiRJ0ojqu6hW1XHgWuBe4HHg7qran+TGJJc3w/4EWA78fZKHk0zN8XaSJEmSpDE3kGdUq2oPsGfWsRtmbG8exDySJEmSpNE3iFt/JUmSJEkaGIuqJEmSJKlTLKqSJEmSpE6xqEqSJEmSOsWiKkmSJEnqFIuqJEmSJKlTLKqSJEmSpE4ZSFFNsiXJE0kOJtlxgnG/kKSSTA5iXkmSJEnS6Om7qCZZDNwKXAZsAq5KsqnHuNOB64AH+p1TkiRJkjS6BnFF9SLgYFU9WVWvA3cBW3uM+yTwKeC1AcwpSZIkSRpRgyiqa4FnZuwfao79vyQXAuur6gsDmE+SJEmSNMKWvN0TJFkE3Axc/QOM3Q5sBzh35Yq3N5gkSZIkqZMGcUX1WWD9jP11zbE3nA68F/jXJN8ELgamen2gUlXtrKrJqpqcWL5sANEkSZIkScNmEEX1QWBjkvOTLAWuBKbeOFlVr1TVqqraUFUbgPuBy6tq3wDmliRJkiSNmL6LalUdB64F7gUeB+6uqv1Jbkxyeb/vL0mSJEkaLwN5RrWq9gB7Zh27YY6xlwxiTkmSJEnSaBrErb+SJEmSJA2MRVWSJEmS1CkWVUmSJElSp6Sq2s7QU5KjwFNt55jHKuDFtkNo7LkO1RWuRXWB61Bd4DpUV3R9LZ5XVRO9TnS2qA6DJPuq6i3fBystJNehusK1qC5wHaoLXIfqimFei976K0mSJEnqFIuqJEmSJKlTLKr92dl2AAnXobrDtagucB2qC1yH6oqhXYs+oypJkiRJ6hSvqEqSJEmSOsWiKkmSJEnqFIuqJEmSJKlTLKqSJEmSpE6xqEqSJEmSOuX/AMrX/mRaLPQKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = 'Looks like rigged polls are back on the menu boys!'\n",
    "tokens = tokenizer(sent)\n",
    "output, attns = sent_pred(sent, model, vocab_dict, tokenizer, max_length, device, batch_size)\n",
    "softmax = torch.nn.Softmax(dim=0)\n",
    "pred_scores = softmax(torch.tensor(output[0]))\n",
    "print(f\"humorous : {pred_scores[0]*100:.2f}%, not : {pred_scores[1]*100:.2f}%\")\n",
    "attn = attns[0].sum(axis=0).cpu().detach().numpy()\n",
    "print(tokens)\n",
    "plt.matshow([attn[:len(tokens)]], cmap='Reds');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86143273, 2.0425642 , 0.61140704, 0.76564044, 0.7658711 ,\n",
       "       0.46230575, 0.24997136, 1.0366608 , 1.0505943 , 1.0172155 ,\n",
       "       0.39953932], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc1ab7f8df0>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(attn[:len(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_marvin)",
   "language": "python",
   "name": "env_marvin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
