{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Create Pseudo-Parallel Dataset with Style Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import sys, os\n",
    "sys.path.append('../paraphrase/')\n",
    "sys.path.append('../jointclassifier/')\n",
    "from paraphraser_args import ModelArguments as pma, DataTrainingArguments as pda, TrainingArguments as pta\n",
    "from paraphraser_dataloader import load_dataset as pld, load_dataset_style as lds\n",
    "from paraphraser_trainer import ParaphraserTrainer\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelWithLMHead, HfArgumentParser\n",
    "from joint_args import ModelArguments as jma, DataTrainingArguments as jda, TrainingArguments as jta\n",
    "from joint_dataloader import load_dataset as jld\n",
    "from joint_trainer import JointTrainer\n",
    "from joint_model_v1 import JointSeqClassifier\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch import cuda, no_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in desired dataset and paraphraser model\n",
    "In the cell below, define the dataset you want to work with and the paraphraser model (here a `\"t5-small\"` [from Hugging Face](https://huggingface.co/t5-small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/processed_filtered/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_model_name = \"t5_paraphrase\"\n",
    "paraphrase_task = 'emo'\n",
    "paraphrase_model_nick = \"t5_paraphrase\"\n",
    "paraphrase_model_type = 't5-small'\n",
    "output_dir = \"../models/\"\n",
    "epochs = \"3\"\n",
    "train_batch_size = \"16\"\n",
    "eval_batch_size = \"16\"\n",
    "save_log_steps = \"400\"\n",
    "\n",
    "parser = HfArgumentParser((pma, pda, pta))\n",
    "model_args_para, data_args_para, training_args_para = parser.parse_args_into_dataclasses([\n",
    "    \"--model_name_or_path\",\n",
    "    paraphrase_model_name,\n",
    "    \"--model_nick\",\n",
    "    paraphrase_model_nick,\n",
    "    \"--data_dir\",\n",
    "    data_dir,\n",
    "    \"--output_dir\",\n",
    "    os.path.join(output_dir, paraphrase_model_nick),\n",
    "    \"--cache_dir\",\n",
    "    os.path.join(output_dir,\"cache\"),\n",
    "    \"--overwrite_cache\",\n",
    "    \"--per_device_train_batch_size\",\n",
    "    train_batch_size,\n",
    "    \"--per_device_eval_batch_size\",\n",
    "    eval_batch_size,\n",
    "    \"--max_seq_len\",\n",
    "    \"64\",\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    \"1\",\n",
    "    \"--num_train_epochs\",\n",
    "    epochs,\n",
    "    \"--logging_steps\",\n",
    "    save_log_steps,\n",
    "    \"--save_steps\",\n",
    "    save_log_steps,\n",
    "    \"--data_parallel\",\n",
    "    \"True\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_task = \"formality+emo\"\n",
    "data_dir = \"../data/processed_filtered/\"\n",
    "joint_model_name = \"distilbert-base-uncased\"\n",
    "joint_model_nick = \"distilbert_uncased_2\"\n",
    "output_dir = \"../models/\"\n",
    "freeze_encoder = \"False\"\n",
    "skip_preclassifier = \"False\"\n",
    "train_jointly = \"True\"\n",
    "epochs = \"5\"\n",
    "train_batch_size = \"256\"\n",
    "eval_batch_size = \"512\"\n",
    "log_save_steps = \"200\"\n",
    "\n",
    "parser = HfArgumentParser((jma, jda, jta))\n",
    "model_args_joint, data_args_joint, training_args_joint = parser.parse_args_into_dataclasses([\n",
    "    \"--model_name_or_path\",\n",
    "    joint_model_name,\n",
    "    \"--model_nick\",\n",
    "    joint_model_nick,\n",
    "    \"--task\",\n",
    "    joint_task,\n",
    "    \"--data_dir\",\n",
    "    data_dir,\n",
    "    \"--output_dir\",\n",
    "    os.path.join(output_dir, joint_model_nick, joint_task, 'joint'),\n",
    "    \"--cache_dir\",\n",
    "    os.path.join(output_dir,\"cache\"),\n",
    "    \"--freeze_encoder\",\n",
    "    freeze_encoder,\n",
    "    \"--skip_preclassifier\",\n",
    "    skip_preclassifier,\n",
    "    \"--train_jointly\",\n",
    "    train_jointly,\n",
    "    \"--overwrite_cache\",\n",
    "    \"--per_device_train_batch_size\",\n",
    "    train_batch_size,\n",
    "    \"--per_device_eval_batch_size\",\n",
    "    eval_batch_size,\n",
    "    \"--max_seq_len\",\n",
    "    \"64\",\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    \"1\",\n",
    "    \"--num_train_epochs\",\n",
    "    epochs,\n",
    "    \"--logging_steps\",\n",
    "    log_save_steps,\n",
    "    \"--save_steps\",\n",
    "    log_save_steps\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the paraphraser tokenizer and dataset objects\n",
    "para_tokenizer = AutoTokenizer.from_pretrained(paraphrase_model_type, cache_dir=model_args_para.cache_dir,\n",
    "                                         model_max_length = data_args_para.max_seq_len)\n",
    "dataset = lds(data_dir, para_tokenizer,\n",
    "                            task=paraphrase_task, mode=\"train\", n_proc=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the paraphrase configuration defined above to create the model\n",
    "model = AutoModelWithLMHead.from_pretrained(os.path.join(output_dir, paraphrase_model_name))\n",
    "#training_args_para.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Paraphraser to Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SequentialSampler(dataset)\n",
    "dataloader = DataLoader(dataset, sampler=sampler, batch_size=16)\n",
    "\n",
    "num_return_sequences = 3\n",
    "\n",
    "device = (\"cuda\" if cuda.is_available() else \"cpu\") #and not self.args.no_cuda\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "predicted1 = []\n",
    "predicted2 = []\n",
    "predicted3 = []\n",
    "\n",
    "epoch_iterator = tqdm(dataloader, desc=\"Iteration\")\n",
    "with no_grad():\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)  # GPU or CPU\n",
    "        generated_outputs = model.generate(input_ids = batch[0], \n",
    "                                           attention_mask = batch[1], \n",
    "                                           max_length=70, \n",
    "                                           num_beams=9,\n",
    "                                           early_stopping=True,\n",
    "                                           encoder_no_repeat_ngram_size=5,\n",
    "                                           num_beam_groups=3,\n",
    "                                           diversity_penalty=0.5,\n",
    "                                           num_return_sequences=num_return_sequences)\n",
    "        paras = para_tokenizer.batch_decode(generated_outputs.detach().cpu().numpy(), \n",
    "                                                 skip_special_tokens=True)\n",
    "        predicted1 += paras[0::3]\n",
    "        predicted2 += paras[1::3]\n",
    "        predicted3 += paras[2::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store outputs to disk using in_filename as the original texts \n",
    "# and writing outputs to out_filename\n",
    "\n",
    "# If you want to do other parts of the dataset other than train, \n",
    "# set the mode in 'dataset' above to the desired mode and then rerun the paraphrase\n",
    "# and change these filenames to point to the slice of the data you want to use (dev, test, etc.)\n",
    "in_filename = 'train.csv'\n",
    "out_filename = 'train_paraphrased.csv'\n",
    "\n",
    "df_para = pd.DataFrame(data={'paraphrased1' : predicted1, \n",
    "                             'paraphrased2' : predicted2, \n",
    "                             'paraphrased3' : predicted3}) \n",
    "df = pd.read_csv(os.path.join(data_dir, paraphrase_task, in_filename), names =['text', 'label'])\n",
    "df['paraphrased1'] = df_para['paraphrased1']\n",
    "df['paraphrased2'] = df_para['paraphrased2']\n",
    "df['paraphrased3'] = df_para['paraphrased3']\n",
    "df.to_csv(os.path.join(data_dir, paraphrase_task, out_filename), \n",
    "               header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use classifier for Scoring\n",
    "This may cause GPU memory issues, so it's possible you may have to shutdown the kernel and restart without running the paraphraser first to run this next portion. If doing so, reload the df that was written to disk in several cells above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in desired dataset and classifier model\n",
    "In the cell below, define the dataset you want to work with and the classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(model_args_joint.model_name_or_path, \n",
    "                                          cache_dir=model_args_joint.cache_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args_joint.model_name_or_path, \n",
    "                                          cache_dir=model_args_joint.cache_dir,\n",
    "                                          model_max_length = data_args_joint.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data as expected by joint classifier\n",
    "tasks = data_args_joint.task.split('+')\n",
    "train_dataset, idx_to_classes = jld(data_args_joint.data_dir, \n",
    "                                             tokenizer, \n",
    "                                             model_name=model_args_joint.model_name_or_path, \n",
    "                           tasks=tasks, mode=\"train\", n_proc=6000)\n",
    "dev_dataset, _ = jld(data_args_joint.data_dir, \n",
    "                              tokenizer, \n",
    "                              model_name=model_args_joint.model_name_or_path, \n",
    "                              tasks=tasks, mode=\"dev\", n_proc=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dims = {task : 1 if len(list(idx_to_classes[task].keys())) == 2 else len(list(idx_to_classes[task].keys())) for task in idx_to_classes}\n",
    "label_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model = JointSeqClassifier.from_pretrained(os.path.join(output_dir,\n",
    "                                                              model_args_joint.model_nick, joint_task),\n",
    "                                           tasks=tasks,\n",
    "                                           model_args=model_args_joint,\n",
    "                                           task_if_single=None, \n",
    "                                           joint = training_args_joint.train_jointly,\n",
    "                                           label_dims=label_dims)\n",
    "\n",
    "trainer = JointTrainer([training_args_joint,model_args_joint, data_args_joint], \n",
    "                       joint_model, train_dataset, dev_dataset, idx_to_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classifier on paraphrased and original text\n",
    "\n",
    "This is currently done with pd DataFrames but could probably be made better by using a batch data loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_paraphrases(row, tasks, cols):\n",
    "    '''\n",
    "    Make style predictions on a given df row for a given set of text columns\n",
    "    and classification tasks. \n",
    "    '''\n",
    "    preds = {}\n",
    "    for col in cols:\n",
    "        sentence = row[col]\n",
    "        out = trainer.predict_for_sentence(sentence, tokenizer)\n",
    "        for task in tasks:\n",
    "            pred = float(out[task]['prob'])\n",
    "            preds[task + '_' + col] = pred\n",
    "    return preds\n",
    "\n",
    "def get_best_pred(row, cols, target_val=0.5):\n",
    "    '''\n",
    "    Helper funtion for determiningg which paraphrase is 'best' \n",
    "    for a given set of paraphrase column style scores and a target value\n",
    "    that you want the scores to be close to. Currently just outputs the best score\n",
    "    but could be modified to get best sentence as well.\n",
    "    '''\n",
    "    best_diff = 1\n",
    "    best_val = None\n",
    "    for col in cols:\n",
    "        diff = abs(row[col] - target_val)\n",
    "        if diff < best_diff:\n",
    "            best_val = row[col]\n",
    "            best_diff = diff\n",
    "    return best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns on which to run the classification\n",
    "cols_to_use = ['text','paraphrased1', 'paraphrased2', 'paraphrased3']\n",
    "# Define the names of the columns where the output scores will be stored\n",
    "cols_preds = ['pred_formality_orig', 'pred_emo_orig',\n",
    "              'pred_formality_paraphrased1', 'pred_emo_paraphrased1',\n",
    "              'pred_formality_paraphrased2', 'pred_emo_paraphrased2',\n",
    "              'pred_formality_paraphrased3', 'pred_emo_paraphrased3']\n",
    "# Store results into df\n",
    "df[cols_preds] = df.progress_apply(lambda x : pred_paraphrases(x, tasks, cols_to_use), \n",
    "                                   axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results of style classification:\n",
    "out_filename = paraphrase_task + '_train_cross_predict_paraphrases.csv'\n",
    "\n",
    "df.to_csv(os.path.join(data_dir, paraphrase_task, out_filename), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_marvin)",
   "language": "python",
   "name": "env_marvin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "70745de62fac122a8ca1204278c5668179019927655d931b4063a8c34fb0e461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
