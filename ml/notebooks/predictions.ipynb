{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Create Pseudo-Parallel Dataset with Style Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import sys, os\n",
    "sys.path.append('../paraphrase/')\n",
    "sys.path.append('../jointclassifier/')\n",
    "from paraphraser_args import ModelArguments as pma, DataTrainingArguments as pda, TrainingArguments as pta\n",
    "from paraphraser_dataloader import load_dataset as pld, load_dataset_style as lds\n",
    "from paraphraser_trainer import ParaphraserTrainer\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelWithLMHead, HfArgumentParser\n",
    "from joint_args import ModelArguments as jma, DataTrainingArguments as jda, TrainingArguments as jta\n",
    "from joint_dataloader import load_dataset as jld\n",
    "from joint_trainer import JointTrainer\n",
    "from joint_model_v1 import JointSeqClassifier\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch import cuda, no_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in desired dataset and paraphraser model\n",
    "In the cell below, define the dataset you want to work with and the paraphraser model (here a `\"t5-small\"` [from Hugging Face](https://huggingface.co/t5-small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/processed_filtered/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "paraphrase_model_name = \"t5_paraphrase\"\n",
    "paraphrase_task = 'wiki'\n",
    "paraphrase_model_nick = \"t5_paraphrase\"\n",
    "paraphrase_model_type = 't5-small'\n",
    "output_dir = \"../models/\"\n",
    "epochs = \"3\"\n",
    "train_batch_size = \"16\"\n",
    "eval_batch_size = \"16\"\n",
    "save_log_steps = \"400\"\n",
    "\n",
    "parser = HfArgumentParser((pma, pda, pta))\n",
    "model_args_para, data_args_para, training_args_para = parser.parse_args_into_dataclasses([\n",
    "    \"--model_name_or_path\",\n",
    "    paraphrase_model_name,\n",
    "    \"--model_nick\",\n",
    "    paraphrase_model_nick,\n",
    "    \"--data_dir\",\n",
    "    data_dir,\n",
    "    \"--output_dir\",\n",
    "    os.path.join(output_dir, paraphrase_model_nick),\n",
    "    \"--cache_dir\",\n",
    "    os.path.join(output_dir,\"cache\"),\n",
    "    \"--overwrite_cache\",\n",
    "    \"--per_device_train_batch_size\",\n",
    "    train_batch_size,\n",
    "    \"--per_device_eval_batch_size\",\n",
    "    eval_batch_size,\n",
    "    \"--max_seq_len\",\n",
    "    \"64\",\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    \"1\",\n",
    "    \"--num_train_epochs\",\n",
    "    epochs,\n",
    "    \"--logging_steps\",\n",
    "    save_log_steps,\n",
    "    \"--save_steps\",\n",
    "    save_log_steps,\n",
    "    \"--data_parallel\",\n",
    "    \"True\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "joint_task = \"wiki\"\n",
    "data_dir = \"../data/processed_filtered/\"\n",
    "joint_model_name = \"distilbert-base-uncased\"\n",
    "joint_model_nick = \"distilbert_uncased_2\"\n",
    "output_dir = \"../models/\"\n",
    "freeze_encoder = \"False\"\n",
    "skip_preclassifier = \"False\"\n",
    "train_jointly = \"True\"\n",
    "epochs = \"5\"\n",
    "train_batch_size = \"256\"\n",
    "eval_batch_size = \"512\"\n",
    "log_save_steps = \"200\"\n",
    "\n",
    "parser = HfArgumentParser((jma, jda, jta))\n",
    "model_args_joint, data_args_joint, training_args_joint = parser.parse_args_into_dataclasses([\n",
    "    \"--model_name_or_path\",\n",
    "    joint_model_name,\n",
    "    \"--model_nick\",\n",
    "    joint_model_nick,\n",
    "    \"--task\",\n",
    "    joint_task,\n",
    "    \"--data_dir\",\n",
    "    data_dir,\n",
    "    \"--output_dir\",\n",
    "    os.path.join(output_dir, joint_model_nick, joint_task, 'joint'),\n",
    "    \"--cache_dir\",\n",
    "    os.path.join(output_dir,\"cache\"),\n",
    "    \"--freeze_encoder\",\n",
    "    freeze_encoder,\n",
    "    \"--skip_preclassifier\",\n",
    "    skip_preclassifier,\n",
    "    \"--train_jointly\",\n",
    "    train_jointly,\n",
    "    \"--overwrite_cache\",\n",
    "    \"--per_device_train_batch_size\",\n",
    "    train_batch_size,\n",
    "    \"--per_device_eval_batch_size\",\n",
    "    eval_batch_size,\n",
    "    \"--max_seq_len\",\n",
    "    \"64\",\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    \"1\",\n",
    "    \"--num_train_epochs\",\n",
    "    epochs,\n",
    "    \"--logging_steps\",\n",
    "    log_save_steps,\n",
    "    \"--save_steps\",\n",
    "    log_save_steps\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at ../models/cache/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at ../models/cache/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at ../models/cache/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the paraphraser tokenizer and dataset objects\n",
    "para_tokenizer = AutoTokenizer.from_pretrained(paraphrase_model_type, cache_dir=model_args_para.cache_dir,\n",
    "                                         model_max_length = data_args_para.max_seq_len)\n",
    "dataset = lds(data_dir, para_tokenizer,\n",
    "                            task=paraphrase_task, mode=\"dev\", n_proc=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file ../models/t5_paraphrase/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ../models/t5_paraphrase/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ../models/t5_paraphrase.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Use the paraphrase configuration defined above to create the model\n",
    "model = AutoModelWithLMHead.from_pretrained(os.path.join(output_dir, paraphrase_model_name))\n",
    "#training_args_para.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Paraphraser to Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=326.0, style=ProgressStyle(description_wi…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e3abd91dc1b45019c0257628d5f6f0c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = SequentialSampler(dataset)\n",
    "dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)\n",
    "\n",
    "num_return_sequences = 3\n",
    "\n",
    "device = (\"cuda\" if cuda.is_available() else \"cpu\") #and not self.args.no_cuda\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "predicted1 = []\n",
    "predicted2 = []\n",
    "predicted3 = []\n",
    "\n",
    "epoch_iterator = tqdm(dataloader, desc=\"Iteration\")\n",
    "with no_grad():\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)  # GPU or CPU\n",
    "        generated_outputs = model.generate(input_ids = batch[0], \n",
    "                                           attention_mask = batch[1], \n",
    "                                           max_length=70, \n",
    "                                           num_beams=9,\n",
    "                                           early_stopping=True,\n",
    "                                           encoder_no_repeat_ngram_size=5,\n",
    "                                           num_beam_groups=3,\n",
    "                                           diversity_penalty=0.5,\n",
    "                                           num_return_sequences=num_return_sequences)\n",
    "        paras = para_tokenizer.batch_decode(generated_outputs.detach().cpu().numpy(), \n",
    "                                                 skip_special_tokens=True)\n",
    "        predicted1 += paras[0::3]\n",
    "        predicted2 += paras[1::3]\n",
    "        predicted3 += paras[2::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store outputs to disk using in_filename as the original texts \n",
    "# and writing outputs to out_filename\n",
    "\n",
    "# If you want to do other parts of the dataset other than train, \n",
    "# set the mode in 'dataset' above to the desired mode and then rerun the paraphrase\n",
    "# and change these filenames to point to the slice of the data you want to use (dev, test, etc.)\n",
    "\n",
    "# in_filename = 'train.csv'\n",
    "# out_filename = 'train_paraphrased.csv'\n",
    "\n",
    "in_filename = 'dev.csv'\n",
    "out_filename = 'dev_paraphrased.csv'\n",
    "\n",
    "df_para = pd.DataFrame(data={'paraphrased1' : predicted1, \n",
    "                             'paraphrased2' : predicted2, \n",
    "                             'paraphrased3' : predicted3}) \n",
    "df = pd.read_csv(os.path.join(data_dir, paraphrase_task, in_filename), names =['text', 'label'])\n",
    "df['paraphrased1'] = df_para['paraphrased1']\n",
    "df['paraphrased2'] = df_para['paraphrased2']\n",
    "df['paraphrased3'] = df_para['paraphrased3']\n",
    "df.to_csv(os.path.join(data_dir, paraphrase_task, out_filename), \n",
    "               header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  label  \\\n",
       "0  The main goal of this inductive transfer mecha...      1   \n",
       "1  In this paper we propose an energy-efficient l...      1   \n",
       "2  In this work, we propose a method of deep arti...      1   \n",
       "3  To achieve this goal, we treat images as bags ...      1   \n",
       "4  The study provides some guidelines: (1) alteri...      1   \n",
       "\n",
       "                                        paraphrased1  \\\n",
       "0  in order to improve the generalization of the ...   \n",
       "1  in this paper, we propose an energy efficient ...   \n",
       "2  we're proposing a method for deep artificial n...   \n",
       "3  the integration of weakly monitored multiple l...   \n",
       "4  (1) it is not always better to change the geom...   \n",
       "\n",
       "                                        paraphrased2  \\\n",
       "0  in order to improve the generalization of the ...   \n",
       "1  we propose a system of energy-efficient learni...   \n",
       "2  we're proposing a method for deep artificial n...   \n",
       "3  the integration of weakly monitored multiple l...   \n",
       "4  (1) it is not always better to change the geom...   \n",
       "\n",
       "                                        paraphrased3  \n",
       "0  in order to improve the generalization of the ...  \n",
       "1  we propose a system of energy-efficient learni...  \n",
       "2  we propose a technique for deep artificial neu...  \n",
       "3  the integration of weakly monitored multiple l...  \n",
       "4  (1) it is not always better to change the geom...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>paraphrased1</th>\n      <th>paraphrased2</th>\n      <th>paraphrased3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The main goal of this inductive transfer mecha...</td>\n      <td>1</td>\n      <td>in order to improve the generalization of the ...</td>\n      <td>in order to improve the generalization of the ...</td>\n      <td>in order to improve the generalization of the ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In this paper we propose an energy-efficient l...</td>\n      <td>1</td>\n      <td>in this paper, we propose an energy efficient ...</td>\n      <td>we propose a system of energy-efficient learni...</td>\n      <td>we propose a system of energy-efficient learni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>In this work, we propose a method of deep arti...</td>\n      <td>1</td>\n      <td>we're proposing a method for deep artificial n...</td>\n      <td>we're proposing a method for deep artificial n...</td>\n      <td>we propose a technique for deep artificial neu...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>To achieve this goal, we treat images as bags ...</td>\n      <td>1</td>\n      <td>the integration of weakly monitored multiple l...</td>\n      <td>the integration of weakly monitored multiple l...</td>\n      <td>the integration of weakly monitored multiple l...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The study provides some guidelines: (1) alteri...</td>\n      <td>1</td>\n      <td>(1) it is not always better to change the geom...</td>\n      <td>(1) it is not always better to change the geom...</td>\n      <td>(1) it is not always better to change the geom...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Inspect some results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text  label  \\\n",
       "10407  This is analogous to the F-test used in linear...      1   \n",
       "10408  Nonlinear models for binary dependent variable...      1   \n",
       "10409  In particular, Advanced Driver Assistance Syst...      1   \n",
       "10410  We prove that the proposed algorithm converges...      1   \n",
       "10411  Toward the end of the 1990s, a significant cha...      1   \n",
       "\n",
       "                                            paraphrased1  \\\n",
       "10407  this is similar to the F test used in linear r...   \n",
       "10408  the probit model and logit model are nonlinear...   \n",
       "10409  in particular, ML is a significant role in adv...   \n",
       "10410  if a minimum reward machine is inferred, the m...   \n",
       "10411  the increased interaction between computer gra...   \n",
       "\n",
       "                                            paraphrased2  \\\n",
       "10407  this is an analogy to the F test used for line...   \n",
       "10408  the probit model and logit model are nonlinear...   \n",
       "10409  in particular, ML is a significant role in adv...   \n",
       "10410  if a minimum reward machine is inferred, the m...   \n",
       "10411  the increased interaction between computer gra...   \n",
       "\n",
       "                                            paraphrased3  \n",
       "10407  the significance of prediction is similar to t...  \n",
       "10408  the probit model and logit model are non-linea...  \n",
       "10409  ML is particularly important in the two areas ...  \n",
       "10410  if a minimum reward machine is inferred, the m...  \n",
       "10411  the increase in the interaction between comput...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>paraphrased1</th>\n      <th>paraphrased2</th>\n      <th>paraphrased3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10407</th>\n      <td>This is analogous to the F-test used in linear...</td>\n      <td>1</td>\n      <td>this is similar to the F test used in linear r...</td>\n      <td>this is an analogy to the F test used for line...</td>\n      <td>the significance of prediction is similar to t...</td>\n    </tr>\n    <tr>\n      <th>10408</th>\n      <td>Nonlinear models for binary dependent variable...</td>\n      <td>1</td>\n      <td>the probit model and logit model are nonlinear...</td>\n      <td>the probit model and logit model are nonlinear...</td>\n      <td>the probit model and logit model are non-linea...</td>\n    </tr>\n    <tr>\n      <th>10409</th>\n      <td>In particular, Advanced Driver Assistance Syst...</td>\n      <td>1</td>\n      <td>in particular, ML is a significant role in adv...</td>\n      <td>in particular, ML is a significant role in adv...</td>\n      <td>ML is particularly important in the two areas ...</td>\n    </tr>\n    <tr>\n      <th>10410</th>\n      <td>We prove that the proposed algorithm converges...</td>\n      <td>1</td>\n      <td>if a minimum reward machine is inferred, the m...</td>\n      <td>if a minimum reward machine is inferred, the m...</td>\n      <td>if a minimum reward machine is inferred, the m...</td>\n    </tr>\n    <tr>\n      <th>10411</th>\n      <td>Toward the end of the 1990s, a significant cha...</td>\n      <td>1</td>\n      <td>the increased interaction between computer gra...</td>\n      <td>the increased interaction between computer gra...</td>\n      <td>the increase in the interaction between comput...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use classifier for Scoring\n",
    "This may cause GPU memory issues, so it's possible you may have to shutdown the kernel and restart without running the paraphraser first to run this next portion. If doing so, reload the df that was written to disk in several cells above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in desired dataset and classifier model\n",
    "In the cell below, define the dataset you want to work with and the classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at ../models/cache/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at ../models/cache/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at ../models/cache/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at ../models/cache/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at ../models/cache/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(model_args_joint.model_name_or_path, \n",
    "                                          cache_dir=model_args_joint.cache_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args_joint.model_name_or_path, \n",
    "                                          cache_dir=model_args_joint.cache_dir,\n",
    "                                          model_max_length = data_args_joint.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  7.15it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 15.66it/s]torch.Size([34302, 64]) torch.Size([34302, 64]) torch.Size([34302, 1]) torch.Size([34302])\n",
      "torch.Size([7759, 64]) torch.Size([7759, 64]) torch.Size([7759, 1]) torch.Size([7759])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data as expected by joint classifier\n",
    "tasks = data_args_joint.task.split('+')\n",
    "train_dataset, idx_to_classes = jld(data_args_joint.data_dir, \n",
    "                                             tokenizer, \n",
    "                                             model_name=model_args_joint.model_name_or_path, \n",
    "                           tasks=tasks, mode=\"train\", n_proc=6000)\n",
    "dev_dataset, _ = jld(data_args_joint.data_dir, \n",
    "                              tokenizer, \n",
    "                              model_name=model_args_joint.model_name_or_path, \n",
    "                              tasks=tasks, mode=\"dev\", n_proc=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'shakespeare': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "label_dims = {task : 1 if len(list(idx_to_classes[task].keys())) == 2 else len(list(idx_to_classes[task].keys())) for task in idx_to_classes}\n",
    "label_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file ../models/distilbert_uncased_2/shakespeare/joint/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"JointSeqClassifier\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../models/distilbert_uncased_2/shakespeare/joint/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing JointSeqClassifier.\n",
      "\n",
      "All the weights of JointSeqClassifier were initialized from the model checkpoint at ../models/distilbert_uncased_2/shakespeare/joint.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use JointSeqClassifier for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "joint_model = JointSeqClassifier.from_pretrained(os.path.join(output_dir,\n",
    "                                                              model_args_joint.model_nick, joint_task,'joint'),\n",
    "                                           tasks=tasks,\n",
    "                                           model_args=model_args_joint,\n",
    "                                           task_if_single=None, \n",
    "                                           joint = training_args_joint.train_jointly,\n",
    "                                           label_dims=label_dims)\n",
    "\n",
    "trainer = JointTrainer([training_args_joint,model_args_joint, data_args_joint], \n",
    "                       joint_model, train_dataset, dev_dataset, idx_to_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classifier on paraphrased and original text\n",
    "\n",
    "This is currently done with pd DataFrames but could probably be made better by using a batch data loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['shakespeare']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_paraphrases(row, tasks, cols):\n",
    "    '''\n",
    "    Make style predictions on a given df row for a given set of text columns\n",
    "    and classification tasks. \n",
    "    '''\n",
    "    preds = {}\n",
    "    for col in cols:\n",
    "        sentence = row[col]\n",
    "        out = trainer.predict_for_sentence(sentence, tokenizer)\n",
    "        for task in tasks:\n",
    "            pred = float(out[task]['prob'])\n",
    "            preds[task + '_' + col] = pred\n",
    "    return preds\n",
    "\n",
    "def get_best_pred(row, cols, target_val=0.5):\n",
    "    '''\n",
    "    Helper funtion for determiningg which paraphrase is 'best' \n",
    "    for a given set of paraphrase column style scores and a target value\n",
    "    that you want the scores to be close to. Currently just outputs the best score\n",
    "    but could be modified to get best sentence as well.\n",
    "    '''\n",
    "    best_diff = 1\n",
    "    best_val = None\n",
    "    for col in cols:\n",
    "        diff = abs(row[col] - target_val)\n",
    "        if diff < best_diff:\n",
    "            best_val = row[col]\n",
    "            best_diff = diff\n",
    "    return best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the file with the paraphrases\n",
    "# joint_task1 = 'abstract'\n",
    "joint_task2 = 'shakespeare'\n",
    "\n",
    "paraphrase_task = 'shakespeare'\n",
    "filename = 'dev_paraphrased.csv'\n",
    "# filename = 'dev_paraphrased.csv'\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, paraphrase_task, filename), header=None)\n",
    "df.columns = ['text','label', 'paraphrased1', 'paraphrased2', 'paraphrased3']\n",
    "\n",
    "df = df[df['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 text  label  \\\n",
       "1          But thus, I trust, you will not marry her.      1   \n",
       "5                              Stand from the hearse.      1   \n",
       "6   I have no will to wander forth of doors, Yet s...      1   \n",
       "9                   How do you mean, removing of him?      1   \n",
       "11  O Thou, whose captain I account myself, Look o...      1   \n",
       "\n",
       "                                         paraphrased1  \\\n",
       "1                       I'm sure you won't marry her.   \n",
       "5                       stand in front of the hearse!   \n",
       "6   I'm not going to walk out of the door, but som...   \n",
       "9                       how do you mean removing him?   \n",
       "11  O Thou, I'm a captain, and I'm a gracious eye ...   \n",
       "\n",
       "                                         paraphrased2  \\\n",
       "1                   but I'm sure you won't marry her.   \n",
       "5                            stand out of the hearse!   \n",
       "6   I don't want to go out of the door, but someth...   \n",
       "9                       what do you mean, remove him?   \n",
       "11  O Thou, I'm a captain, and I'm a gracious eye ...   \n",
       "\n",
       "                                         paraphrased3  \n",
       "1                    so I'm sure you won't marry her.  \n",
       "5                                stand by the hearse!  \n",
       "6   I'm not going to go out of the door, but somet...  \n",
       "9                       how do you mean he's removed?  \n",
       "11     O Thou, I'm a captain, and I'm a gracious man.  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>paraphrased1</th>\n      <th>paraphrased2</th>\n      <th>paraphrased3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>But thus, I trust, you will not marry her.</td>\n      <td>1</td>\n      <td>I'm sure you won't marry her.</td>\n      <td>but I'm sure you won't marry her.</td>\n      <td>so I'm sure you won't marry her.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Stand from the hearse.</td>\n      <td>1</td>\n      <td>stand in front of the hearse!</td>\n      <td>stand out of the hearse!</td>\n      <td>stand by the hearse!</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>I have no will to wander forth of doors, Yet s...</td>\n      <td>1</td>\n      <td>I'm not going to walk out of the door, but som...</td>\n      <td>I don't want to go out of the door, but someth...</td>\n      <td>I'm not going to go out of the door, but somet...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>How do you mean, removing of him?</td>\n      <td>1</td>\n      <td>how do you mean removing him?</td>\n      <td>what do you mean, remove him?</td>\n      <td>how do you mean he's removed?</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>O Thou, whose captain I account myself, Look o...</td>\n      <td>1</td>\n      <td>O Thou, I'm a captain, and I'm a gracious eye ...</td>\n      <td>O Thou, I'm a captain, and I'm a gracious eye ...</td>\n      <td>O Thou, I'm a captain, and I'm a gracious man.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3151/3151 [01:12<00:00, 43.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define columns on which to run the classification\n",
    "cols_to_use = ['text','paraphrased1', 'paraphrased2', 'paraphrased3']\n",
    "# Define the names of the columns where the output scores will be stored\n",
    "cols_preds = [f'pred_{joint_task2}_orig',\n",
    "            #   f'pred_{joint_task1}_paraphrased1', f'pred_{joint_task2}_paraphrased1',\n",
    "            #   f'pred_{joint_task1}_paraphrased2', f'pred_{joint_task2}_paraphrased2',\n",
    "            #   f'pred_{joint_task1}_paraphrased3', f'pred_{joint_task2}_paraphrased3']\n",
    "            f'pred_{joint_task2}_paraphrased1',\n",
    "             f'pred_{joint_task2}_paraphrased2',\n",
    "             f'pred_{joint_task2}_paraphrased3']\n",
    "# Store results into df\n",
    "df[cols_preds] = df.progress_apply(lambda x : pred_paraphrases(x, tasks, cols_to_use), \n",
    "                                   axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results of style classification:\n",
    "out_filename = paraphrase_task + '_dev_cross_predict_paraphrases.csv'\n",
    "\n",
    "df.to_csv(os.path.join(data_dir, paraphrase_task, out_filename), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                       1   \\\n",
       "text                           But thus, I trust, you will not marry her.   \n",
       "label                                                                   1   \n",
       "paraphrased1                                I'm sure you won't marry her.   \n",
       "paraphrased2                            but I'm sure you won't marry her.   \n",
       "paraphrased3                             so I'm sure you won't marry her.   \n",
       "pred_shakespeare_orig                                            0.821566   \n",
       "pred_shakespeare_paraphrased1                                    0.004811   \n",
       "pred_shakespeare_paraphrased2                                     0.00474   \n",
       "pred_shakespeare_paraphrased3                                    0.004765   \n",
       "\n",
       "                                                          5   \\\n",
       "text                                  Stand from the hearse.   \n",
       "label                                                      1   \n",
       "paraphrased1                   stand in front of the hearse!   \n",
       "paraphrased2                        stand out of the hearse!   \n",
       "paraphrased3                            stand by the hearse!   \n",
       "pred_shakespeare_orig                               0.631334   \n",
       "pred_shakespeare_paraphrased1                       0.019826   \n",
       "pred_shakespeare_paraphrased2                       0.209268   \n",
       "pred_shakespeare_paraphrased3                        0.15968   \n",
       "\n",
       "                                                                              6   \\\n",
       "text                           I have no will to wander forth of doors, Yet s...   \n",
       "label                                                                          1   \n",
       "paraphrased1                   I'm not going to walk out of the door, but som...   \n",
       "paraphrased2                   I don't want to go out of the door, but someth...   \n",
       "paraphrased3                   I'm not going to go out of the door, but somet...   \n",
       "pred_shakespeare_orig                                                    0.98707   \n",
       "pred_shakespeare_paraphrased1                                           0.005536   \n",
       "pred_shakespeare_paraphrased2                                           0.005621   \n",
       "pred_shakespeare_paraphrased3                                           0.005888   \n",
       "\n",
       "                                                              9   \\\n",
       "text                           How do you mean, removing of him?   \n",
       "label                                                          1   \n",
       "paraphrased1                       how do you mean removing him?   \n",
       "paraphrased2                       what do you mean, remove him?   \n",
       "paraphrased3                       how do you mean he's removed?   \n",
       "pred_shakespeare_orig                                   0.960118   \n",
       "pred_shakespeare_paraphrased1                           0.185102   \n",
       "pred_shakespeare_paraphrased2                           0.267502   \n",
       "pred_shakespeare_paraphrased3                           0.115327   \n",
       "\n",
       "                                                                              11  \n",
       "text                           O Thou, whose captain I account myself, Look o...  \n",
       "label                                                                          1  \n",
       "paraphrased1                   O Thou, I'm a captain, and I'm a gracious eye ...  \n",
       "paraphrased2                   O Thou, I'm a captain, and I'm a gracious eye ...  \n",
       "paraphrased3                      O Thou, I'm a captain, and I'm a gracious man.  \n",
       "pred_shakespeare_orig                                                   0.993091  \n",
       "pred_shakespeare_paraphrased1                                           0.990232  \n",
       "pred_shakespeare_paraphrased2                                           0.990249  \n",
       "pred_shakespeare_paraphrased3                                             0.9904  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>5</th>\n      <th>6</th>\n      <th>9</th>\n      <th>11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>text</th>\n      <td>But thus, I trust, you will not marry her.</td>\n      <td>Stand from the hearse.</td>\n      <td>I have no will to wander forth of doors, Yet s...</td>\n      <td>How do you mean, removing of him?</td>\n      <td>O Thou, whose captain I account myself, Look o...</td>\n    </tr>\n    <tr>\n      <th>label</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>paraphrased1</th>\n      <td>I'm sure you won't marry her.</td>\n      <td>stand in front of the hearse!</td>\n      <td>I'm not going to walk out of the door, but som...</td>\n      <td>how do you mean removing him?</td>\n      <td>O Thou, I'm a captain, and I'm a gracious eye ...</td>\n    </tr>\n    <tr>\n      <th>paraphrased2</th>\n      <td>but I'm sure you won't marry her.</td>\n      <td>stand out of the hearse!</td>\n      <td>I don't want to go out of the door, but someth...</td>\n      <td>what do you mean, remove him?</td>\n      <td>O Thou, I'm a captain, and I'm a gracious eye ...</td>\n    </tr>\n    <tr>\n      <th>paraphrased3</th>\n      <td>so I'm sure you won't marry her.</td>\n      <td>stand by the hearse!</td>\n      <td>I'm not going to go out of the door, but somet...</td>\n      <td>how do you mean he's removed?</td>\n      <td>O Thou, I'm a captain, and I'm a gracious man.</td>\n    </tr>\n    <tr>\n      <th>pred_shakespeare_orig</th>\n      <td>0.821566</td>\n      <td>0.631334</td>\n      <td>0.98707</td>\n      <td>0.960118</td>\n      <td>0.993091</td>\n    </tr>\n    <tr>\n      <th>pred_shakespeare_paraphrased1</th>\n      <td>0.004811</td>\n      <td>0.019826</td>\n      <td>0.005536</td>\n      <td>0.185102</td>\n      <td>0.990232</td>\n    </tr>\n    <tr>\n      <th>pred_shakespeare_paraphrased2</th>\n      <td>0.00474</td>\n      <td>0.209268</td>\n      <td>0.005621</td>\n      <td>0.267502</td>\n      <td>0.990249</td>\n    </tr>\n    <tr>\n      <th>pred_shakespeare_paraphrased3</th>\n      <td>0.004765</td>\n      <td>0.15968</td>\n      <td>0.005888</td>\n      <td>0.115327</td>\n      <td>0.9904</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd098037a696054ba6333485ba1eda7d4b13de5ba8596b9581751e7239af6bf3f61",
   "display_name": "Python 3.8.8 64-bit ('marvin': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "70745de62fac122a8ca1204278c5668179019927655d931b4063a8c34fb0e461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}