{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "starting-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-sheep",
   "metadata": {},
   "source": [
    "## 1. Formality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "presidential-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formality dataset (GYAFC)\n",
    "data_dir = '../data/GYAFC_Corpus'\n",
    "output_dir = '../data/processed/formality'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "entertainment = f\"{data_dir}/Entertainment_Music\"\n",
    "family = f\"{data_dir}/Family_Relationships\"\n",
    "\n",
    "train_sent = []\n",
    "train_labels = []\n",
    "dev_sent = []\n",
    "dev_labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "invisible-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_ in [entertainment, family]:\n",
    "    for l, label in enumerate(['informal', 'formal']):\n",
    "        with open(f\"{dir_}/train/{label}\",\"r\") as fob:\n",
    "            temp = fob.readlines()\n",
    "            train_sent += temp\n",
    "            train_labels += ([l] * len(temp))\n",
    "        with open(f\"{dir_}/test/{label}\",\"r\") as fob:\n",
    "            temp = fob.readlines()\n",
    "            dev_sent += temp\n",
    "            dev_labels += ([l] * len(temp))\n",
    "            \n",
    "train_sent = [x.strip() for x in train_sent]\n",
    "dev_sent = [x.strip() for x in dev_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "czech-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'sentence': train_sent, 'label': train_labels})\n",
    "dev_df = pd.DataFrame({'sentence': dev_sent, 'label': dev_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "isolated-haiti",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (209124, 3), original dev size : (4849, 3)\n",
      "filtered train size : (207366, 3), filtered dev size : (4803, 3)\n",
      "shuffled train size : (169735, 2), shuffled dev size : (42434, 2)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])\n",
    "\n",
    "#mix train and dev, and reseparate them based on train: 80% and dev 20%\n",
    "total_df = pd.concat([train_df,dev_df])\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(total_df) *  0.8)\n",
    "dev_samples = len(total_df) - train_samples\n",
    "\n",
    "dev_df = total_df.tail(dev_samples)\n",
    "train_df = total_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "basic-tractor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(IE: Seeing #2 without #1 knowing.)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yea its Elton The FAG John there ya go !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My Java teacher is dumb and crazy.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What the hell is wrong with you?!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have fun finding out because I don't know the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0                (IE: Seeing #2 without #1 knowing.)      0\n",
       "1           Yea its Elton The FAG John there ya go !      0\n",
       "2                 My Java teacher is dumb and crazy.      1\n",
       "3                  What the hell is wrong with you?!      0\n",
       "4  Have fun finding out because I don't know the ...      1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "derived-order",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Perc :  label\n",
      "0    0.505335\n",
      "1    0.494665\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.507282\n",
      "1    0.492718\n",
      "dtype: float64 \n",
      "\n",
      "Train Split Perc :  label\n",
      "0    0.496\n",
      "1    0.504\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.54\n",
      "1    0.46\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('label').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('label').size()/len(dev_df),'\\n')\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.head(1000).groupby('label').size()/1000,'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.head(200).groupby('label').size()/200,'\\n')\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fluid-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"formality\",\n",
    "    \"description\" : \"Derived from the GYAFC Corpus\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"informal\",\n",
    "        1 : \"formal\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from the GYAFC Corpus; Toy dataset\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"informal\",\n",
    "        1 : \"formal\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-expression",
   "metadata": {},
   "source": [
    "## 2. Short Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "meaningful-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/ShortJokeKaggle/'\n",
    "output_dir = '../data/processed/jokes'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "\n",
    "train_df = pd.read_csv(f\"{data_dir}/train.tsv\", sep=\"\\t\", header=None)\n",
    "dev_df = pd.read_csv(f\"{data_dir}/dev.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "train_df.columns = ['idx', 'source', 'label', 'sentence']\n",
    "dev_df.columns = ['idx', 'source', 'label', 'sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "oriented-checklist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (406682, 5), original dev size : (22512, 5)\n",
      "filtered train size : (357062, 5), filtered dev size : (19797, 5)\n",
      "shuffled train size : (301487, 2), shuffled dev size : (75372, 2)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])\n",
    "\n",
    "\n",
    "#mix train and dev, and reseparate them based on train: 80% and dev 20%\n",
    "total_df = pd.concat([train_df,dev_df])\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(total_df) *  0.8)\n",
    "dev_samples = len(total_df) - train_samples\n",
    "\n",
    "dev_df = total_df.tail(dev_samples)\n",
    "train_df = total_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "egyptian-tucson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>haha, exactly what ive been thinking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usually security guards patrol the grounds at ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insomnia sufferers, look on the bright side. o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have never once hit a drink or treated one b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Outvoted 1-1 by my wife again.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0               haha, exactly what ive been thinking      0\n",
       "1  usually security guards patrol the grounds at ...      0\n",
       "2  Insomnia sufferers, look on the bright side. o...      1\n",
       "3  I have never once hit a drink or treated one b...      1\n",
       "4                     Outvoted 1-1 by my wife again.      1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "virtual-amateur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Perc :  label\n",
      "0    0.433488\n",
      "1    0.566512\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.434339\n",
      "1    0.565661\n",
      "dtype: float64 \n",
      "\n",
      "Train Split Perc :  label\n",
      "0    0.427\n",
      "1    0.573\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.435\n",
      "1    0.565\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('label').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('label').size()/len(dev_df),'\\n')\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.head(1000).groupby('label').size()/1000,'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.head(200).groupby('label').size()/200,'\\n')\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "flying-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"jokes\",\n",
    "    \"description\" : \"Derived from SARC, shortjokes.csv, BiasSum\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nojoke\",\n",
    "        1 : \"joke\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from SARC, shortjokes.csv, BiasSum; Toy dataset\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nojoke\",\n",
    "        1 : \"joke\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-handle",
   "metadata": {},
   "source": [
    "## 3. Metaphor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "chief-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/VUA/'\n",
    "output_dir = '../data/processed/metaphor'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "\n",
    "train_df = pd.read_csv(f\"{data_dir}/train.tsv\", sep=\"\\t\", header=None)\n",
    "dev_df = pd.read_csv(f\"{data_dir}/dev.tsv\", sep=\"\\t\", header=None)\n",
    "test_df = pd.read_csv(f\"{data_dir}/test.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "empty-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns = ['source', 'sentence', 'label']\n",
    "dev_df.columns = ['source', 'sentence', 'label']\n",
    "test_df.columns = ['source', 'sentence', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "obvious-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.concat([dev_df,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cordless-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (15157, 4), original dev size : (7511, 4)\n",
      "filtered train size : (14484, 4), filtered dev size : (7061, 4)\n",
      "shuffled train size : (17236, 2), shuffled dev size : (4309, 2)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])\n",
    "\n",
    "\n",
    "#mix train and dev, and reseparate them based on train: 80% and dev 20%\n",
    "total_df = pd.concat([train_df,dev_df])\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(total_df) *  0.8)\n",
    "dev_samples = len(total_df) - train_samples\n",
    "\n",
    "dev_df = total_df.tail(dev_samples)\n",
    "train_df = total_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "combined-ribbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As his eyes focused he realized he was looking...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The increase will not be matched by dividend r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If the complaint is proved , a nuisance order ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let me chop you that much , you eat up that let</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Workers in blue overalls drifted around us and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  As his eyes focused he realized he was looking...      1\n",
       "1  The increase will not be matched by dividend r...      1\n",
       "2  If the complaint is proved , a nuisance order ...      0\n",
       "3    Let me chop you that much , you eat up that let      0\n",
       "4  Workers in blue overalls drifted around us and...      1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "marine-illness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Perc :  label\n",
      "0    0.715653\n",
      "1    0.284347\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.707357\n",
      "1    0.292643\n",
      "dtype: float64 \n",
      "\n",
      "Train Split Perc :  label\n",
      "0    0.705\n",
      "1    0.295\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.71\n",
      "1    0.29\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('label').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('label').size()/len(dev_df),'\\n')\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.head(1000).groupby('label').size()/1000,'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.head(200).groupby('label').size()/200,'\\n')\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "standard-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"jokes\",\n",
    "    \"description\" : \"Derived from VUA\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nometaphor\",\n",
    "        1 : \"metaphor\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from VUA\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nometaphor\",\n",
    "        1 : \"metaphor\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "source": [
    "## 4. Abstracts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_path1 = '../data/temp/abstract_sents.csv'\n",
    "abstract_path2 = '../data/temp/wikis_ml.csv'\n",
    "abstract_train_path = '../data/processed/abstract/train.csv'\n",
    "abstract_dev_path = '../data/processed/abstract/dev.csv'\n",
    "abstract_config_path = '../data/processed/abstract/config.json'\n",
    "output_dir = '../data/processed/abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "57116\n80212\n"
     ]
    }
   ],
   "source": [
    "with open(abstract_path1) as fob:\n",
    "    abs_data = fob.readlines()\n",
    "print(len(abs_data))\n",
    "\n",
    "with open(abstract_path2) as fob:\n",
    "    wiki_data = fob.readlines()\n",
    "print(len(wiki_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_data = [x.strip().strip('\\\"').strip() for x in abs_data]\n",
    "abs_df = pd.DataFrame({'abs':abs_data, 'class': 1})\n",
    "wiki_data = [x.strip().strip('\\\"').strip() for x in wiki_data]\n",
    "wiki_df = pd.DataFrame({'abs':wiki_data, 'class': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                     abs  class\n",
       "0      38% are observed for OSIRIS and IriCore, respe...      1\n",
       "1      A smartphone with a mobile app is connected to...      1\n",
       "2      Good scalability is achieved through flexible ...      1\n",
       "3      This problem is further compounded due to the ...      1\n",
       "4      This deep learning based technique is shown to...      1\n",
       "...                                                  ...    ...\n",
       "80207  To arrive at the multinomial logit model, one ...      0\n",
       "80208  However, the premises of the permutation model...      0\n",
       "80209  2  approaches zero, this limit reduces to \"not...      0\n",
       "80210  Co-training is a semi-supervised learning tech...      0\n",
       "80211  CAPs describe potentially causal connections b...      0\n",
       "\n",
       "[137328 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>abs</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>38% are observed for OSIRIS and IriCore, respe...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A smartphone with a mobile app is connected to...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Good scalability is achieved through flexible ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>This problem is further compounded due to the ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>This deep learning based technique is shown to...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>80207</th>\n      <td>To arrive at the multinomial logit model, one ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>80208</th>\n      <td>However, the premises of the permutation model...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>80209</th>\n      <td>2  approaches zero, this limit reduces to \"not...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>80210</th>\n      <td>Co-training is a semi-supervised learning tech...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>80211</th>\n      <td>CAPs describe potentially causal connections b...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>137328 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "abs_df = pd.concat([abs_df,wiki_df], axis=0)\n",
    "abs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shuffled train size : (109862, 2), shuffled dev size : (27466, 2)\n"
     ]
    }
   ],
   "source": [
    "abs_df = abs_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(abs_df) *  0.8)\n",
    "dev_samples = len(abs_df) - train_samples\n",
    "\n",
    "dev_df = abs_df.tail(dev_samples)\n",
    "train_df = abs_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Split Perc :  class\n0    0.58416\n1    0.41584\ndtype: float64 \n\nDev Split Perc :  class\n0    0.583813\n1    0.416187\ndtype: float64 \n\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(abstract_train_path, index=False, header=False)\n",
    "dev_df.to_csv(abstract_dev_path, index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('class').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('class').size()/len(dev_df),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"abstract\",\n",
    "    \"description\" : \"Derived from Abstracts of papers\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"wiki\",\n",
    "        1 : \"abstract\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "source": [
    "## 5. shakespeare"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shake_path = '../data/temp/shake_original.txt'\n",
    "shake_mod_path = '../data/temp/shake_modern.txt'\n",
    "shake_train_path = '../data/processed/shakespeare/train.csv'\n",
    "shake_dev_path = '../data/processed/shakespeare/dev.csv'\n",
    "shake_config_path = '../data/processed/shakespeare/config.json'\n",
    "output_dir = '../data/processed/shakespeare'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "28239\n28239\n"
     ]
    }
   ],
   "source": [
    "with open(shake_path) as fob:\n",
    "    shake = fob.readlines()\n",
    "print(len(shake))\n",
    "\n",
    "with open(shake_mod_path) as fob:\n",
    "    shake_mod = fob.readlines()\n",
    "print(len(shake_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   shake  class\n",
       "0        I have a mind to strike thee ere thou speak’st.      1\n",
       "1      Yet if thou say Antony lives, is well, Or frie...      1\n",
       "2                                      Madam, he’s well.      1\n",
       "3                                             Well said.      1\n",
       "4                               And friends with Caesar.      1\n",
       "...                                                  ...    ...\n",
       "28234  What a thrice-double ass Was I to take this dr...      1\n",
       "28235                                       Go to, away!      1\n",
       "28236  to Stephano and Trinculo] Hence, and bestow yo...      1\n",
       "28237                               Or stole it, rather.      1\n",
       "28238  I long To hear the story of your life, which m...      1\n",
       "\n",
       "[28239 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>shake</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I have a mind to strike thee ere thou speak’st.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Yet if thou say Antony lives, is well, Or frie...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Madam, he’s well.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Well said.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>And friends with Caesar.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>28234</th>\n      <td>What a thrice-double ass Was I to take this dr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28235</th>\n      <td>Go to, away!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28236</th>\n      <td>to Stephano and Trinculo] Hence, and bestow yo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28237</th>\n      <td>Or stole it, rather.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28238</th>\n      <td>I long To hear the story of your life, which m...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>28239 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "shake = [x.strip().strip('\\\"') for x in shake]\n",
    "shake_mod = [x.strip().strip('\\\"') for x in shake_mod]\n",
    "shake_df = pd.DataFrame({'shake':shake, 'class': 1})\n",
    "shake_mod_df = pd.DataFrame({'shake':shake_mod, 'class': 0})\n",
    "shake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shuffled train size : (45182, 2), shuffled dev size : (11296, 2)\n"
     ]
    }
   ],
   "source": [
    "train_samples = int(len(shake_df) *  0.8)\n",
    "dev_samples = len(shake_df) - train_samples\n",
    "\n",
    "dev_df = pd.concat([shake_df.tail(dev_samples),shake_mod_df.tail(dev_samples)])\n",
    "train_df = pd.concat([shake_df.head(train_samples),shake_mod_df.head(train_samples)])\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Split Perc :  class\n0    0.5\n1    0.5\ndtype: float64 \n\nDev Split Perc :  class\n0    0.5\n1    0.5\ndtype: float64 \n\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(shake_train_path, index=False, header=False)\n",
    "dev_df.to_csv(shake_dev_path, index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('class').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('class').size()/len(dev_df),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"shakespeare\",\n",
    "    \"description\" : \"Derived from Shakespeare Plays (https://github.com/cocoxu/Shakespeare)\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"noshakespeare\",\n",
    "        1 : \"shakespeare\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Filter Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, json, shutil\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## formality\n",
    "data_input = '../data/processed/formality'\n",
    "data_output = '../data/processed_filtered/formality'\n",
    "data_output_full = '../data/processed_filtered/formality_full'\n",
    "\n",
    "formality_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "formality_d = pd.read_csv(f'{data_input}/dev.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 2280015), ('e', 982719), ('t', 770665), ('o', 756069), ('a', 641948), ('i', 572556), ('n', 558029), ('s', 509686), ('h', 457329), ('r', 428444), ('l', 360810), ('u', 313756), ('d', 292019), ('y', 262229), ('m', 221332), ('.', 220916), ('w', 188720), ('g', 176637), ('c', 170854), ('f', 155649), ('b', 134281), ('p', 118800), ('k', 110853), ('I', 103104), ('v', 95533), (',', 84555), ('T', 47063), (\"'\", 45414), ('!', 40085), ('E', 33123), ('A', 32658), ('S', 30865), ('O', 29809), ('H', 26929), ('?', 26852), ('N', 24120), ('Y', 23070), ('D', 20951), ('L', 19131), ('M', 19070), ('W', 18745), ('j', 18628), ('R', 17674), ('B', 16991), ('C', 14101), ('G', 13595), ('x', 13413), ('U', 11484), ('P', 11439), ('F', 9829), ('-', 9403), ('\"', 9262), ('q', 7274), ('z', 7037), ('K', 6774), (')', 6382), ('1', 6195), ('J', 6089), ('0', 5422), ('V', 5253), ('2', 5204), (':', 4589), ('(', 4524), ('3', 2791), ('5', 2565), ('*', 2404), ('4', 2355), (';', 2335), ('9', 2076), ('/', 1909), ('8', 1758), ('6', 1652), ('&', 1452), ('7', 1268), ('X', 811), ('Z', 769), ('_', 727), ('@', 667), ('$', 629), ('Q', 610), ('=', 481), ('#', 361), ('>', 231), ('^', 199), ('%', 178), ('~', 167), ('`', 163), (']', 147), ('+', 134), ('[', 116), ('<', 100), ('’', 100), ('¨', 78), ('´', 68), ('}', 41), ('{', 28), ('“', 27), ('—', 27), ('…', 22), ('”', 21), ('é', 21), ('|', 20), ('¡', 20), ('£', 18), ('Ü', 13), ('–', 12), ('ı', 10), ('♥', 10), ('¿', 6), ('ñ', 5), ('·', 4), ('\\\\', 4), ('\\u200b', 4), ('ü', 3), ('¢', 3), ('ö', 3), ('§', 3), ('á', 3), ('è', 2), ('˝', 2), ('嘉', 2), ('義', 2), ('人', 2), ('因', 2), ('為', 2), ('綠', 2), ('豆', 2), ('加', 2), ('薏', 2), ('仁', 2), ('©', 2), ('™', 2), ('‘', 2), ('☺', 2), ('ŕ', 2), ('ā', 2), ('ə', 2), ('®', 2), ('š', 2), ('†', 2), ('Æ', 1), ('恭', 1), ('喜', 1), ('發', 1), ('財', 1), ('♡', 1), ('½', 1), ('í', 1), ('ƒ', 1), ('Ä', 1), ('ù', 1), ('س', 1), ('ا', 1), ('م', 1), ('ه', 1), ('º', 1), ('¹', 1), ('œ', 1), ('•', 1), ('ó', 1), ('►', 1), ('λ', 1), ('◄', 1), ('à', 1), ('»', 1), ('ĕ', 1), ('û', 1), ('ï', 1)])\n\n [' ', 'e', 't', 'o', 'a', 'i', 'n', 's', 'h', 'r', 'l', 'u', 'd', 'y', 'm', '.', 'w', 'g', 'c', 'f', 'b', 'p', 'k', 'I', 'v', ',', 'T', \"'\", '!', 'E', 'A', 'S', 'O', 'H', '?', 'N', 'Y', 'D', 'L', 'M', 'W', 'j', 'R', 'B', 'C', 'G', 'x', 'U', 'P', 'F', '-', '\"', 'q', 'z', 'K', ')', '1', 'J', '0', 'V', '2', ':', '(', '3', '5', '*', '4', ';', '9', '/', '8', '6', '&', '7', 'X', 'Z', '_', '@', '$', 'Q', '=', '#', '>', '^', '%', '~', '`', ']', '+', '[', '<', '’', '¨', '´', '}', '{', '“', '—', '…', '”', 'é', '|', '¡', '£', 'Ü', '–', 'ı', '♥', '¿', 'ñ', '·', '\\\\', '\\u200b', 'ü', '¢', 'ö', '§', 'á', 'è', '˝', '嘉', '義', '人', '因', '為', '綠', '豆', '加', '薏', '仁', '©', '™', '‘', '☺', 'ŕ', 'ā', 'ə', '®', 'š', '†', 'Æ', '恭', '喜', '發', '財', '♡', '½', 'í', 'ƒ', 'Ä', 'ù', 'س', 'ا', 'م', 'ه', 'º', '¹', 'œ', '•', 'ó', '►', 'λ', '◄', 'à', '»', 'ĕ', 'û', 'ï']\n"
     ]
    }
   ],
   "source": [
    "p = OrderedDict(Counter(list(' '.join(formality_t[0]) + ' '.join(formality_d[0]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "formality_t['weird']= formality_t[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '_', '@', 'Q', '=', '#', '>', '^', '%', '~', '`', ']', '+', '[', '<', '’', '¨', '´', '}', '{', '“', '—', '…', '”', 'é', '|', '¡', 'Ü', '–', 'ı', '♥', '¿', 'ñ', '·', '\\\\', '\\u200b', 'ü', '¢', 'ö', '§', 'á', 'è', '˝', '嘉', '義', '人', '因', '為', '綠', '豆', '加', '薏', '仁', '©', '™', '‘', '☺', 'ŕ', 'ā', 'ə', '®', 'š', '†', 'Æ', '恭', '喜', '發', '財', '♡', '½', 'í', 'ƒ', 'Ä', 'ù', 'س', 'ا', 'م', 'ه', 'º', '¹', 'œ', '•', 'ó', '►', 'λ', '◄', 'à', '»', 'ĕ', 'û', 'ï', '(',')',':','--', '....', '!!!', 'www', 'http']])) )\n",
    "formality_d['weird']= formality_d[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '_', '@', 'Q', '=', '#', '>', '^', '%', '~', '`', ']', '+', '[', '<', '’', '¨', '´', '}', '{', '“', '—', '…', '”', 'é', '|', '¡', 'Ü', '–', 'ı', '♥', '¿', 'ñ', '·', '\\\\', '\\u200b', 'ü', '¢', 'ö', '§', 'á', 'è', '˝', '嘉', '義', '人', '因', '為', '綠', '豆', '加', '薏', '仁', '©', '™', '‘', '☺', 'ŕ', 'ā', 'ə', '®', 'š', '†', 'Æ', '恭', '喜', '發', '財', '♡', '½', 'í', 'ƒ', 'Ä', 'ù', 'س', 'ا', 'م', 'ه', 'º', '¹', 'œ', '•', 'ó', '►', 'λ', '◄', 'à', '»', 'ĕ', 'û', 'ï', '(',')',':','--', '....', '!!!', 'www', 'http']])) )\n",
    "\n",
    "formality_t['tokens'] = formality_t[0].apply(lambda x: len(x.split(' ')))\n",
    "formality_d['tokens'] = formality_d[0].apply(lambda x: len(x.split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(169735, 4) (154938, 4)\n(42434, 4) (38709, 4)\n"
     ]
    }
   ],
   "source": [
    "print(formality_t.shape, formality_t[(~formality_t['weird']) & (formality_t['tokens']>=5) & (formality_t['tokens']<=30)].shape)\n",
    "print(formality_d.shape, formality_d[(~formality_d['weird']) & (formality_d['tokens']>=5) & (formality_d['tokens']<=30)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "formality_t = formality_t[(~formality_t['weird']) & (formality_t['tokens']>=5) & (formality_t['tokens']<=30)].filter([0,1])\n",
    "formality_d = formality_d[(~formality_d['weird']) & (formality_d['tokens']>=5) & (formality_d['tokens']<=30)].filter([0,1])\n",
    "\n",
    "formality_t = formality_t.sample(frac=1).reset_index(drop=True)\n",
    "formality_d = formality_d.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "formality_tt = formality_t[:100000]\n",
    "formality_dd = formality_d[:25000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0    47230\n",
       "1    52770\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "formality_tt.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/formality/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "formality_tt.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "formality_dd.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/formality_full/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "if not os.path.exists(data_output_full):\n",
    "    os.makedirs(data_output_full)\n",
    "formality_t.to_csv(f'{data_output_full}/train.csv', header=False, index=False)\n",
    "formality_d.to_csv(f'{data_output_full}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output_full}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arousal\n",
    "data_input = '../data/processed/arousal'\n",
    "data_output = '../data/processed_filtered/arousal'\n",
    "\n",
    "arousal_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "arousal_d = pd.read_csv(f'{data_input}/dev.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 144718), ('e', 79546), ('t', 57680), ('a', 53839), ('o', 52043), ('n', 46969), ('i', 46767), ('s', 43431), ('r', 40361), ('h', 31456), ('l', 28804), ('d', 24916), ('u', 19126), ('c', 18819), ('m', 15939), ('f', 14100), ('g', 13587), ('p', 13325), ('y', 11882), ('w', 11653), ('b', 9178), (',', 8470), ('.', 7980), ('v', 6690), ('k', 5292), ('I', 3115), ('T', 2694), ('A', 2191), ('S', 2020), ('-', 1960), (\"'\", 1796), ('C', 1693), ('\"', 1616), ('0', 1516), ('M', 1386), ('x', 1325), ('1', 1148), ('H', 1070), ('N', 1056), ('W', 1051), ('B', 1012), ('P', 959), ('’', 881), ('E', 854), ('R', 848), ('F', 831), ('j', 767), ('2', 757), ('D', 720), ('9', 696), ('L', 691), ('z', 672), ('q', 638), (':', 638), ('O', 637), ('”', 605), ('“', 601), ('G', 577), ('5', 551), ('Y', 491), ('K', 470), (')', 470), ('3', 469), ('(', 465), ('U', 455), ('J', 412), ('?', 380), ('8', 344), ('4', 340), ('V', 320), ('7', 315), ('6', 307), ('$', 301), (';', 276), ('—', 153), ('!', 140), ('/', 119), ('\\u2002', 116), ('–', 86), ('Z', 76), ('&', 71), ('%', 68), ('♭', 45), ('Q', 37), ('…', 34), ('\\xad', 30), ('♯', 27), ('_', 23), ('[', 22), (']', 22), ('X', 20), ('ó', 19), ('#', 16), ('‘', 14), ('·', 12), ('=', 11), ('+', 11), ('\\xa0', 10), ('|', 7), ('`', 7), ('é', 5), ('<', 4), ('>', 4), ('ç', 4), ('®', 4), ('*', 4), ('ñ', 3), ('ã', 3), ('@', 3), ('è', 2), ('õ', 2), ('♮', 1), ('ü', 1), ('í', 1)])\n\n [' ', 'e', 't', 'a', 'o', 'n', 'i', 's', 'r', 'h', 'l', 'd', 'u', 'c', 'm', 'f', 'g', 'p', 'y', 'w', 'b', ',', '.', 'v', 'k', 'I', 'T', 'A', 'S', '-', \"'\", 'C', '\"', '0', 'M', 'x', '1', 'H', 'N', 'W', 'B', 'P', '’', 'E', 'R', 'F', 'j', '2', 'D', '9', 'L', 'z', 'q', ':', 'O', '”', '“', 'G', '5', 'Y', 'K', ')', '3', '(', 'U', 'J', '?', '8', '4', 'V', '7', '6', '$', ';', '—', '!', '/', '\\u2002', '–', 'Z', '&', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í']\n"
     ]
    }
   ],
   "source": [
    "p = OrderedDict(Counter(list(' '.join(arousal_t[0]) + ' '.join(arousal_d[0]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "arousal_t['weird']= arousal_t[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '\\u2002', '–', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í', '(',')',':','--', '....', '!!!', 'www', 'http','_',':',\"”\",\"“\"]])) )\n",
    "arousal_d['weird']= arousal_d[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in ['\\u2002', '–', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í', '(',')',':','--', '....', '!!!', 'www', 'http','_',':',\"”\",\"“\"]])) )\n",
    "\n",
    "arousal_t['tokens'] = arousal_t[0].apply(lambda x: len(x.split(' ')))\n",
    "arousal_d['tokens'] = arousal_d[0].apply(lambda x: len(x.split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(6901, 4) (5178, 4)\n(1726, 4) (1320, 4)\n"
     ]
    }
   ],
   "source": [
    "print(arousal_t.shape, arousal_t[(~arousal_t['weird']) & (arousal_t['tokens']>=5) & (arousal_t['tokens']<=40)].shape)\n",
    "print(arousal_d.shape, arousal_d[(~arousal_d['weird']) & (arousal_d['tokens']>=5) & (arousal_d['tokens']<=40)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0    2422\n",
       "1    4479\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "arousal_t.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/arousal/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "arousal_t =  arousal_t[(~arousal_t['weird']) & (arousal_t['tokens']>=5) & (arousal_t['tokens']<=40)].filter([0,1])\n",
    "arousal_d = arousal_d[(~arousal_d['weird']) & (arousal_d['tokens']>=5) & (arousal_d['tokens']<=40)].filter([0,1])\n",
    "\n",
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "arousal_t.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "arousal_d.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Emo\n",
    "data_input = '../data/processed/emo'\n",
    "data_output = '../data/processed_filtered/emo'\n",
    "\n",
    "emo_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "emo_d = pd.read_csv(f'{data_input}/dev.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 550295), ('e', 250830), ('t', 180123), ('o', 165422), ('a', 160428), ('i', 151709), ('n', 131936), ('s', 114983), ('l', 102090), ('r', 100713), ('h', 97975), ('d', 73543), ('u', 64931), ('m', 60146), ('y', 58064), ('g', 51959), ('f', 49950), ('c', 44549), ('w', 43190), ('p', 34100), ('b', 31669), ('.', 31654), ('k', 27177), ('v', 22213), ('I', 14849), (',', 9914), (\"'\", 9734), ('!', 9656), ('T', 6468), ('j', 4802), ('@', 4690), ('x', 3847), ('S', 3578), ('?', 3516), ('W', 3406), ('H', 3190), ('A', 3087), ('O', 2970), ('M', 2844), ('Y', 2767), ('L', 2042), ('’', 1973), ('D', 1930), ('N', 1921), ('z', 1911), ('B', 1907), ('G', 1905), ('C', 1834), ('E', 1704), ('q', 1656), ('/', 1500), ('-', 1358), ('P', 1256), ('R', 1202), (';', 1180), ('F', 1049), ('J', 1021), ('&', 993), ('0', 976), ('1', 956), (':', 875), ('2', 865), ('K', 805), ('3', 648), ('U', 639), ('V', 596), ('4', 555), ('_', 555), ('5', 435), ('6', 399), (')', 384), ('8', 366), ('7', 344), ('(', 330), ('9', 321), ('*', 264), ('#', 226), ('Z', 128), ('X', 125), ('Q', 95), ('=', 95), ('ï', 90), ('¿', 90), ('½', 90), ('$', 85), ('~', 77), ('+', 50), (']', 44), ('%', 30), ('\"', 25), ('[', 20), ('^', 20), ('`', 13), ('”', 12), ('“', 10), ('‘', 9), ('|', 9), ('\\\\', 6), ('—', 5), ('–', 4), ('。', 3), ('Â', 1), ('¡', 1), ('′', 1), ('{', 1), ('}', 1)])\n\n [' ', 'e', 't', 'o', 'a', 'i', 'n', 's', 'l', 'r', 'h', 'd', 'u', 'm', 'y', 'g', 'f', 'c', 'w', 'p', 'b', '.', 'k', 'v', 'I', ',', \"'\", '!', 'T', 'j', '@', 'x', 'S', '?', 'W', 'H', 'A', 'O', 'M', 'Y', 'L', '’', 'D', 'N', 'z', 'B', 'G', 'C', 'E', 'q', '/', '-', 'P', 'R', ';', 'F', 'J', '&', '0', '1', ':', '2', 'K', '3', 'U', 'V', '4', '_', '5', '6', ')', '8', '7', '(', '9', '*', '#', 'Z', 'X', 'Q', '=', 'ï', '¿', '½', '$', '~', '+', ']', '%', '\"', '[', '^', '`', '”', '“', '‘', '|', '\\\\', '—', '–', '。', 'Â', '¡', '′', '{', '}']\n"
     ]
    }
   ],
   "source": [
    "p = OrderedDict(Counter(list(' '.join(emo_t[0]) + ' '.join(emo_d[0]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_t['weird']= emo_t[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '\\u2002', '–', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í', '(',')',':','--', '....', '!!!', 'www', 'http','_',':',\"”\",\"“\",'ï', '¿', '½', '$', '~', '+', ']', '%', '[', '^', '`', '”', '“', '|', '‘', '\\\\', '—', '–', '。', 'Â', '¡', '′', '{', '}','=', 'ï', '¿', '½', '~', '+', ']']])) )\n",
    "emo_d['weird']= emo_d[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '\\u2002', '–', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í', '(',')',':','--', '....', '!!!', 'www', 'http','_',':',\"”\",\"“\",'ï', '¿', '½', '$', '~', '+', ']', '%', '[', '^', '`', '”', '“', '|', '‘', '\\\\', '—', '–', '。', 'Â', '¡', '′', '{', '}','=', 'ï', '¿', '½', '~', '+', ']']])) )\n",
    "\n",
    "emo_t['tokens'] = emo_t[0].apply(lambda x: len(x.split(' ')))\n",
    "emo_d['tokens'] = emo_d[0].apply(lambda x: len(x.split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(27432, 4) (21866, 4)\n(6858, 4) (5533, 4)\n"
     ]
    }
   ],
   "source": [
    "print(emo_t.shape, emo_t[(~emo_t['weird']) & (emo_t['tokens']>=5) & (emo_t['tokens']<=40)].shape)\n",
    "print(emo_d.shape, emo_d[(~emo_d['weird']) & (emo_d['tokens']>=5) & (emo_d['tokens']<=40)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_t = emo_t[(~emo_t['weird']) & (emo_t['tokens']>=5) & (emo_t['tokens']<=40)].filter([0,1])\n",
    "emo_d =  emo_d[(~emo_d['weird']) & (emo_d['tokens']>=5) & (emo_d['tokens']<=40)].filter([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0     6733\n",
       "1    15133\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "emo_t.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/emo/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "emo_t.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "emo_d.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 2829917), ('e', 1838050), ('t', 1307202), ('a', 1251970), ('i', 1215050), ('n', 1131075), ('o', 1085875), ('s', 999410), ('r', 970333), ('l', 631594), ('c', 566534), ('h', 533177), ('d', 520514), ('m', 415734), ('u', 384495), ('p', 379593), ('f', 322573), ('g', 305191), ('b', 195292), ('y', 191021), ('v', 167202), ('w', 165397), (',', 137407), ('.', 80212), ('k', 70205), ('-', 60182), ('x', 50873), ('T', 47578), ('A', 36092), (')', 35839), ('(', 35660), ('I', 34027), ('S', 31235), ('M', 29194), ('C', 26620), ('N', 24026), ('z', 23067), ('D', 22526), ('q', 22477), ('1', 20130), ('L', 20008), ('0', 19944), ('R', 19189), ('P', 18729), ('F', 15038), ('W', 14925), ('E', 14828), ('B', 14634), ('2', 14202), ('\"', 13639), ('G', 12271), ('O', 11862), ('j', 11674), ('H', 10950), ('9', 8973), (':', 7893), ('U', 7846), (\"'\", 7211), ('V', 6193), ('3', 5784), ('5', 4796), ('K', 4592), ('8', 4404), ('4', 4021), ('6', 3939), ('7', 3632), (';', 3560), ('/', 3233), ('J', 3087), ('X', 2832), ('%', 2694), ('Q', 2540), ('–', 2341), ('+', 1946), ('$', 1858), ('−', 1711), ('Y', 1586), ('\\\\', 1495), ('}', 1304), (']', 1233), ('[', 1210), ('|', 1138), ('Z', 772), ('{', 723), ('—', 708), ('β', 678), ('’', 651), ('&', 588), ('\\u2061', 543), ('^', 523), ('θ', 497), ('∈', 478), ('”', 458), ('“', 452), ('>', 450), ('?', 438), ('→', 414), ('<', 408), ('_', 400), ('‖', 383), ('∣', 376), ('…', 331), ('ε', 290), ('⋅', 275), ('∑', 272), ('λ', 270), ('μ', 269), ('×', 267), ('α', 266), ('=', 256), ('′', 243), ('σ', 222), ('∗', 215), ('#', 212), ('≤', 202), ('π', 186), ('*', 180), ('é', 177), ('@', 174), ('~', 165), ('Σ', 158), ('!', 143), ('ℓ', 142), ('ö', 141), ('‘', 138), ('γ', 132), ('≥', 127), ('`', 109), ('\\xa0', 104), ('ü', 103), ('φ', 103), ('δ', 102), ('\\u2009', 91), ('ï', 88), ('τ', 82), ('ϕ', 80), ('η', 80), ('←', 77), ('⋯', 73), ('⟩', 69), ('²', 67), ('∞', 67), ('⟨', 67), ('ı', 65), ('∼', 64), ('≠', 64), ('ϵ', 63), ('ρ', 59), ('Δ', 58), ('Θ', 57), ('Φ', 57), ('¬', 51), ('ä', 47), ('≈', 47), ('∫', 46), ('∪', 46), ('±', 45), ('⊤', 45), ('∀', 44), ('á', 43), ('и', 42), ('⇒', 41), ('ξ', 41), ('⊆', 39), ('∇', 38), ('\\u200b', 36), ('∂', 36), ('\\u202f', 35), ('¯', 35), ('ν', 34), ('Ω', 33), ('∏', 32), ('°', 32), ('е', 32), ('è', 30), ('а', 27), ('ζ', 27), ('∝', 26), ('ń', 25), ('χ', 23), ('∩', 22), ('ş', 22), ('ç', 22), ('ɛ', 21), ('·', 21), ('в', 21), ('о', 21), ('§', 20), ('⊥', 20), ('ω', 20), ('∧', 19), ('‐', 18), ('ã', 18), ('н', 18), ('Λ', 17), ('⟶', 17), ('⊂', 16), ('˙', 16), ('Γ', 16), ('Ψ', 15), ('κ', 15), ('ч', 14), ('í', 13), ('ƒ', 13), ('⋮', 13), ('√', 13), ('р', 13), ('ψ', 13), ('⊗', 12), ('ó', 12), ('ā', 12), ('Č', 12), ('л', 12), ('к', 11), ('с', 11), ('¨', 11), ('Π', 10), ('ℝ', 10), ('«', 10), ('ŷ', 9), ('ğ', 9), ('©', 9), ('»', 9), ('⌊', 9), ('⌋', 9), ('ô', 9), ('м', 9), ('†', 8), ('µ', 8), ('™', 8), ('≧', 8), ('я', 8), ('υ', 8), ('ê', 8), ('т', 8), ('Å', 7), ('″', 7), ('æ', 7), ('É', 7), ('ø', 7), ('⊎', 7), ('й', 7), ('д', 7), ('®', 7), ('у', 7), ('ο', 6), ('ς', 6), ('ă', 6), ('à', 6), ('≻', 6), ('‡', 6), ('↔', 6), ('š', 6), ('ž', 6), ('â', 6), ('М', 6), ('ː', 6), ('³', 5), ('•', 5), ('⇔', 5), ('ə', 5), ('Ö', 5), ('≪', 5), ('⩽', 5), ('⊨', 5), ('ú', 5), ('г', 5), ('б', 5), ('⌈', 5), ('⌉', 5), ('ι', 5), ('ό', 5), ('≫', 5), ('≡', 5), ('↦', 5), ('⊺', 5), ('⊙', 4), ('∥', 4), ('В', 4), ('ō', 4), ('⟺', 4), ('\\x8c', 4), ('ß', 4), ('∘', 4), ('ł', 4), ('ά', 4), ('Š', 4), ('\\x80', 4), ('∉', 4), ('⇝', 3), ('，', 3), ('ẓ', 3), ('€', 3), ('ő', 3), ('⋆', 3), ('⊭', 3), ('Г', 3), ('И', 3), ('⩾', 3), ('₅', 3), ('₀', 3), ('ì', 3), ('Н', 3), ('х', 3), ('ī', 3), ('£', 3), ('з', 3), ('А', 3), ('⊖', 3), ('ˆ', 3), ('⁵', 3), ('¹', 3), ('⁰', 3), ('く', 3), ('も', 3), ('å', 2), ('∖', 2), ('ﬁ', 2), ('⊕', 2), ('‑', 2), ('ˈ', 2), ('ɪ', 2), ('̆', 2), ('⊃', 2), ('Ş', 2), ('ь', 2), ('÷', 2), ('𝛁', 2), ('ē', 2), ('С', 2), ('Б', 2), ('ﬀ', 2), ('\\u200a', 2), ('\\u2060', 2), ('Ł', 2), ('П', 2), ('║', 2), ('ή', 2), ('⪰', 2), ('ū', 2), ('Ø', 2), ('Υ', 2), ('č', 2), ('´', 2), ('ℜ', 2), ('必', 2), ('ی', 2), ('ن', 2), ('ș', 2), ('ě', 2), ('∨', 2), ('î', 1), ('↽', 1), ('⇀', 1), ('ῶ', 1), ('̃', 1), ('\\u2005', 1), ('ṇ', 1), ('Е', 1), ('Д', 1), ('Л', 1), ('⋃', 1), ('ἀ', 1), ('È', 1), ('ĭ', 1), ('́', 1), ('↓', 1), ('⁄', 1), ('\\x96', 1), ('≔', 1), ('ò', 1), ('≅', 1), ('ю', 1), ('щ', 1), ('ф', 1), ('ц', 1), ('ж', 1), ('О', 1), ('孫', 1), ('叔', 1), ('敖', 1), ('△', 1), ('К', 1), ('↑', 1), ('û', 1), ('̯', 1), ('͡', 1), ('ы', 1), ('ć', 1), ('⊇', 1), ('ḥ', 1), ('¾', 1), ('ź', 1), ('Р', 1), ('έ', 1), ('ṅ', 1), ('₁', 1), ('₂', 1), ('ë', 1), ('⋱', 1), ('ﬃ', 1), ('⨄', 1), ('ý', 1), ('\\x83', 1), ('\\x8a', 1), ('п', 1), ('吳', 1), ('恩', 1), ('達', 1), ('ñ', 1), ('蜘', 1), ('蛛', 1), ('雲', 1), ('˜', 1), ('Ü', 1), ('\\u200d', 1), ('\\u200c', 1), ('－', 1), ('应', 1), ('應', 1), ('ʻ', 1), ('Ã', 1), ('‰', 1), ('ř', 1), ('ز', 1), ('و', 1), ('ب', 1), ('ق', 1), ('ه', 1), ('ر', 1), ('م', 1), ('ا', 1), ('\\u200e', 1), ('✩', 1), ('ˌ', 1), ('ɹ', 1), ('ʌ', 1), ('ʊ', 1), ('↗', 1), ('↙', 1), ('西', 1), ('門', 1), ('豹', 1), ('Æ', 1), ('⨂', 1), ('\\x93', 1), ('Ă', 1), ('Ť', 1), ('Ç', 1), ('У', 1), ('Κ', 1), ('∙', 1)])\n\n [' ', 'e', 't', 'a', 'i', 'n', 'o', 's', 'r', 'l', 'c', 'h', 'd', 'm', 'u', 'p', 'f', 'g', 'b', 'y', 'v', 'w', ',', '.', 'k', '-', 'x', 'T', 'A', ')', '(', 'I', 'S', 'M', 'C', 'N', 'z', 'D', 'q', '1', 'L', '0', 'R', 'P', 'F', 'W', 'E', 'B', '2', '\"', 'G', 'O', 'j', 'H', '9', ':', 'U', \"'\", 'V', '3', '5', 'K', '8', '4', '6', '7', ';', '/', 'J', 'X', '%', 'Q', '–', '+', '$', '−', 'Y', '\\\\', '}', ']', '[', '|', 'Z', '{', '—', 'β', '’', '&', '\\u2061', '^', 'θ', '∈', '”', '“', '>', '?', '→', '<', '_', '‖', '∣', '…', 'ε', '⋅', '∑', 'λ', 'μ', '×', 'α', '=', '′', 'σ', '∗', '#', '≤', 'π', '*', 'é', '@', '~', 'Σ', '!', 'ℓ', 'ö', '‘', 'γ', '≥', '`', '\\xa0', 'ü', 'φ', 'δ', '\\u2009', 'ï', 'τ', 'ϕ', 'η', '←', '⋯', '⟩', '²', '∞', '⟨', 'ı', '∼', '≠', 'ϵ', 'ρ', 'Δ', 'Θ', 'Φ', '¬', 'ä', '≈', '∫', '∪', '±', '⊤', '∀', 'á', 'и', '⇒', 'ξ', '⊆', '∇', '\\u200b', '∂', '\\u202f', '¯', 'ν', 'Ω', '∏', '°', 'е', 'è', 'а', 'ζ', '∝', 'ń', 'χ', '∩', 'ş', 'ç', 'ɛ', '·', 'в', 'о', '§', '⊥', 'ω', '∧', '‐', 'ã', 'н', 'Λ', '⟶', '⊂', '˙', 'Γ', 'Ψ', 'κ', 'ч', 'í', 'ƒ', '⋮', '√', 'р', 'ψ', '⊗', 'ó', 'ā', 'Č', 'л', 'к', 'с', '¨', 'Π', 'ℝ', '«', 'ŷ', 'ğ', '©', '»', '⌊', '⌋', 'ô', 'м', '†', 'µ', '™', '≧', 'я', 'υ', 'ê', 'т', 'Å', '″', 'æ', 'É', 'ø', '⊎', 'й', 'д', '®', 'у', 'ο', 'ς', 'ă', 'à', '≻', '‡', '↔', 'š', 'ž', 'â', 'М', 'ː', '³', '•', '⇔', 'ə', 'Ö', '≪', '⩽', '⊨', 'ú', 'г', 'б', '⌈', '⌉', 'ι', 'ό', '≫', '≡', '↦', '⊺', '⊙', '∥', 'В', 'ō', '⟺', '\\x8c', 'ß', '∘', 'ł', 'ά', 'Š', '\\x80', '∉', '⇝', '，', 'ẓ', '€', 'ő', '⋆', '⊭', 'Г', 'И', '⩾', '₅', '₀', 'ì', 'Н', 'х', 'ī', '£', 'з', 'А', '⊖', 'ˆ', '⁵', '¹', '⁰', 'く', 'も', 'å', '∖', 'ﬁ', '⊕', '‑', 'ˈ', 'ɪ', '̆', '⊃', 'Ş', 'ь', '÷', '𝛁', 'ē', 'С', 'Б', 'ﬀ', '\\u200a', '\\u2060', 'Ł', 'П', '║', 'ή', '⪰', 'ū', 'Ø', 'Υ', 'č', '´', 'ℜ', '必', 'ی', 'ن', 'ș', 'ě', '∨', 'î', '↽', '⇀', 'ῶ', '̃', '\\u2005', 'ṇ', 'Е', 'Д', 'Л', '⋃', 'ἀ', 'È', 'ĭ', '́', '↓', '⁄', '\\x96', '≔', 'ò', '≅', 'ю', 'щ', 'ф', 'ц', 'ж', 'О', '孫', '叔', '敖', '△', 'К', '↑', 'û', '̯', '͡', 'ы', 'ć', '⊇', 'ḥ', '¾', 'ź', 'Р', 'έ', 'ṅ', '₁', '₂', 'ë', '⋱', 'ﬃ', '⨄', 'ý', '\\x83', '\\x8a', 'п', '吳', '恩', '達', 'ñ', '蜘', '蛛', '雲', '˜', 'Ü', '\\u200d', '\\u200c', '－', '应', '應', 'ʻ', 'Ã', '‰', 'ř', 'ز', 'و', 'ب', 'ق', 'ه', 'ر', 'م', 'ا', '\\u200e', '✩', 'ˌ', 'ɹ', 'ʌ', 'ʊ', '↗', '↙', '西', '門', '豹', 'Æ', '⨂', '\\x93', 'Ă', 'Ť', 'Ç', 'У', 'Κ', '∙']\n"
     ]
    }
   ],
   "source": [
    "## Abstract\n",
    "data_input = '../data/processed/abstract'\n",
    "data_output = '../data/processed_filtered/abstract'\n",
    "\n",
    "abs_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "abs_d = pd.read_csv(f'{data_input}/dev.csv', header=None)\n",
    "\n",
    "p = OrderedDict(Counter(list(' '.join([str(x) for x in abs_t[0]]) + ' '.join([str(x) for x in abs_d[0]]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_t['weird']= abs_t[0].apply(lambda x: bool(sum([str(x).find(i)>=0 for i in [ '%', '–', '+', '$', '−', 'Y', '\\\\', '}', ']', '[', '|', 'Z', '{', '—', 'β', '’', '&', '\\u2061', '^', 'θ', '∈', '”', '“', '>', '?', '→', '<', '_', '‖', '∣', '…', 'ε', '⋅', '∑', 'λ', 'μ', '×', 'α', '=', '′', 'σ', '∗', '#', '≤', 'π', '*', 'é', '@', '~', 'Σ', '!', 'ℓ', 'ö', '‘', 'γ', '≥', '`', '\\xa0', 'ü', 'φ', 'δ', '\\u2009', 'ï', 'τ', 'ϕ', 'η', '←', '⋯', '⟩', '²', '∞', '⟨', 'ı', '∼', '≠', 'ϵ', 'ρ', 'Δ', 'Θ', 'Φ', '¬', 'ä', '≈', '∫', '∪', '±', '⊤', '∀', 'á', 'и', '⇒', 'ξ', '⊆', '∇', '\\u200b', '∂', '\\u202f', '¯', 'ν', 'Ω', '∏', '°', 'е', 'è', 'а', 'ζ', '∝', 'ń', 'χ', '∩', 'ş', 'ç', 'ɛ', '·', 'в', 'о', '§', '⊥', 'ω', '∧', '‐', 'ã', 'н', 'Λ', '⟶', '⊂', '˙', 'Γ', 'Ψ', 'κ', 'ч', 'í', 'ƒ', '⋮', '√', 'р', 'ψ', '⊗', 'ó', 'ā', 'Č', 'л', 'к', 'с', '¨', 'Π', 'ℝ', '«', 'ŷ', 'ğ', '©', '»', '⌊', '⌋', 'ô', 'м', '†', 'µ', '™', '≧', 'я', 'υ', 'ê', 'т', 'Å', '″', 'æ', 'É', 'ø', '⊎', 'й', 'д', '®', 'у', 'ο', 'ς', 'ă', 'à', '≻', '‡', '↔', 'š', 'ž', 'â', 'М', 'ː', '³', '•', '⇔', 'ə', 'Ö', '≪', '⩽', '⊨', 'ú', 'г', 'б', '⌈', '⌉', 'ι', 'ό', '≫', '≡', '↦', '⊺', '⊙', '∥', 'В', 'ō', '⟺', '\\x8c', 'ß', '∘', 'ł', 'ά', 'Š', '\\x80', '∉', '⇝', '，', 'ẓ', '€', 'ő', '⋆', '⊭', 'Г', 'И', '⩾', '₅', '₀', 'ì', 'Н', 'х', 'ī', '£', 'з', 'А', '⊖', 'ˆ', '⁵', '¹', '⁰', 'く', 'も', 'å', '∖', 'ﬁ', '⊕', '‑', 'ˈ', 'ɪ', '̆', '⊃', 'Ş', 'ь', '÷', '𝛁', 'ē', 'С', 'Б', 'ﬀ', '\\u200a', '\\u2060', 'Ł', 'П', '║', 'ή', '⪰', 'ū', 'Ø', 'Υ', 'č', '´', 'ℜ', '必', 'ی', 'ن', 'ș', 'ě', '∨', 'î', '↽', '⇀', 'ῶ', '̃', '\\u2005', 'ṇ', 'Е', 'Д', 'Л', '⋃', 'ἀ', 'È', 'ĭ', '́', '↓', '⁄', '\\x96', '≔', 'ò', '≅', 'ю', 'щ', 'ф', 'ц', 'ж', 'О', '孫', '叔', '敖', '△', 'К', '↑', 'û', '̯', '͡', 'ы', 'ć', '⊇', 'ḥ', '¾', 'ź', 'Р', 'έ', 'ṅ', '₁', '₂', 'ë', '⋱', 'ﬃ', '⨄', 'ý', '\\x83', '\\x8a', 'п', '吳', '恩', '達', 'ñ', '蜘', '蛛', '雲', '˜', 'Ü', '\\u200d', '\\u200c', '－', '应', '應', 'ʻ', 'Ã', '‰', 'ř', 'ز', 'و', 'ب', 'ق', 'ه', 'ر', 'م', 'ا', '\\u200e', '✩', 'ˌ', 'ɹ', 'ʌ', 'ʊ', '↗', '↙', '西', '門', '豹', 'Æ', '⨂', '\\x93', 'Ă', 'Ť', 'Ç', 'У', 'Κ', '∙','http','www','--','!!','displaystyle']])) )\n",
    "abs_d['weird']= abs_d[0].apply(lambda x: bool(sum([str(x).find(i)>=0 for i in ['%', '–', '+', '$', '−', 'Y', '\\\\', '}', ']', '[', '|', 'Z', '{', '—', 'β', '’', '&', '\\u2061', '^', 'θ', '∈', '”', '“', '>', '?', '→', '<', '_', '‖', '∣', '…', 'ε', '⋅', '∑', 'λ', 'μ', '×', 'α', '=', '′', 'σ', '∗', '#', '≤', 'π', '*', 'é', '@', '~', 'Σ', '!', 'ℓ', 'ö', '‘', 'γ', '≥', '`', '\\xa0', 'ü', 'φ', 'δ', '\\u2009', 'ï', 'τ', 'ϕ', 'η', '←', '⋯', '⟩', '²', '∞', '⟨', 'ı', '∼', '≠', 'ϵ', 'ρ', 'Δ', 'Θ', 'Φ', '¬', 'ä', '≈', '∫', '∪', '±', '⊤', '∀', 'á', 'и', '⇒', 'ξ', '⊆', '∇', '\\u200b', '∂', '\\u202f', '¯', 'ν', 'Ω', '∏', '°', 'е', 'è', 'а', 'ζ', '∝', 'ń', 'χ', '∩', 'ş', 'ç', 'ɛ', '·', 'в', 'о', '§', '⊥', 'ω', '∧', '‐', 'ã', 'н', 'Λ', '⟶', '⊂', '˙', 'Γ', 'Ψ', 'κ', 'ч', 'í', 'ƒ', '⋮', '√', 'р', 'ψ', '⊗', 'ó', 'ā', 'Č', 'л', 'к', 'с', '¨', 'Π', 'ℝ', '«', 'ŷ', 'ğ', '©', '»', '⌊', '⌋', 'ô', 'м', '†', 'µ', '™', '≧', 'я', 'υ', 'ê', 'т', 'Å', '″', 'æ', 'É', 'ø', '⊎', 'й', 'д', '®', 'у', 'ο', 'ς', 'ă', 'à', '≻', '‡', '↔', 'š', 'ž', 'â', 'М', 'ː', '³', '•', '⇔', 'ə', 'Ö', '≪', '⩽', '⊨', 'ú', 'г', 'б', '⌈', '⌉', 'ι', 'ό', '≫', '≡', '↦', '⊺', '⊙', '∥', 'В', 'ō', '⟺', '\\x8c', 'ß', '∘', 'ł', 'ά', 'Š', '\\x80', '∉', '⇝', '，', 'ẓ', '€', 'ő', '⋆', '⊭', 'Г', 'И', '⩾', '₅', '₀', 'ì', 'Н', 'х', 'ī', '£', 'з', 'А', '⊖', 'ˆ', '⁵', '¹', '⁰', 'く', 'も', 'å', '∖', 'ﬁ', '⊕', '‑', 'ˈ', 'ɪ', '̆', '⊃', 'Ş', 'ь', '÷', '𝛁', 'ē', 'С', 'Б', 'ﬀ', '\\u200a', '\\u2060', 'Ł', 'П', '║', 'ή', '⪰', 'ū', 'Ø', 'Υ', 'č', '´', 'ℜ', '必', 'ی', 'ن', 'ș', 'ě', '∨', 'î', '↽', '⇀', 'ῶ', '̃', '\\u2005', 'ṇ', 'Е', 'Д', 'Л', '⋃', 'ἀ', 'È', 'ĭ', '́', '↓', '⁄', '\\x96', '≔', 'ò', '≅', 'ю', 'щ', 'ф', 'ц', 'ж', 'О', '孫', '叔', '敖', '△', 'К', '↑', 'û', '̯', '͡', 'ы', 'ć', '⊇', 'ḥ', '¾', 'ź', 'Р', 'έ', 'ṅ', '₁', '₂', 'ë', '⋱', 'ﬃ', '⨄', 'ý', '\\x83', '\\x8a', 'п', '吳', '恩', '達', 'ñ', '蜘', '蛛', '雲', '˜', 'Ü', '\\u200d', '\\u200c', '－', '应', '應', 'ʻ', 'Ã', '‰', 'ř', 'ز', 'و', 'ب', 'ق', 'ه', 'ر', 'م', 'ا', '\\u200e', '✩', 'ˌ', 'ɹ', 'ʌ', 'ʊ', '↗', '↙', '西', '門', '豹', 'Æ', '⨂', '\\x93', 'Ă', 'Ť', 'Ç', 'У', 'Κ', '∙','http','www','--','!!','displaystyle']])) )\n",
    "\n",
    "abs_t['tokens'] = abs_t[0].apply(lambda x: len([p for p in str(x).split(' ') if len(p)>3]))\n",
    "abs_d['tokens'] = abs_d[0].apply(lambda x: len([p for p in str(x).split(' ') if len(p)>3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_t.loc[abs_t[0].str[0].isin([',', '(', '2', '1','0', ')', '5',  '7',  '4',  '3',  '6', '8', ':',  '9',  ';', '-', '/', '\"']),'weird'] = False\n",
    "abs_d.loc[abs_d[0].str[0].isin([',', '(', '2', '1','0', ')', '5',  '7',  '4',  '3',  '6', '8', ':',  '9',  ';', '-', '/', '\"']),'weird'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           weird\n",
       "tokens          \n",
       "6       4.902641\n",
       "7       5.708788\n",
       "8       6.506411\n",
       "9       7.178607\n",
       "10      7.518358\n",
       "11      7.321083\n",
       "12      7.463559\n",
       "13      7.256542\n",
       "14      6.886348\n",
       "15      6.342016\n",
       "16      5.632071\n",
       "17      5.191247\n",
       "18      4.683447\n",
       "19      3.949147\n",
       "20      3.391420\n",
       "21      2.783765\n",
       "22      2.406264\n",
       "23      1.960569\n",
       "24      1.609859\n",
       "25      1.307858"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>weird</th>\n    </tr>\n    <tr>\n      <th>tokens</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>4.902641</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>5.708788</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>6.506411</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>7.178607</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>7.518358</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>7.321083</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>7.463559</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>7.256542</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>6.886348</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>6.342016</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>5.632071</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>5.191247</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>4.683447</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>3.949147</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>3.391420</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2.783765</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2.406264</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1.960569</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1.609859</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1.307858</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "abs_t.groupby('tokens').agg({'weird':'count'}).head(40)*100/len(abs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(82119, 4) (82119, 4)\n(20527, 4) (20527, 4)\n"
     ]
    }
   ],
   "source": [
    "print(abs_t.shape, abs_t[(~abs_t['weird']) & (abs_t['tokens']>5) & (abs_t['tokens']<=25)].shape)\n",
    "print(abs_d.shape, abs_d[(~abs_d['weird']) & (abs_d['tokens']>5) & (abs_d['tokens']<=25)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0    46320\n",
       "1    35799\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "abs_t = abs_t[(~abs_t['weird']) & (abs_t['tokens']>5) & (abs_t['tokens']<=25)].filter([0,1])\n",
    "abs_d =  abs_d[(~abs_d['weird']) & (abs_d['tokens']>5) & (abs_d['tokens']<=25)].filter([0,1])\n",
    "abs_t.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/abstract/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "abs_t.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "abs_d.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 565826), ('e', 270473), ('o', 192783), ('t', 187990), ('a', 162903), ('s', 140861), ('n', 139712), ('h', 138712), ('r', 130948), ('i', 130668), ('l', 99223), ('d', 87066), ('u', 78010), ('m', 65292), ('y', 63586), (',', 52309), ('w', 49863), ('f', 42731), ('c', 40971), ('.', 40937), ('g', 40660), ('b', 30265), ('p', 29289), ('v', 23606), ('I', 23084), ('k', 21827), ('’', 14066), ('T', 11577), ('?', 10178), ('W', 9258), ('A', 9070), ('!', 6388), ('H', 5643), ('B', 4871), ('S', 4568), ('C', 4323), (\"'\", 4262), ('O', 3952), ('M', 3947), ('Y', 3429), (';', 3160), ('N', 3064), ('G', 3028), ('—', 2815), ('L', 2790), ('D', 2627), ('F', 2430), ('P', 1937), ('j', 1919), ('x', 1639), ('-', 1509), ('q', 1476), ('E', 1334), ('R', 1226), ('z', 894), ('”', 773), (':', 713), ('K', 609), ('“', 585), ('J', 556), ('\\xa0', 497), ('U', 415), ('V', 373), ('è', 241), ('Q', 196), ('\\u2003', 183), (']', 168), ('[', 75), ('(', 48), (')', 48), ('\"', 36), ('‘', 32), ('…', 17), ('é', 15), ('–', 10), ('Z', 10), ('ç', 8), ('1', 6), ('5', 6), ('0', 5), ('à', 2), ('Æ', 2), ('&', 2), ('ï', 2), ('2', 2), ('æ', 1), ('9', 1), ('/', 1)])\n\n [' ', 'e', 'o', 't', 'a', 's', 'n', 'h', 'r', 'i', 'l', 'd', 'u', 'm', 'y', ',', 'w', 'f', 'c', '.', 'g', 'b', 'p', 'v', 'I', 'k', '’', 'T', '?', 'W', 'A', '!', 'H', 'B', 'S', 'C', \"'\", 'O', 'M', 'Y', ';', 'N', 'G', '—', 'L', 'D', 'F', 'P', 'j', 'x', '-', 'q', 'E', 'R', 'z', '”', ':', 'K', '“', 'J', '\\xa0', 'U', 'V', 'è', 'Q', '\\u2003', ']', '[', '(', ')', '\"', '‘', '…', 'é', '–', 'Z', 'ç', '1', '5', '0', 'à', 'Æ', '&', 'ï', '2', 'æ', '9', '/']\n"
     ]
    }
   ],
   "source": [
    "# Shakespeare\n",
    "## Abstract\n",
    "data_input = '../data/processed/shakespeare'\n",
    "data_output = '../data/processed_filtered/shakespeare'\n",
    "\n",
    "abs_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "abs_d = pd.read_csv(f'{data_input}/dev.csv', header=None)\n",
    "\n",
    "p = OrderedDict(Counter(list(' '.join([str(x) for x in abs_t[0]]) + ' '.join([str(x) for x in abs_d[0]]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_t['weird']= abs_t[0].apply(lambda x: bool(sum([str(x).find(i)>=0 for i in [ '-', '”', ':', '“', '\\xa0', 'è', '\\u2003', '…', 'é', '–', 'ç', 'à', 'Æ', 'ï','æ', '/', 'http','www','--','!!']])) )\n",
    "abs_d['weird']= abs_d[0].apply(lambda x: bool(sum([str(x).find(i)>=0 for i in ['-', '”', ':', '“', '\\xa0', 'è', '\\u2003', '…', 'é', '–', 'ç', 'à', 'Æ', 'ï','æ', '/', 'http','www','--','!!']])) )\n",
    "\n",
    "abs_t['tokens'] = abs_t[0].apply(lambda x: len(str(x).split(' ')))\n",
    "abs_d['tokens'] = abs_d[0].apply(lambda x: len(str(x).split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            weird\n",
       "tokens           \n",
       "1        2.636005\n",
       "2        5.061750\n",
       "3        8.388296\n",
       "4       12.443008\n",
       "5        9.154088\n",
       "6        8.051879\n",
       "7        6.161746\n",
       "8        6.706210\n",
       "9        5.924926\n",
       "10       4.273826\n",
       "11       3.463769\n",
       "12       2.908238\n",
       "13       2.671418\n",
       "14       2.598380\n",
       "15       2.441238\n",
       "16       2.169005\n",
       "17       1.859147\n",
       "18       1.702005\n",
       "19       1.350095\n",
       "20       1.157541\n",
       "21       1.031384\n",
       "22       0.900801\n",
       "23       0.834403\n",
       "24       0.663981\n",
       "25       0.619716\n",
       "26       0.520119\n",
       "27       0.482493\n",
       "28       0.369616\n",
       "29       0.343057\n",
       "30       0.349697\n",
       "31       0.289938\n",
       "32       0.241247\n",
       "33       0.243460\n",
       "34       0.177062\n",
       "35       0.177062\n",
       "36       0.185915\n",
       "37       0.121730\n",
       "38       0.112877\n",
       "39       0.106237\n",
       "40       0.110664"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>weird</th>\n    </tr>\n    <tr>\n      <th>tokens</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2.636005</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.061750</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8.388296</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12.443008</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>9.154088</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8.051879</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6.161746</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>6.706210</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>5.924926</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4.273826</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3.463769</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2.908238</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2.671418</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2.598380</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2.441238</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2.169005</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1.859147</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1.702005</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1.350095</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1.157541</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1.031384</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.900801</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.834403</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.663981</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.619716</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.520119</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.482493</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.369616</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.343057</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.349697</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.289938</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.241247</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.243460</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.177062</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.177062</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.185915</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.121730</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.112877</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.106237</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.110664</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "abs_t.groupby('tokens').agg({'weird':'count'}).head(40)*100/len(abs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(45182, 4) (34302, 4)\n(11296, 4) (7759, 4)\n"
     ]
    }
   ],
   "source": [
    "print(abs_t.shape, abs_t[(~abs_t['weird']) & (abs_t['tokens']>=4) & (abs_t['tokens']<=25)].shape)\n",
    "print(abs_d.shape, abs_d[(~abs_d['weird']) & (abs_d['tokens']>=4) & (abs_d['tokens']<=25)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0    17345\n",
       "1    16957\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "abs_t = abs_t[(~abs_t['weird']) & (abs_t['tokens']>=4) & (abs_t['tokens']<=25)].filter([0,1])\n",
    "abs_d =  abs_d[(~abs_d['weird']) & (abs_d['tokens']>=4) & (abs_d['tokens']<=25)].filter([0,1])\n",
    "abs_t.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/shakespeare/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "abs_t.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "abs_d.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd098037a696054ba6333485ba1eda7d4b13de5ba8596b9581751e7239af6bf3f61",
   "display_name": "Python 3.8.8 64-bit ('marvin': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}