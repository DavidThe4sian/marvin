{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "starting-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-sheep",
   "metadata": {},
   "source": [
    "## 1. Formality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "presidential-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formality dataset (GYAFC)\n",
    "data_dir = '../data/GYAFC_Corpus'\n",
    "output_dir = '../data/processed/formality'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "entertainment = f\"{data_dir}/Entertainment_Music\"\n",
    "family = f\"{data_dir}/Family_Relationships\"\n",
    "\n",
    "train_sent = []\n",
    "train_labels = []\n",
    "dev_sent = []\n",
    "dev_labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "invisible-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_ in [entertainment, family]:\n",
    "    for l, label in enumerate(['informal', 'formal']):\n",
    "        with open(f\"{dir_}/train/{label}\",\"r\") as fob:\n",
    "            temp = fob.readlines()\n",
    "            train_sent += temp\n",
    "            train_labels += ([l] * len(temp))\n",
    "        with open(f\"{dir_}/test/{label}\",\"r\") as fob:\n",
    "            temp = fob.readlines()\n",
    "            dev_sent += temp\n",
    "            dev_labels += ([l] * len(temp))\n",
    "            \n",
    "train_sent = [x.strip() for x in train_sent]\n",
    "dev_sent = [x.strip() for x in dev_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "czech-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'sentence': train_sent, 'label': train_labels})\n",
    "dev_df = pd.DataFrame({'sentence': dev_sent, 'label': dev_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "isolated-haiti",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (209124, 3), original dev size : (4849, 3)\n",
      "filtered train size : (207366, 3), filtered dev size : (4803, 3)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "derived-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fluid-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"formality\",\n",
    "    \"description\" : \"Derived from the GYAFC Corpus\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"informal\",\n",
    "        1 : \"formal\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from the GYAFC Corpus; Toy dataset\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"informal\",\n",
    "        1 : \"formal\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-expression",
   "metadata": {},
   "source": [
    "## 2. Short Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "meaningful-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/ShortJokeKaggle/'\n",
    "output_dir = '../data/processed/jokes'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "\n",
    "train_df = pd.read_csv(f\"{data_dir}/train.tsv\", sep=\"\\t\", header=None)\n",
    "dev_df = pd.read_csv(f\"{data_dir}/dev.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "train_df.columns = ['idx', 'source', 'label', 'sentence']\n",
    "dev_df.columns = ['idx', 'source', 'label', 'sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "oriented-checklist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (406682, 5), original dev size : (22512, 5)\n",
      "filtered train size : (357062, 5), filtered dev size : (19797, 5)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "virtual-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "flying-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"jokes\",\n",
    "    \"description\" : \"Derived from SARC, shortjokes.csv, BiasSum\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nojoke\",\n",
    "        1 : \"joke\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from SARC, shortjokes.csv, BiasSum; Toy dataset\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nojoke\",\n",
    "        1 : \"joke\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-lecture",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
