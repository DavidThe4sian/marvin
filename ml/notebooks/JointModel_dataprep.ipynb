{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "starting-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-sheep",
   "metadata": {},
   "source": [
    "## 1. Formality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "presidential-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formality dataset (GYAFC)\n",
    "data_dir = '../data/GYAFC_Corpus'\n",
    "output_dir = '../data/processed/formality'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "entertainment = f\"{data_dir}/Entertainment_Music\"\n",
    "family = f\"{data_dir}/Family_Relationships\"\n",
    "\n",
    "train_sent = []\n",
    "train_labels = []\n",
    "dev_sent = []\n",
    "dev_labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "invisible-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_ in [entertainment, family]:\n",
    "    for l, label in enumerate(['informal', 'formal']):\n",
    "        with open(f\"{dir_}/train/{label}\",\"r\") as fob:\n",
    "            temp = fob.readlines()\n",
    "            train_sent += temp\n",
    "            train_labels += ([l] * len(temp))\n",
    "        with open(f\"{dir_}/test/{label}\",\"r\") as fob:\n",
    "            temp = fob.readlines()\n",
    "            dev_sent += temp\n",
    "            dev_labels += ([l] * len(temp))\n",
    "            \n",
    "train_sent = [x.strip() for x in train_sent]\n",
    "dev_sent = [x.strip() for x in dev_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "czech-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'sentence': train_sent, 'label': train_labels})\n",
    "dev_df = pd.DataFrame({'sentence': dev_sent, 'label': dev_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "isolated-haiti",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (209124, 3), original dev size : (4849, 3)\n",
      "filtered train size : (207366, 3), filtered dev size : (4803, 3)\n",
      "shuffled train size : (169735, 2), shuffled dev size : (42434, 2)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])\n",
    "\n",
    "#mix train and dev, and reseparate them based on train: 80% and dev 20%\n",
    "total_df = pd.concat([train_df,dev_df])\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(total_df) *  0.8)\n",
    "dev_samples = len(total_df) - train_samples\n",
    "\n",
    "dev_df = total_df.tail(dev_samples)\n",
    "train_df = total_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "basic-tractor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(IE: Seeing #2 without #1 knowing.)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yea its Elton The FAG John there ya go !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My Java teacher is dumb and crazy.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What the hell is wrong with you?!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have fun finding out because I don't know the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0                (IE: Seeing #2 without #1 knowing.)      0\n",
       "1           Yea its Elton The FAG John there ya go !      0\n",
       "2                 My Java teacher is dumb and crazy.      1\n",
       "3                  What the hell is wrong with you?!      0\n",
       "4  Have fun finding out because I don't know the ...      1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "derived-order",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Perc :  label\n",
      "0    0.505335\n",
      "1    0.494665\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.507282\n",
      "1    0.492718\n",
      "dtype: float64 \n",
      "\n",
      "Train Split Perc :  label\n",
      "0    0.496\n",
      "1    0.504\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.54\n",
      "1    0.46\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('label').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('label').size()/len(dev_df),'\\n')\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.head(1000).groupby('label').size()/1000,'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.head(200).groupby('label').size()/200,'\\n')\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fluid-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"formality\",\n",
    "    \"description\" : \"Derived from the GYAFC Corpus\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"informal\",\n",
    "        1 : \"formal\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from the GYAFC Corpus; Toy dataset\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"informal\",\n",
    "        1 : \"formal\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-expression",
   "metadata": {},
   "source": [
    "## 2. Short Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "meaningful-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/ShortJokeKaggle/'\n",
    "output_dir = '../data/processed/jokes'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "\n",
    "train_df = pd.read_csv(f\"{data_dir}/train.tsv\", sep=\"\\t\", header=None)\n",
    "dev_df = pd.read_csv(f\"{data_dir}/dev.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "train_df.columns = ['idx', 'source', 'label', 'sentence']\n",
    "dev_df.columns = ['idx', 'source', 'label', 'sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "oriented-checklist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (406682, 5), original dev size : (22512, 5)\n",
      "filtered train size : (357062, 5), filtered dev size : (19797, 5)\n",
      "shuffled train size : (301487, 2), shuffled dev size : (75372, 2)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])\n",
    "\n",
    "\n",
    "#mix train and dev, and reseparate them based on train: 80% and dev 20%\n",
    "total_df = pd.concat([train_df,dev_df])\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(total_df) *  0.8)\n",
    "dev_samples = len(total_df) - train_samples\n",
    "\n",
    "dev_df = total_df.tail(dev_samples)\n",
    "train_df = total_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "egyptian-tucson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>haha, exactly what ive been thinking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usually security guards patrol the grounds at ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insomnia sufferers, look on the bright side. o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have never once hit a drink or treated one b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Outvoted 1-1 by my wife again.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0               haha, exactly what ive been thinking      0\n",
       "1  usually security guards patrol the grounds at ...      0\n",
       "2  Insomnia sufferers, look on the bright side. o...      1\n",
       "3  I have never once hit a drink or treated one b...      1\n",
       "4                     Outvoted 1-1 by my wife again.      1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "virtual-amateur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Perc :  label\n",
      "0    0.433488\n",
      "1    0.566512\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.434339\n",
      "1    0.565661\n",
      "dtype: float64 \n",
      "\n",
      "Train Split Perc :  label\n",
      "0    0.427\n",
      "1    0.573\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.435\n",
      "1    0.565\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('label').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('label').size()/len(dev_df),'\\n')\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.head(1000).groupby('label').size()/1000,'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.head(200).groupby('label').size()/200,'\\n')\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "flying-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"jokes\",\n",
    "    \"description\" : \"Derived from SARC, shortjokes.csv, BiasSum\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nojoke\",\n",
    "        1 : \"joke\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from SARC, shortjokes.csv, BiasSum; Toy dataset\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nojoke\",\n",
    "        1 : \"joke\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-handle",
   "metadata": {},
   "source": [
    "## 3. Metaphor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "chief-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/VUA/'\n",
    "output_dir = '../data/processed/metaphor'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "\n",
    "train_df = pd.read_csv(f\"{data_dir}/train.tsv\", sep=\"\\t\", header=None)\n",
    "dev_df = pd.read_csv(f\"{data_dir}/dev.tsv\", sep=\"\\t\", header=None)\n",
    "test_df = pd.read_csv(f\"{data_dir}/test.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "empty-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns = ['source', 'sentence', 'label']\n",
    "dev_df.columns = ['source', 'sentence', 'label']\n",
    "test_df.columns = ['source', 'sentence', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "obvious-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.concat([dev_df,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cordless-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (15157, 4), original dev size : (7511, 4)\n",
      "filtered train size : (14484, 4), filtered dev size : (7061, 4)\n",
      "shuffled train size : (17236, 2), shuffled dev size : (4309, 2)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])\n",
    "\n",
    "\n",
    "#mix train and dev, and reseparate them based on train: 80% and dev 20%\n",
    "total_df = pd.concat([train_df,dev_df])\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(total_df) *  0.8)\n",
    "dev_samples = len(total_df) - train_samples\n",
    "\n",
    "dev_df = total_df.tail(dev_samples)\n",
    "train_df = total_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "combined-ribbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As his eyes focused he realized he was looking...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The increase will not be matched by dividend r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If the complaint is proved , a nuisance order ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let me chop you that much , you eat up that let</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Workers in blue overalls drifted around us and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  As his eyes focused he realized he was looking...      1\n",
       "1  The increase will not be matched by dividend r...      1\n",
       "2  If the complaint is proved , a nuisance order ...      0\n",
       "3    Let me chop you that much , you eat up that let      0\n",
       "4  Workers in blue overalls drifted around us and...      1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "marine-illness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Perc :  label\n",
      "0    0.715653\n",
      "1    0.284347\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.707357\n",
      "1    0.292643\n",
      "dtype: float64 \n",
      "\n",
      "Train Split Perc :  label\n",
      "0    0.705\n",
      "1    0.295\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.71\n",
      "1    0.29\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('label').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('label').size()/len(dev_df),'\\n')\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.head(1000).groupby('label').size()/1000,'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.head(200).groupby('label').size()/200,'\\n')\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "standard-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"jokes\",\n",
    "    \"description\" : \"Derived from VUA\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nometaphor\",\n",
    "        1 : \"metaphor\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from VUA\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nometaphor\",\n",
    "        1 : \"metaphor\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-scholar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
