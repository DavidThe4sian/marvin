{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "starting-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-sheep",
   "metadata": {},
   "source": [
    "## 1. Formality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "presidential-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formality dataset (GYAFC)\n",
    "data_dir = '../data/GYAFC_Corpus'\n",
    "output_dir = '../data/processed/formality'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "entertainment = f\"{data_dir}/Entertainment_Music\"\n",
    "family = f\"{data_dir}/Family_Relationships\"\n",
    "\n",
    "train_sent = []\n",
    "train_labels = []\n",
    "dev_sent = []\n",
    "dev_labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "invisible-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_ in [entertainment, family]:\n",
    "    for l, label in enumerate(['informal', 'formal']):\n",
    "        with open(f\"{dir_}/train/{label}\",\"r\") as fob:\n",
    "            temp = fob.readlines()\n",
    "            train_sent += temp\n",
    "            train_labels += ([l] * len(temp))\n",
    "        with open(f\"{dir_}/test/{label}\",\"r\") as fob:\n",
    "            temp = fob.readlines()\n",
    "            dev_sent += temp\n",
    "            dev_labels += ([l] * len(temp))\n",
    "            \n",
    "train_sent = [x.strip() for x in train_sent]\n",
    "dev_sent = [x.strip() for x in dev_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "czech-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'sentence': train_sent, 'label': train_labels})\n",
    "dev_df = pd.DataFrame({'sentence': dev_sent, 'label': dev_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "isolated-haiti",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (209124, 3), original dev size : (4849, 3)\n",
      "filtered train size : (207366, 3), filtered dev size : (4803, 3)\n",
      "shuffled train size : (169735, 2), shuffled dev size : (42434, 2)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])\n",
    "\n",
    "#mix train and dev, and reseparate them based on train: 80% and dev 20%\n",
    "total_df = pd.concat([train_df,dev_df])\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(total_df) *  0.8)\n",
    "dev_samples = len(total_df) - train_samples\n",
    "\n",
    "dev_df = total_df.tail(dev_samples)\n",
    "train_df = total_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "basic-tractor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(IE: Seeing #2 without #1 knowing.)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yea its Elton The FAG John there ya go !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My Java teacher is dumb and crazy.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What the hell is wrong with you?!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have fun finding out because I don't know the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0                (IE: Seeing #2 without #1 knowing.)      0\n",
       "1           Yea its Elton The FAG John there ya go !      0\n",
       "2                 My Java teacher is dumb and crazy.      1\n",
       "3                  What the hell is wrong with you?!      0\n",
       "4  Have fun finding out because I don't know the ...      1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "derived-order",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Perc :  label\n",
      "0    0.505335\n",
      "1    0.494665\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.507282\n",
      "1    0.492718\n",
      "dtype: float64 \n",
      "\n",
      "Train Split Perc :  label\n",
      "0    0.496\n",
      "1    0.504\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.54\n",
      "1    0.46\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('label').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('label').size()/len(dev_df),'\\n')\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.head(1000).groupby('label').size()/1000,'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.head(200).groupby('label').size()/200,'\\n')\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fluid-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"formality\",\n",
    "    \"description\" : \"Derived from the GYAFC Corpus\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"informal\",\n",
    "        1 : \"formal\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from the GYAFC Corpus; Toy dataset\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"informal\",\n",
    "        1 : \"formal\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-expression",
   "metadata": {},
   "source": [
    "## 2. Short Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "meaningful-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/ShortJokeKaggle/'\n",
    "output_dir = '../data/processed/jokes'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "\n",
    "train_df = pd.read_csv(f\"{data_dir}/train.tsv\", sep=\"\\t\", header=None)\n",
    "dev_df = pd.read_csv(f\"{data_dir}/dev.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "train_df.columns = ['idx', 'source', 'label', 'sentence']\n",
    "dev_df.columns = ['idx', 'source', 'label', 'sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "oriented-checklist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (406682, 5), original dev size : (22512, 5)\n",
      "filtered train size : (357062, 5), filtered dev size : (19797, 5)\n",
      "shuffled train size : (301487, 2), shuffled dev size : (75372, 2)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])\n",
    "\n",
    "\n",
    "#mix train and dev, and reseparate them based on train: 80% and dev 20%\n",
    "total_df = pd.concat([train_df,dev_df])\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(total_df) *  0.8)\n",
    "dev_samples = len(total_df) - train_samples\n",
    "\n",
    "dev_df = total_df.tail(dev_samples)\n",
    "train_df = total_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "egyptian-tucson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>haha, exactly what ive been thinking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usually security guards patrol the grounds at ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insomnia sufferers, look on the bright side. o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have never once hit a drink or treated one b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Outvoted 1-1 by my wife again.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0               haha, exactly what ive been thinking      0\n",
       "1  usually security guards patrol the grounds at ...      0\n",
       "2  Insomnia sufferers, look on the bright side. o...      1\n",
       "3  I have never once hit a drink or treated one b...      1\n",
       "4                     Outvoted 1-1 by my wife again.      1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "virtual-amateur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Perc :  label\n",
      "0    0.433488\n",
      "1    0.566512\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.434339\n",
      "1    0.565661\n",
      "dtype: float64 \n",
      "\n",
      "Train Split Perc :  label\n",
      "0    0.427\n",
      "1    0.573\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.435\n",
      "1    0.565\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('label').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('label').size()/len(dev_df),'\\n')\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.head(1000).groupby('label').size()/1000,'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.head(200).groupby('label').size()/200,'\\n')\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "flying-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"jokes\",\n",
    "    \"description\" : \"Derived from SARC, shortjokes.csv, BiasSum\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nojoke\",\n",
    "        1 : \"joke\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from SARC, shortjokes.csv, BiasSum; Toy dataset\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nojoke\",\n",
    "        1 : \"joke\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-handle",
   "metadata": {},
   "source": [
    "## 3. Metaphor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "chief-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/VUA/'\n",
    "output_dir = '../data/processed/metaphor'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "\n",
    "train_df = pd.read_csv(f\"{data_dir}/train.tsv\", sep=\"\\t\", header=None)\n",
    "dev_df = pd.read_csv(f\"{data_dir}/dev.tsv\", sep=\"\\t\", header=None)\n",
    "test_df = pd.read_csv(f\"{data_dir}/test.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "empty-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns = ['source', 'sentence', 'label']\n",
    "dev_df.columns = ['source', 'sentence', 'label']\n",
    "test_df.columns = ['source', 'sentence', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "obvious-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.concat([dev_df,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cordless-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (15157, 4), original dev size : (7511, 4)\n",
      "filtered train size : (14484, 4), filtered dev size : (7061, 4)\n",
      "shuffled train size : (17236, 2), shuffled dev size : (4309, 2)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])\n",
    "\n",
    "\n",
    "#mix train and dev, and reseparate them based on train: 80% and dev 20%\n",
    "total_df = pd.concat([train_df,dev_df])\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(total_df) *  0.8)\n",
    "dev_samples = len(total_df) - train_samples\n",
    "\n",
    "dev_df = total_df.tail(dev_samples)\n",
    "train_df = total_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "combined-ribbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As his eyes focused he realized he was looking...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The increase will not be matched by dividend r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If the complaint is proved , a nuisance order ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let me chop you that much , you eat up that let</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Workers in blue overalls drifted around us and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  As his eyes focused he realized he was looking...      1\n",
       "1  The increase will not be matched by dividend r...      1\n",
       "2  If the complaint is proved , a nuisance order ...      0\n",
       "3    Let me chop you that much , you eat up that let      0\n",
       "4  Workers in blue overalls drifted around us and...      1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "marine-illness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Perc :  label\n",
      "0    0.715653\n",
      "1    0.284347\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.707357\n",
      "1    0.292643\n",
      "dtype: float64 \n",
      "\n",
      "Train Split Perc :  label\n",
      "0    0.705\n",
      "1    0.295\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.71\n",
      "1    0.29\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('label').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('label').size()/len(dev_df),'\\n')\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.head(1000).groupby('label').size()/1000,'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.head(200).groupby('label').size()/200,'\\n')\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "standard-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"jokes\",\n",
    "    \"description\" : \"Derived from VUA\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nometaphor\",\n",
    "        1 : \"metaphor\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from VUA\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nometaphor\",\n",
    "        1 : \"metaphor\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-scholar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Filter Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, json, shutil\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## formality\n",
    "data_input = '../data/processed/formality'\n",
    "data_output = '../data/processed_filtered/formality'\n",
    "\n",
    "formality_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "formality_d = pd.read_csv(f'{data_input}/dev.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 2280015), ('e', 982719), ('t', 770665), ('o', 756069), ('a', 641948), ('i', 572556), ('n', 558029), ('s', 509686), ('h', 457329), ('r', 428444), ('l', 360810), ('u', 313756), ('d', 292019), ('y', 262229), ('m', 221332), ('.', 220916), ('w', 188720), ('g', 176637), ('c', 170854), ('f', 155649), ('b', 134281), ('p', 118800), ('k', 110853), ('I', 103104), ('v', 95533), (',', 84555), ('T', 47063), (\"'\", 45414), ('!', 40085), ('E', 33123), ('A', 32658), ('S', 30865), ('O', 29809), ('H', 26929), ('?', 26852), ('N', 24120), ('Y', 23070), ('D', 20951), ('L', 19131), ('M', 19070), ('W', 18745), ('j', 18628), ('R', 17674), ('B', 16991), ('C', 14101), ('G', 13595), ('x', 13413), ('U', 11484), ('P', 11439), ('F', 9829), ('-', 9403), ('\"', 9262), ('q', 7274), ('z', 7037), ('K', 6774), (')', 6382), ('1', 6195), ('J', 6089), ('0', 5422), ('V', 5253), ('2', 5204), (':', 4589), ('(', 4524), ('3', 2791), ('5', 2565), ('*', 2404), ('4', 2355), (';', 2335), ('9', 2076), ('/', 1909), ('8', 1758), ('6', 1652), ('&', 1452), ('7', 1268), ('X', 811), ('Z', 769), ('_', 727), ('@', 667), ('$', 629), ('Q', 610), ('=', 481), ('#', 361), ('>', 231), ('^', 199), ('%', 178), ('~', 167), ('`', 163), (']', 147), ('+', 134), ('[', 116), ('<', 100), ('’', 100), ('¨', 78), ('´', 68), ('}', 41), ('{', 28), ('“', 27), ('—', 27), ('…', 22), ('”', 21), ('é', 21), ('|', 20), ('¡', 20), ('£', 18), ('Ü', 13), ('–', 12), ('ı', 10), ('♥', 10), ('¿', 6), ('ñ', 5), ('·', 4), ('\\\\', 4), ('\\u200b', 4), ('ü', 3), ('¢', 3), ('ö', 3), ('§', 3), ('á', 3), ('è', 2), ('˝', 2), ('嘉', 2), ('義', 2), ('人', 2), ('因', 2), ('為', 2), ('綠', 2), ('豆', 2), ('加', 2), ('薏', 2), ('仁', 2), ('©', 2), ('™', 2), ('‘', 2), ('☺', 2), ('ŕ', 2), ('ā', 2), ('ə', 2), ('®', 2), ('š', 2), ('†', 2), ('Æ', 1), ('恭', 1), ('喜', 1), ('發', 1), ('財', 1), ('♡', 1), ('½', 1), ('í', 1), ('ƒ', 1), ('Ä', 1), ('ù', 1), ('س', 1), ('ا', 1), ('م', 1), ('ه', 1), ('º', 1), ('¹', 1), ('œ', 1), ('•', 1), ('ó', 1), ('►', 1), ('λ', 1), ('◄', 1), ('à', 1), ('»', 1), ('ĕ', 1), ('û', 1), ('ï', 1)])\n\n [' ', 'e', 't', 'o', 'a', 'i', 'n', 's', 'h', 'r', 'l', 'u', 'd', 'y', 'm', '.', 'w', 'g', 'c', 'f', 'b', 'p', 'k', 'I', 'v', ',', 'T', \"'\", '!', 'E', 'A', 'S', 'O', 'H', '?', 'N', 'Y', 'D', 'L', 'M', 'W', 'j', 'R', 'B', 'C', 'G', 'x', 'U', 'P', 'F', '-', '\"', 'q', 'z', 'K', ')', '1', 'J', '0', 'V', '2', ':', '(', '3', '5', '*', '4', ';', '9', '/', '8', '6', '&', '7', 'X', 'Z', '_', '@', '$', 'Q', '=', '#', '>', '^', '%', '~', '`', ']', '+', '[', '<', '’', '¨', '´', '}', '{', '“', '—', '…', '”', 'é', '|', '¡', '£', 'Ü', '–', 'ı', '♥', '¿', 'ñ', '·', '\\\\', '\\u200b', 'ü', '¢', 'ö', '§', 'á', 'è', '˝', '嘉', '義', '人', '因', '為', '綠', '豆', '加', '薏', '仁', '©', '™', '‘', '☺', 'ŕ', 'ā', 'ə', '®', 'š', '†', 'Æ', '恭', '喜', '發', '財', '♡', '½', 'í', 'ƒ', 'Ä', 'ù', 'س', 'ا', 'م', 'ه', 'º', '¹', 'œ', '•', 'ó', '►', 'λ', '◄', 'à', '»', 'ĕ', 'û', 'ï']\n"
     ]
    }
   ],
   "source": [
    "p = OrderedDict(Counter(list(' '.join(formality_t[0]) + ' '.join(formality_d[0]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "formality_t['weird']= formality_t[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '_', '@', 'Q', '=', '#', '>', '^', '%', '~', '`', ']', '+', '[', '<', '’', '¨', '´', '}', '{', '“', '—', '…', '”', 'é', '|', '¡', 'Ü', '–', 'ı', '♥', '¿', 'ñ', '·', '\\\\', '\\u200b', 'ü', '¢', 'ö', '§', 'á', 'è', '˝', '嘉', '義', '人', '因', '為', '綠', '豆', '加', '薏', '仁', '©', '™', '‘', '☺', 'ŕ', 'ā', 'ə', '®', 'š', '†', 'Æ', '恭', '喜', '發', '財', '♡', '½', 'í', 'ƒ', 'Ä', 'ù', 'س', 'ا', 'م', 'ه', 'º', '¹', 'œ', '•', 'ó', '►', 'λ', '◄', 'à', '»', 'ĕ', 'û', 'ï', '(',')',':','--', '....', '!!!', 'www', 'http']])) )\n",
    "formality_d['weird']= formality_d[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '_', '@', 'Q', '=', '#', '>', '^', '%', '~', '`', ']', '+', '[', '<', '’', '¨', '´', '}', '{', '“', '—', '…', '”', 'é', '|', '¡', 'Ü', '–', 'ı', '♥', '¿', 'ñ', '·', '\\\\', '\\u200b', 'ü', '¢', 'ö', '§', 'á', 'è', '˝', '嘉', '義', '人', '因', '為', '綠', '豆', '加', '薏', '仁', '©', '™', '‘', '☺', 'ŕ', 'ā', 'ə', '®', 'š', '†', 'Æ', '恭', '喜', '發', '財', '♡', '½', 'í', 'ƒ', 'Ä', 'ù', 'س', 'ا', 'م', 'ه', 'º', '¹', 'œ', '•', 'ó', '►', 'λ', '◄', 'à', '»', 'ĕ', 'û', 'ï', '(',')',':','--', '....', '!!!', 'www', 'http']])) )\n",
    "\n",
    "formality_t['tokens'] = formality_t[0].apply(lambda x: len(x.split(' ')))\n",
    "formality_d['tokens'] = formality_d[0].apply(lambda x: len(x.split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(169735, 4) (154938, 4)\n(42434, 4) (38709, 4)\n"
     ]
    }
   ],
   "source": [
    "print(formality_t.shape, formality_t[(~formality_t['weird']) & (formality_t['tokens']>=5) & (formality_t['tokens']<=30)].shape)\n",
    "print(formality_d.shape, formality_d[(~formality_d['weird']) & (formality_d['tokens']>=5) & (formality_d['tokens']<=30)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "formality_t = formality_t[(~formality_t['weird']) & (formality_t['tokens']>=5) & (formality_t['tokens']<=30)].filter([0,1])\n",
    "formality_d = formality_d[(~formality_d['weird']) & (formality_d['tokens']>=5) & (formality_d['tokens']<=30)].filter([0,1])\n",
    "\n",
    "formality_t = formality_t.sample(frac=1).reset_index(drop=True)\n",
    "formality_d = formality_d.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "formality_t = formality_t[:50000]\n",
    "formality_d = formality_d[:12500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0    23560\n",
       "1    26440\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "formality_t.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/formality/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "formality_t.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "formality_d.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arousal\n",
    "data_input = '../data/processed/arousal'\n",
    "data_output = '../data/processed_filtered/arousal'\n",
    "\n",
    "arousal_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "arousal_d = pd.read_csv(f'{data_input}/dev.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 144718), ('e', 79546), ('t', 57680), ('a', 53839), ('o', 52043), ('n', 46969), ('i', 46767), ('s', 43431), ('r', 40361), ('h', 31456), ('l', 28804), ('d', 24916), ('u', 19126), ('c', 18819), ('m', 15939), ('f', 14100), ('g', 13587), ('p', 13325), ('y', 11882), ('w', 11653), ('b', 9178), (',', 8470), ('.', 7980), ('v', 6690), ('k', 5292), ('I', 3115), ('T', 2694), ('A', 2191), ('S', 2020), ('-', 1960), (\"'\", 1796), ('C', 1693), ('\"', 1616), ('0', 1516), ('M', 1386), ('x', 1325), ('1', 1148), ('H', 1070), ('N', 1056), ('W', 1051), ('B', 1012), ('P', 959), ('’', 881), ('E', 854), ('R', 848), ('F', 831), ('j', 767), ('2', 757), ('D', 720), ('9', 696), ('L', 691), ('z', 672), ('q', 638), (':', 638), ('O', 637), ('”', 605), ('“', 601), ('G', 577), ('5', 551), ('Y', 491), ('K', 470), (')', 470), ('3', 469), ('(', 465), ('U', 455), ('J', 412), ('?', 380), ('8', 344), ('4', 340), ('V', 320), ('7', 315), ('6', 307), ('$', 301), (';', 276), ('—', 153), ('!', 140), ('/', 119), ('\\u2002', 116), ('–', 86), ('Z', 76), ('&', 71), ('%', 68), ('♭', 45), ('Q', 37), ('…', 34), ('\\xad', 30), ('♯', 27), ('_', 23), ('[', 22), (']', 22), ('X', 20), ('ó', 19), ('#', 16), ('‘', 14), ('·', 12), ('=', 11), ('+', 11), ('\\xa0', 10), ('|', 7), ('`', 7), ('é', 5), ('<', 4), ('>', 4), ('ç', 4), ('®', 4), ('*', 4), ('ñ', 3), ('ã', 3), ('@', 3), ('è', 2), ('õ', 2), ('♮', 1), ('ü', 1), ('í', 1)])\n\n [' ', 'e', 't', 'a', 'o', 'n', 'i', 's', 'r', 'h', 'l', 'd', 'u', 'c', 'm', 'f', 'g', 'p', 'y', 'w', 'b', ',', '.', 'v', 'k', 'I', 'T', 'A', 'S', '-', \"'\", 'C', '\"', '0', 'M', 'x', '1', 'H', 'N', 'W', 'B', 'P', '’', 'E', 'R', 'F', 'j', '2', 'D', '9', 'L', 'z', 'q', ':', 'O', '”', '“', 'G', '5', 'Y', 'K', ')', '3', '(', 'U', 'J', '?', '8', '4', 'V', '7', '6', '$', ';', '—', '!', '/', '\\u2002', '–', 'Z', '&', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í']\n"
     ]
    }
   ],
   "source": [
    "p = OrderedDict(Counter(list(' '.join(arousal_t[0]) + ' '.join(arousal_d[0]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "arousal_t['weird']= arousal_t[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '\\u2002', '–', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í', '(',')',':','--', '....', '!!!', 'www', 'http','_',':',\"”\",\"“\"]])) )\n",
    "arousal_d['weird']= arousal_d[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in ['\\u2002', '–', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í', '(',')',':','--', '....', '!!!', 'www', 'http','_',':',\"”\",\"“\"]])) )\n",
    "\n",
    "arousal_t['tokens'] = arousal_t[0].apply(lambda x: len(x.split(' ')))\n",
    "arousal_d['tokens'] = arousal_d[0].apply(lambda x: len(x.split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(6901, 4) (5178, 4)\n(1726, 4) (1320, 4)\n"
     ]
    }
   ],
   "source": [
    "print(arousal_t.shape, arousal_t[(~arousal_t['weird']) & (arousal_t['tokens']>=5) & (arousal_t['tokens']<=40)].shape)\n",
    "print(arousal_d.shape, arousal_d[(~arousal_d['weird']) & (arousal_d['tokens']>=5) & (arousal_d['tokens']<=40)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0    2422\n",
       "1    4479\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "arousal_t.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/arousal/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "arousal_t =  arousal_t[(~arousal_t['weird']) & (arousal_t['tokens']>=5) & (arousal_t['tokens']<=40)].filter([0,1])\n",
    "arousal_d = arousal_d[(~arousal_d['weird']) & (arousal_d['tokens']>=5) & (arousal_d['tokens']<=40)].filter([0,1])\n",
    "\n",
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "arousal_t.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "arousal_d.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Emo\n",
    "data_input = '../data/processed/emo'\n",
    "data_output = '../data/processed_filtered/emo'\n",
    "\n",
    "emo_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "emo_d = pd.read_csv(f'{data_input}/dev.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 313640), ('e', 119655), ('o', 97454), ('t', 94683), ('a', 87416), ('i', 68036), ('n', 67804), ('s', 61513), ('r', 56078), ('h', 53958), ('l', 48032), ('u', 37771), ('d', 37031), ('y', 35929), ('.', 31654), ('m', 31511), ('g', 27342), ('c', 23940), ('w', 23914), ('f', 19013), ('p', 17490), ('k', 16851), ('b', 16071), ('I', 14849), ('v', 11605), (',', 9914), (\"'\", 9734), ('!', 9656), ('T', 6468), ('@', 4690), ('S', 3578), ('?', 3516), ('W', 3406), ('H', 3190), ('A', 3087), ('O', 2970), ('M', 2844), ('Y', 2767), ('j', 2736), ('x', 2387), ('L', 2042), ('’', 1973), ('D', 1930), ('N', 1921), ('B', 1907), ('G', 1905), ('C', 1834), ('E', 1704), ('/', 1500), ('-', 1358), ('z', 1292), ('P', 1256), ('R', 1202), (';', 1180), ('q', 1050), ('F', 1049), ('J', 1021), ('&', 993), ('0', 976), ('1', 956), (':', 875), ('2', 865), ('K', 805), ('3', 648), ('U', 639), ('V', 596), ('4', 555), ('_', 555), ('5', 435), ('6', 399), (')', 384), ('8', 366), ('7', 344), ('(', 330), ('9', 321), ('*', 264), ('#', 226), ('Z', 128), ('X', 125), ('=', 95), ('Q', 95), ('ï', 90), ('¿', 90), ('½', 90), ('$', 85), ('~', 77), ('+', 50), (']', 44), ('%', 30), ('\"', 25), ('[', 20), ('^', 20), ('`', 13), ('”', 12), ('“', 10), ('|', 9), ('‘', 9), ('\\\\', 6), ('—', 5), ('–', 4), ('。', 3), ('Â', 1), ('¡', 1), ('′', 1), ('{', 1), ('}', 1)])\n\n [' ', 'e', 'o', 't', 'a', 'i', 'n', 's', 'r', 'h', 'l', 'u', 'd', 'y', '.', 'm', 'g', 'c', 'w', 'f', 'p', 'k', 'b', 'I', 'v', ',', \"'\", '!', 'T', '@', 'S', '?', 'W', 'H', 'A', 'O', 'M', 'Y', 'j', 'x', 'L', '’', 'D', 'N', 'B', 'G', 'C', 'E', '/', '-', 'z', 'P', 'R', ';', 'q', 'F', 'J', '&', '0', '1', ':', '2', 'K', '3', 'U', 'V', '4', '_', '5', '6', ')', '8', '7', '(', '9', '*', '#', 'Z', 'X', '=', 'Q', 'ï', '¿', '½', '$', '~', '+', ']', '%', '\"', '[', '^', '`', '”', '“', '|', '‘', '\\\\', '—', '–', '。', 'Â', '¡', '′', '{', '}']\n"
     ]
    }
   ],
   "source": [
    "p = OrderedDict(Counter(list(' '.join(emo_t[0]) + ' '.join(emo_d[0]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_t['weird']= emo_t[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '\\u2002', '–', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í', '(',')',':','--', '....', '!!!', 'www', 'http','_',':',\"”\",\"“\",'ï', '¿', '½', '$', '~', '+', ']', '%', '[', '^', '`', '”', '“', '|', '‘', '\\\\', '—', '–', '。', 'Â', '¡', '′', '{', '}']])) )\n",
    "emo_d['weird']= emo_d[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '\\u2002', '–', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í', '(',')',':','--', '....', '!!!', 'www', 'http','_',':',\"”\",\"“\",'ï', '¿', '½', '$', '~', '+', ']', '%', '[', '^', '`', '”', '“', '|', '‘', '\\\\', '—', '–', '。', 'Â', '¡', '′', '{', '}']])) )\n",
    "\n",
    "emo_t['tokens'] = emo_t[0].apply(lambda x: len(x.split(' ')))\n",
    "emo_d['tokens'] = emo_d[0].apply(lambda x: len(x.split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(17617, 4) (12708, 4)\n(4405, 4) (3230, 4)\n"
     ]
    }
   ],
   "source": [
    "print(emo_t.shape, emo_t[(~emo_t['weird']) & (emo_t['tokens']>=5) & (emo_t['tokens']<=40)].shape)\n",
    "print(emo_d.shape, emo_d[(~emo_d['weird']) & (emo_d['tokens']>=5) & (emo_d['tokens']<=40)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_t = emo_t[(~emo_t['weird']) & (emo_t['tokens']>=5) & (emo_t['tokens']<=40)].filter([0,1])\n",
    "emo_d =  emo_d[(~emo_d['weird']) & (emo_d['tokens']>=5) & (emo_d['tokens']<=40)].filter([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0     2611\n",
       "1    10097\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "emo_t.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/emo/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "emo_t.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "emo_d.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}