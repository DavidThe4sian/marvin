{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "starting-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-sheep",
   "metadata": {},
   "source": [
    "## 1. Formality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "presidential-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formality dataset (GYAFC)\n",
    "data_dir = '../data/GYAFC_Corpus'\n",
    "output_dir = '../data/processed/formality'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "entertainment = f\"{data_dir}/Entertainment_Music\"\n",
    "family = f\"{data_dir}/Family_Relationships\"\n",
    "\n",
    "train_sent = []\n",
    "train_labels = []\n",
    "dev_sent = []\n",
    "dev_labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "invisible-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_ in [entertainment, family]:\n",
    "    for l, label in enumerate(['informal', 'formal']):\n",
    "        with open(f\"{dir_}/train/{label}\",\"r\") as fob:\n",
    "            temp = fob.readlines()\n",
    "            train_sent += temp\n",
    "            train_labels += ([l] * len(temp))\n",
    "        with open(f\"{dir_}/test/{label}\",\"r\") as fob:\n",
    "            temp = fob.readlines()\n",
    "            dev_sent += temp\n",
    "            dev_labels += ([l] * len(temp))\n",
    "            \n",
    "train_sent = [x.strip() for x in train_sent]\n",
    "dev_sent = [x.strip() for x in dev_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "czech-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'sentence': train_sent, 'label': train_labels})\n",
    "dev_df = pd.DataFrame({'sentence': dev_sent, 'label': dev_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "isolated-haiti",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (209124, 3), original dev size : (4849, 3)\n",
      "filtered train size : (207366, 3), filtered dev size : (4803, 3)\n",
      "shuffled train size : (169735, 2), shuffled dev size : (42434, 2)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])\n",
    "\n",
    "#mix train and dev, and reseparate them based on train: 80% and dev 20%\n",
    "total_df = pd.concat([train_df,dev_df])\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(total_df) *  0.8)\n",
    "dev_samples = len(total_df) - train_samples\n",
    "\n",
    "dev_df = total_df.tail(dev_samples)\n",
    "train_df = total_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "basic-tractor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(IE: Seeing #2 without #1 knowing.)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yea its Elton The FAG John there ya go !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My Java teacher is dumb and crazy.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What the hell is wrong with you?!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have fun finding out because I don't know the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0                (IE: Seeing #2 without #1 knowing.)      0\n",
       "1           Yea its Elton The FAG John there ya go !      0\n",
       "2                 My Java teacher is dumb and crazy.      1\n",
       "3                  What the hell is wrong with you?!      0\n",
       "4  Have fun finding out because I don't know the ...      1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "derived-order",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Perc :  label\n",
      "0    0.505335\n",
      "1    0.494665\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.507282\n",
      "1    0.492718\n",
      "dtype: float64 \n",
      "\n",
      "Train Split Perc :  label\n",
      "0    0.496\n",
      "1    0.504\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.54\n",
      "1    0.46\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('label').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('label').size()/len(dev_df),'\\n')\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.head(1000).groupby('label').size()/1000,'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.head(200).groupby('label').size()/200,'\\n')\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fluid-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"formality\",\n",
    "    \"description\" : \"Derived from the GYAFC Corpus\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"informal\",\n",
    "        1 : \"formal\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from the GYAFC Corpus; Toy dataset\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"informal\",\n",
    "        1 : \"formal\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-expression",
   "metadata": {},
   "source": [
    "## 2. Short Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "meaningful-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/ShortJokeKaggle/'\n",
    "output_dir = '../data/processed/jokes'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "\n",
    "train_df = pd.read_csv(f\"{data_dir}/train.tsv\", sep=\"\\t\", header=None)\n",
    "dev_df = pd.read_csv(f\"{data_dir}/dev.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "train_df.columns = ['idx', 'source', 'label', 'sentence']\n",
    "dev_df.columns = ['idx', 'source', 'label', 'sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "oriented-checklist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (406682, 5), original dev size : (22512, 5)\n",
      "filtered train size : (357062, 5), filtered dev size : (19797, 5)\n",
      "shuffled train size : (301487, 2), shuffled dev size : (75372, 2)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])\n",
    "\n",
    "\n",
    "#mix train and dev, and reseparate them based on train: 80% and dev 20%\n",
    "total_df = pd.concat([train_df,dev_df])\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(total_df) *  0.8)\n",
    "dev_samples = len(total_df) - train_samples\n",
    "\n",
    "dev_df = total_df.tail(dev_samples)\n",
    "train_df = total_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "egyptian-tucson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>haha, exactly what ive been thinking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usually security guards patrol the grounds at ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insomnia sufferers, look on the bright side. o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have never once hit a drink or treated one b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Outvoted 1-1 by my wife again.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0               haha, exactly what ive been thinking      0\n",
       "1  usually security guards patrol the grounds at ...      0\n",
       "2  Insomnia sufferers, look on the bright side. o...      1\n",
       "3  I have never once hit a drink or treated one b...      1\n",
       "4                     Outvoted 1-1 by my wife again.      1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "virtual-amateur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Perc :  label\n",
      "0    0.433488\n",
      "1    0.566512\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.434339\n",
      "1    0.565661\n",
      "dtype: float64 \n",
      "\n",
      "Train Split Perc :  label\n",
      "0    0.427\n",
      "1    0.573\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.435\n",
      "1    0.565\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('label').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('label').size()/len(dev_df),'\\n')\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.head(1000).groupby('label').size()/1000,'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.head(200).groupby('label').size()/200,'\\n')\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "flying-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"jokes\",\n",
    "    \"description\" : \"Derived from SARC, shortjokes.csv, BiasSum\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nojoke\",\n",
    "        1 : \"joke\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from SARC, shortjokes.csv, BiasSum; Toy dataset\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nojoke\",\n",
    "        1 : \"joke\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-handle",
   "metadata": {},
   "source": [
    "## 3. Metaphor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "chief-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/VUA/'\n",
    "output_dir = '../data/processed/metaphor'\n",
    "output_dir_toy = f'{output_dir}_toy'\n",
    "\n",
    "train_df = pd.read_csv(f\"{data_dir}/train.tsv\", sep=\"\\t\", header=None)\n",
    "dev_df = pd.read_csv(f\"{data_dir}/dev.tsv\", sep=\"\\t\", header=None)\n",
    "test_df = pd.read_csv(f\"{data_dir}/test.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "empty-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns = ['source', 'sentence', 'label']\n",
    "dev_df.columns = ['source', 'sentence', 'label']\n",
    "test_df.columns = ['source', 'sentence', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "obvious-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.concat([dev_df,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cordless-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train size : (15157, 4), original dev size : (7511, 4)\n",
      "filtered train size : (14484, 4), filtered dev size : (7061, 4)\n",
      "shuffled train size : (17236, 2), shuffled dev size : (4309, 2)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataset\n",
    "train_df['words'] = train_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "dev_df['words'] = dev_df['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "print(f\"original train size : {train_df.shape}, original dev size : {dev_df.shape}\")\n",
    "\n",
    "# Filter out sentences with tokens less than 5 and greater than 64\n",
    "train_df = train_df[(train_df['words']>4) & (train_df['words']<64)]\n",
    "dev_df = dev_df[(dev_df['words']>4) & (dev_df['words']<64)]\n",
    "print(f\"filtered train size : {train_df.shape}, filtered dev size : {dev_df.shape}\")\n",
    "\n",
    "\n",
    "#Select necessary columns\n",
    "train_df = train_df.filter(['sentence','label'])\n",
    "dev_df = dev_df.filter(['sentence','label'])\n",
    "\n",
    "\n",
    "#mix train and dev, and reseparate them based on train: 80% and dev 20%\n",
    "total_df = pd.concat([train_df,dev_df])\n",
    "total_df = total_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(total_df) *  0.8)\n",
    "dev_samples = len(total_df) - train_samples\n",
    "\n",
    "dev_df = total_df.tail(dev_samples)\n",
    "train_df = total_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "combined-ribbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As his eyes focused he realized he was looking...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The increase will not be matched by dividend r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If the complaint is proved , a nuisance order ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let me chop you that much , you eat up that let</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Workers in blue overalls drifted around us and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  As his eyes focused he realized he was looking...      1\n",
       "1  The increase will not be matched by dividend r...      1\n",
       "2  If the complaint is proved , a nuisance order ...      0\n",
       "3    Let me chop you that much , you eat up that let      0\n",
       "4  Workers in blue overalls drifted around us and...      1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "marine-illness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Perc :  label\n",
      "0    0.715653\n",
      "1    0.284347\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.707357\n",
      "1    0.292643\n",
      "dtype: float64 \n",
      "\n",
      "Train Split Perc :  label\n",
      "0    0.705\n",
      "1    0.295\n",
      "dtype: float64 \n",
      "\n",
      "Dev Split Perc :  label\n",
      "0    0.71\n",
      "1    0.29\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(f'{output_dir}/train.csv', index=False, header=False)\n",
    "dev_df.to_csv(f'{output_dir}/dev.csv', index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('label').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('label').size()/len(dev_df),'\\n')\n",
    "\n",
    "if not os.path.exists(output_dir_toy):\n",
    "    os.makedirs(output_dir_toy)\n",
    "    \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.head(1000).groupby('label').size()/1000,'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.head(200).groupby('label').size()/200,'\\n')\n",
    "\n",
    "train_df.head(1000).to_csv(f'{output_dir_toy}/train.csv', index=False, header=False)\n",
    "dev_df.head(200).to_csv(f'{output_dir_toy}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "standard-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"jokes\",\n",
    "    \"description\" : \"Derived from VUA\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nometaphor\",\n",
    "        1 : \"metaphor\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)\n",
    "    \n",
    "config = {\n",
    "    \"name\" : \"formality_toy\",\n",
    "    \"description\" : \"Derived from VUA\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"nometaphor\",\n",
    "        1 : \"metaphor\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir_toy}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "source": [
    "## 4. Abstracts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_path = '../data/temp/abstract_sents.csv'\n",
    "abstract_train_path = '../data/processed/abstract/train.csv'\n",
    "abstract_dev_path = '../data/processed/abstract/dev.csv'\n",
    "abstract_config_path = '../data/processed/abstract/config.json'\n",
    "output_dir = '../data/processed/abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "57116"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "with open(abstract_path) as fob:\n",
    "    abs_data = fob.readlines()\n",
    "len(abs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "formality_df = pd.read_csv('../data/processed_filtered/formality_full/train.csv', header=None)\n",
    "formality_df[1] = 0\n",
    "formality_df.to_csv('../data/processed_filtered/formality_full/negative.csv',header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_data = [x.strip().strip('\\\"').strip() for x in abs_data]\n",
    "abs_df = pd.DataFrame({'abs':abs_data, 'class': 1})\n",
    "formality = pd.read_csv('../data/processed_filtered/formality_full/negative.csv', header=None)\n",
    "formality.columns = ['abs','class']\n",
    "formality = formality.sample(n=60000,replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                      abs  class\n",
       "0       38% are observed for OSIRIS and IriCore, respe...      1\n",
       "1       A smartphone with a mobile app is connected to...      1\n",
       "2       Good scalability is achieved through flexible ...      1\n",
       "3       This problem is further compounded due to the ...      1\n",
       "4       This deep learning based technique is shown to...      1\n",
       "...                                                   ...    ...\n",
       "11295   I believe that if you really are in love with ...      0\n",
       "98270   Final destination 3 unless u don't like really...      0\n",
       "144365  YOu should be asking your self what kind of pe...      0\n",
       "128053             just go for a tiny bit more each time.      0\n",
       "148812  I do not believe she is involved with it in an...      0\n",
       "\n",
       "[117116 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>abs</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>38% are observed for OSIRIS and IriCore, respe...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A smartphone with a mobile app is connected to...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Good scalability is achieved through flexible ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>This problem is further compounded due to the ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>This deep learning based technique is shown to...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11295</th>\n      <td>I believe that if you really are in love with ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>98270</th>\n      <td>Final destination 3 unless u don't like really...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>144365</th>\n      <td>YOu should be asking your self what kind of pe...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>128053</th>\n      <td>just go for a tiny bit more each time.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>148812</th>\n      <td>I do not believe she is involved with it in an...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>117116 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "abs_df = pd.concat([abs_df,formality], axis=0)\n",
    "abs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shuffled train size : (93692, 2), shuffled dev size : (23424, 2)\n"
     ]
    }
   ],
   "source": [
    "abs_df = abs_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_samples = int(len(abs_df) *  0.8)\n",
    "dev_samples = len(abs_df) - train_samples\n",
    "\n",
    "dev_df = abs_df.tail(dev_samples)\n",
    "train_df = abs_df.head(train_samples)\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Split Perc :  class\n0    0.512723\n1    0.487277\ndtype: float64 \n\nDev Split Perc :  class\n0    0.510673\n1    0.489327\ndtype: float64 \n\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(abstract_train_path, index=False, header=False)\n",
    "dev_df.to_csv(abstract_dev_path, index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('class').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('class').size()/len(dev_df),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"abstract\",\n",
    "    \"description\" : \"Derived from Abstracts of papers\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"noabstract\",\n",
    "        1 : \"abstract\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "source": [
    "## 5. shakespeare"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shake_path = '../data/temp/shake_original.txt'\n",
    "shake_mod_path = '../data/temp/shake_modern.txt'\n",
    "shake_train_path = '../data/processed/shakespeare/train.csv'\n",
    "shake_dev_path = '../data/processed/shakespeare/dev.csv'\n",
    "shake_config_path = '../data/processed/shakespeare/config.json'\n",
    "output_dir = '../data/processed/shakespeare'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "28239\n28239\n"
     ]
    }
   ],
   "source": [
    "with open(shake_path) as fob:\n",
    "    shake = fob.readlines()\n",
    "print(len(shake))\n",
    "\n",
    "with open(shake_mod_path) as fob:\n",
    "    shake_mod = fob.readlines()\n",
    "print(len(shake_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   shake  class\n",
       "0        I have a mind to strike thee ere thou speak’st.      1\n",
       "1      Yet if thou say Antony lives, is well, Or frie...      1\n",
       "2                                      Madam, he’s well.      1\n",
       "3                                             Well said.      1\n",
       "4                               And friends with Caesar.      1\n",
       "...                                                  ...    ...\n",
       "28234  What a thrice-double ass Was I to take this dr...      1\n",
       "28235                                       Go to, away!      1\n",
       "28236  to Stephano and Trinculo] Hence, and bestow yo...      1\n",
       "28237                               Or stole it, rather.      1\n",
       "28238  I long To hear the story of your life, which m...      1\n",
       "\n",
       "[28239 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>shake</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I have a mind to strike thee ere thou speak’st.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Yet if thou say Antony lives, is well, Or frie...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Madam, he’s well.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Well said.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>And friends with Caesar.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>28234</th>\n      <td>What a thrice-double ass Was I to take this dr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28235</th>\n      <td>Go to, away!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28236</th>\n      <td>to Stephano and Trinculo] Hence, and bestow yo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28237</th>\n      <td>Or stole it, rather.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28238</th>\n      <td>I long To hear the story of your life, which m...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>28239 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "shake = [x.strip().strip('\\\"') for x in shake]\n",
    "shake_mod = [x.strip().strip('\\\"') for x in shake_mod]\n",
    "shake_df = pd.DataFrame({'shake':shake, 'class': 1})\n",
    "shake_mod_df = pd.DataFrame({'shake':shake_mod, 'class': 0})\n",
    "shake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shuffled train size : (45182, 2), shuffled dev size : (11296, 2)\n"
     ]
    }
   ],
   "source": [
    "train_samples = int(len(shake_df) *  0.8)\n",
    "dev_samples = len(shake_df) - train_samples\n",
    "\n",
    "dev_df = pd.concat([shake_df.tail(dev_samples),shake_mod_df.tail(dev_samples)])\n",
    "train_df = pd.concat([shake_df.head(train_samples),shake_mod_df.head(train_samples)])\n",
    "print(f\"shuffled train size : {train_df.shape}, shuffled dev size : {dev_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Split Perc :  class\n0    0.5\n1    0.5\ndtype: float64 \n\nDev Split Perc :  class\n0    0.5\n1    0.5\ndtype: float64 \n\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "train_df.to_csv(shake_train_path, index=False, header=False)\n",
    "dev_df.to_csv(shake_dev_path, index=False, header=False)\n",
    "\n",
    "print(\"Train Split Perc : \", train_df.groupby('class').size()/len(train_df),'\\n')\n",
    "print(\"Dev Split Perc : \", dev_df.groupby('class').size()/len(dev_df),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Config\n",
    "config = {\n",
    "    \"name\" : \"shakespeare\",\n",
    "    \"description\" : \"Derived from Shakespeare Plays (https://github.com/cocoxu/Shakespeare)\",\n",
    "    \"input_files\" : {\n",
    "        \"train\" : \"train.csv\",\n",
    "        \"dev\" : \"dev.csv\"\n",
    "    },\n",
    "    \"classes\" : {\n",
    "        0 : \"noshakespeare\",\n",
    "        1 : \"shakespeare\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/config.json\",\"w\") as fob:\n",
    "    json.dump(config, fob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Filter Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, json, shutil\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## formality\n",
    "data_input = '../data/processed/formality'\n",
    "data_output = '../data/processed_filtered/formality'\n",
    "data_output_full = '../data/processed_filtered/formality_full'\n",
    "\n",
    "formality_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "formality_d = pd.read_csv(f'{data_input}/dev.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 2280015), ('e', 982719), ('t', 770665), ('o', 756069), ('a', 641948), ('i', 572556), ('n', 558029), ('s', 509686), ('h', 457329), ('r', 428444), ('l', 360810), ('u', 313756), ('d', 292019), ('y', 262229), ('m', 221332), ('.', 220916), ('w', 188720), ('g', 176637), ('c', 170854), ('f', 155649), ('b', 134281), ('p', 118800), ('k', 110853), ('I', 103104), ('v', 95533), (',', 84555), ('T', 47063), (\"'\", 45414), ('!', 40085), ('E', 33123), ('A', 32658), ('S', 30865), ('O', 29809), ('H', 26929), ('?', 26852), ('N', 24120), ('Y', 23070), ('D', 20951), ('L', 19131), ('M', 19070), ('W', 18745), ('j', 18628), ('R', 17674), ('B', 16991), ('C', 14101), ('G', 13595), ('x', 13413), ('U', 11484), ('P', 11439), ('F', 9829), ('-', 9403), ('\"', 9262), ('q', 7274), ('z', 7037), ('K', 6774), (')', 6382), ('1', 6195), ('J', 6089), ('0', 5422), ('V', 5253), ('2', 5204), (':', 4589), ('(', 4524), ('3', 2791), ('5', 2565), ('*', 2404), ('4', 2355), (';', 2335), ('9', 2076), ('/', 1909), ('8', 1758), ('6', 1652), ('&', 1452), ('7', 1268), ('X', 811), ('Z', 769), ('_', 727), ('@', 667), ('$', 629), ('Q', 610), ('=', 481), ('#', 361), ('>', 231), ('^', 199), ('%', 178), ('~', 167), ('`', 163), (']', 147), ('+', 134), ('[', 116), ('<', 100), ('’', 100), ('¨', 78), ('´', 68), ('}', 41), ('{', 28), ('“', 27), ('—', 27), ('…', 22), ('”', 21), ('é', 21), ('|', 20), ('¡', 20), ('£', 18), ('Ü', 13), ('–', 12), ('ı', 10), ('♥', 10), ('¿', 6), ('ñ', 5), ('·', 4), ('\\\\', 4), ('\\u200b', 4), ('ü', 3), ('¢', 3), ('ö', 3), ('§', 3), ('á', 3), ('è', 2), ('˝', 2), ('嘉', 2), ('義', 2), ('人', 2), ('因', 2), ('為', 2), ('綠', 2), ('豆', 2), ('加', 2), ('薏', 2), ('仁', 2), ('©', 2), ('™', 2), ('‘', 2), ('☺', 2), ('ŕ', 2), ('ā', 2), ('ə', 2), ('®', 2), ('š', 2), ('†', 2), ('Æ', 1), ('恭', 1), ('喜', 1), ('發', 1), ('財', 1), ('♡', 1), ('½', 1), ('í', 1), ('ƒ', 1), ('Ä', 1), ('ù', 1), ('س', 1), ('ا', 1), ('م', 1), ('ه', 1), ('º', 1), ('¹', 1), ('œ', 1), ('•', 1), ('ó', 1), ('►', 1), ('λ', 1), ('◄', 1), ('à', 1), ('»', 1), ('ĕ', 1), ('û', 1), ('ï', 1)])\n\n [' ', 'e', 't', 'o', 'a', 'i', 'n', 's', 'h', 'r', 'l', 'u', 'd', 'y', 'm', '.', 'w', 'g', 'c', 'f', 'b', 'p', 'k', 'I', 'v', ',', 'T', \"'\", '!', 'E', 'A', 'S', 'O', 'H', '?', 'N', 'Y', 'D', 'L', 'M', 'W', 'j', 'R', 'B', 'C', 'G', 'x', 'U', 'P', 'F', '-', '\"', 'q', 'z', 'K', ')', '1', 'J', '0', 'V', '2', ':', '(', '3', '5', '*', '4', ';', '9', '/', '8', '6', '&', '7', 'X', 'Z', '_', '@', '$', 'Q', '=', '#', '>', '^', '%', '~', '`', ']', '+', '[', '<', '’', '¨', '´', '}', '{', '“', '—', '…', '”', 'é', '|', '¡', '£', 'Ü', '–', 'ı', '♥', '¿', 'ñ', '·', '\\\\', '\\u200b', 'ü', '¢', 'ö', '§', 'á', 'è', '˝', '嘉', '義', '人', '因', '為', '綠', '豆', '加', '薏', '仁', '©', '™', '‘', '☺', 'ŕ', 'ā', 'ə', '®', 'š', '†', 'Æ', '恭', '喜', '發', '財', '♡', '½', 'í', 'ƒ', 'Ä', 'ù', 'س', 'ا', 'م', 'ه', 'º', '¹', 'œ', '•', 'ó', '►', 'λ', '◄', 'à', '»', 'ĕ', 'û', 'ï']\n"
     ]
    }
   ],
   "source": [
    "p = OrderedDict(Counter(list(' '.join(formality_t[0]) + ' '.join(formality_d[0]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "formality_t['weird']= formality_t[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '_', '@', 'Q', '=', '#', '>', '^', '%', '~', '`', ']', '+', '[', '<', '’', '¨', '´', '}', '{', '“', '—', '…', '”', 'é', '|', '¡', 'Ü', '–', 'ı', '♥', '¿', 'ñ', '·', '\\\\', '\\u200b', 'ü', '¢', 'ö', '§', 'á', 'è', '˝', '嘉', '義', '人', '因', '為', '綠', '豆', '加', '薏', '仁', '©', '™', '‘', '☺', 'ŕ', 'ā', 'ə', '®', 'š', '†', 'Æ', '恭', '喜', '發', '財', '♡', '½', 'í', 'ƒ', 'Ä', 'ù', 'س', 'ا', 'م', 'ه', 'º', '¹', 'œ', '•', 'ó', '►', 'λ', '◄', 'à', '»', 'ĕ', 'û', 'ï', '(',')',':','--', '....', '!!!', 'www', 'http']])) )\n",
    "formality_d['weird']= formality_d[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '_', '@', 'Q', '=', '#', '>', '^', '%', '~', '`', ']', '+', '[', '<', '’', '¨', '´', '}', '{', '“', '—', '…', '”', 'é', '|', '¡', 'Ü', '–', 'ı', '♥', '¿', 'ñ', '·', '\\\\', '\\u200b', 'ü', '¢', 'ö', '§', 'á', 'è', '˝', '嘉', '義', '人', '因', '為', '綠', '豆', '加', '薏', '仁', '©', '™', '‘', '☺', 'ŕ', 'ā', 'ə', '®', 'š', '†', 'Æ', '恭', '喜', '發', '財', '♡', '½', 'í', 'ƒ', 'Ä', 'ù', 'س', 'ا', 'م', 'ه', 'º', '¹', 'œ', '•', 'ó', '►', 'λ', '◄', 'à', '»', 'ĕ', 'û', 'ï', '(',')',':','--', '....', '!!!', 'www', 'http']])) )\n",
    "\n",
    "formality_t['tokens'] = formality_t[0].apply(lambda x: len(x.split(' ')))\n",
    "formality_d['tokens'] = formality_d[0].apply(lambda x: len(x.split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(169735, 4) (154938, 4)\n(42434, 4) (38709, 4)\n"
     ]
    }
   ],
   "source": [
    "print(formality_t.shape, formality_t[(~formality_t['weird']) & (formality_t['tokens']>=5) & (formality_t['tokens']<=30)].shape)\n",
    "print(formality_d.shape, formality_d[(~formality_d['weird']) & (formality_d['tokens']>=5) & (formality_d['tokens']<=30)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "formality_t = formality_t[(~formality_t['weird']) & (formality_t['tokens']>=5) & (formality_t['tokens']<=30)].filter([0,1])\n",
    "formality_d = formality_d[(~formality_d['weird']) & (formality_d['tokens']>=5) & (formality_d['tokens']<=30)].filter([0,1])\n",
    "\n",
    "formality_t = formality_t.sample(frac=1).reset_index(drop=True)\n",
    "formality_d = formality_d.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "formality_tt = formality_t[:100000]\n",
    "formality_dd = formality_d[:25000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0    47230\n",
       "1    52770\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "formality_tt.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/formality/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "formality_tt.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "formality_dd.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/formality_full/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "if not os.path.exists(data_output_full):\n",
    "    os.makedirs(data_output_full)\n",
    "formality_t.to_csv(f'{data_output_full}/train.csv', header=False, index=False)\n",
    "formality_d.to_csv(f'{data_output_full}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output_full}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arousal\n",
    "data_input = '../data/processed/arousal'\n",
    "data_output = '../data/processed_filtered/arousal'\n",
    "\n",
    "arousal_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "arousal_d = pd.read_csv(f'{data_input}/dev.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 144718), ('e', 79546), ('t', 57680), ('a', 53839), ('o', 52043), ('n', 46969), ('i', 46767), ('s', 43431), ('r', 40361), ('h', 31456), ('l', 28804), ('d', 24916), ('u', 19126), ('c', 18819), ('m', 15939), ('f', 14100), ('g', 13587), ('p', 13325), ('y', 11882), ('w', 11653), ('b', 9178), (',', 8470), ('.', 7980), ('v', 6690), ('k', 5292), ('I', 3115), ('T', 2694), ('A', 2191), ('S', 2020), ('-', 1960), (\"'\", 1796), ('C', 1693), ('\"', 1616), ('0', 1516), ('M', 1386), ('x', 1325), ('1', 1148), ('H', 1070), ('N', 1056), ('W', 1051), ('B', 1012), ('P', 959), ('’', 881), ('E', 854), ('R', 848), ('F', 831), ('j', 767), ('2', 757), ('D', 720), ('9', 696), ('L', 691), ('z', 672), ('q', 638), (':', 638), ('O', 637), ('”', 605), ('“', 601), ('G', 577), ('5', 551), ('Y', 491), ('K', 470), (')', 470), ('3', 469), ('(', 465), ('U', 455), ('J', 412), ('?', 380), ('8', 344), ('4', 340), ('V', 320), ('7', 315), ('6', 307), ('$', 301), (';', 276), ('—', 153), ('!', 140), ('/', 119), ('\\u2002', 116), ('–', 86), ('Z', 76), ('&', 71), ('%', 68), ('♭', 45), ('Q', 37), ('…', 34), ('\\xad', 30), ('♯', 27), ('_', 23), ('[', 22), (']', 22), ('X', 20), ('ó', 19), ('#', 16), ('‘', 14), ('·', 12), ('=', 11), ('+', 11), ('\\xa0', 10), ('|', 7), ('`', 7), ('é', 5), ('<', 4), ('>', 4), ('ç', 4), ('®', 4), ('*', 4), ('ñ', 3), ('ã', 3), ('@', 3), ('è', 2), ('õ', 2), ('♮', 1), ('ü', 1), ('í', 1)])\n\n [' ', 'e', 't', 'a', 'o', 'n', 'i', 's', 'r', 'h', 'l', 'd', 'u', 'c', 'm', 'f', 'g', 'p', 'y', 'w', 'b', ',', '.', 'v', 'k', 'I', 'T', 'A', 'S', '-', \"'\", 'C', '\"', '0', 'M', 'x', '1', 'H', 'N', 'W', 'B', 'P', '’', 'E', 'R', 'F', 'j', '2', 'D', '9', 'L', 'z', 'q', ':', 'O', '”', '“', 'G', '5', 'Y', 'K', ')', '3', '(', 'U', 'J', '?', '8', '4', 'V', '7', '6', '$', ';', '—', '!', '/', '\\u2002', '–', 'Z', '&', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í']\n"
     ]
    }
   ],
   "source": [
    "p = OrderedDict(Counter(list(' '.join(arousal_t[0]) + ' '.join(arousal_d[0]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "arousal_t['weird']= arousal_t[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '\\u2002', '–', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í', '(',')',':','--', '....', '!!!', 'www', 'http','_',':',\"”\",\"“\"]])) )\n",
    "arousal_d['weird']= arousal_d[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in ['\\u2002', '–', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í', '(',')',':','--', '....', '!!!', 'www', 'http','_',':',\"”\",\"“\"]])) )\n",
    "\n",
    "arousal_t['tokens'] = arousal_t[0].apply(lambda x: len(x.split(' ')))\n",
    "arousal_d['tokens'] = arousal_d[0].apply(lambda x: len(x.split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(6901, 4) (5178, 4)\n(1726, 4) (1320, 4)\n"
     ]
    }
   ],
   "source": [
    "print(arousal_t.shape, arousal_t[(~arousal_t['weird']) & (arousal_t['tokens']>=5) & (arousal_t['tokens']<=40)].shape)\n",
    "print(arousal_d.shape, arousal_d[(~arousal_d['weird']) & (arousal_d['tokens']>=5) & (arousal_d['tokens']<=40)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0    2422\n",
       "1    4479\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "arousal_t.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/arousal/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "arousal_t =  arousal_t[(~arousal_t['weird']) & (arousal_t['tokens']>=5) & (arousal_t['tokens']<=40)].filter([0,1])\n",
    "arousal_d = arousal_d[(~arousal_d['weird']) & (arousal_d['tokens']>=5) & (arousal_d['tokens']<=40)].filter([0,1])\n",
    "\n",
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "arousal_t.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "arousal_d.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Emo\n",
    "data_input = '../data/processed/emo'\n",
    "data_output = '../data/processed_filtered/emo'\n",
    "\n",
    "emo_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "emo_d = pd.read_csv(f'{data_input}/dev.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 550295), ('e', 250830), ('t', 180123), ('o', 165422), ('a', 160428), ('i', 151709), ('n', 131936), ('s', 114983), ('l', 102090), ('r', 100713), ('h', 97975), ('d', 73543), ('u', 64931), ('m', 60146), ('y', 58064), ('g', 51959), ('f', 49950), ('c', 44549), ('w', 43190), ('p', 34100), ('b', 31669), ('.', 31654), ('k', 27177), ('v', 22213), ('I', 14849), (',', 9914), (\"'\", 9734), ('!', 9656), ('T', 6468), ('j', 4802), ('@', 4690), ('x', 3847), ('S', 3578), ('?', 3516), ('W', 3406), ('H', 3190), ('A', 3087), ('O', 2970), ('M', 2844), ('Y', 2767), ('L', 2042), ('’', 1973), ('D', 1930), ('N', 1921), ('z', 1911), ('B', 1907), ('G', 1905), ('C', 1834), ('E', 1704), ('q', 1656), ('/', 1500), ('-', 1358), ('P', 1256), ('R', 1202), (';', 1180), ('F', 1049), ('J', 1021), ('&', 993), ('0', 976), ('1', 956), (':', 875), ('2', 865), ('K', 805), ('3', 648), ('U', 639), ('V', 596), ('4', 555), ('_', 555), ('5', 435), ('6', 399), (')', 384), ('8', 366), ('7', 344), ('(', 330), ('9', 321), ('*', 264), ('#', 226), ('Z', 128), ('X', 125), ('Q', 95), ('=', 95), ('ï', 90), ('¿', 90), ('½', 90), ('$', 85), ('~', 77), ('+', 50), (']', 44), ('%', 30), ('\"', 25), ('[', 20), ('^', 20), ('`', 13), ('”', 12), ('“', 10), ('‘', 9), ('|', 9), ('\\\\', 6), ('—', 5), ('–', 4), ('。', 3), ('Â', 1), ('¡', 1), ('′', 1), ('{', 1), ('}', 1)])\n\n [' ', 'e', 't', 'o', 'a', 'i', 'n', 's', 'l', 'r', 'h', 'd', 'u', 'm', 'y', 'g', 'f', 'c', 'w', 'p', 'b', '.', 'k', 'v', 'I', ',', \"'\", '!', 'T', 'j', '@', 'x', 'S', '?', 'W', 'H', 'A', 'O', 'M', 'Y', 'L', '’', 'D', 'N', 'z', 'B', 'G', 'C', 'E', 'q', '/', '-', 'P', 'R', ';', 'F', 'J', '&', '0', '1', ':', '2', 'K', '3', 'U', 'V', '4', '_', '5', '6', ')', '8', '7', '(', '9', '*', '#', 'Z', 'X', 'Q', '=', 'ï', '¿', '½', '$', '~', '+', ']', '%', '\"', '[', '^', '`', '”', '“', '‘', '|', '\\\\', '—', '–', '。', 'Â', '¡', '′', '{', '}']\n"
     ]
    }
   ],
   "source": [
    "p = OrderedDict(Counter(list(' '.join(emo_t[0]) + ' '.join(emo_d[0]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_t['weird']= emo_t[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '\\u2002', '–', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í', '(',')',':','--', '....', '!!!', 'www', 'http','_',':',\"”\",\"“\",'ï', '¿', '½', '$', '~', '+', ']', '%', '[', '^', '`', '”', '“', '|', '‘', '\\\\', '—', '–', '。', 'Â', '¡', '′', '{', '}','=', 'ï', '¿', '½', '~', '+', ']']])) )\n",
    "emo_d['weird']= emo_d[0].apply(lambda x: bool(sum([x.find(i)>=0 for i in [ '\\u2002', '–', '%', '♭', 'Q', '…', '\\xad', '♯', '_', '[', ']', 'X', 'ó', '#', '‘', '·', '=', '+', '\\xa0', '|', '`', 'é', '<', '>', 'ç', '®', '*', 'ñ', 'ã', '@', 'è', 'õ', '♮', 'ü', 'í', '(',')',':','--', '....', '!!!', 'www', 'http','_',':',\"”\",\"“\",'ï', '¿', '½', '$', '~', '+', ']', '%', '[', '^', '`', '”', '“', '|', '‘', '\\\\', '—', '–', '。', 'Â', '¡', '′', '{', '}','=', 'ï', '¿', '½', '~', '+', ']']])) )\n",
    "\n",
    "emo_t['tokens'] = emo_t[0].apply(lambda x: len(x.split(' ')))\n",
    "emo_d['tokens'] = emo_d[0].apply(lambda x: len(x.split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(27432, 4) (21866, 4)\n(6858, 4) (5533, 4)\n"
     ]
    }
   ],
   "source": [
    "print(emo_t.shape, emo_t[(~emo_t['weird']) & (emo_t['tokens']>=5) & (emo_t['tokens']<=40)].shape)\n",
    "print(emo_d.shape, emo_d[(~emo_d['weird']) & (emo_d['tokens']>=5) & (emo_d['tokens']<=40)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_t = emo_t[(~emo_t['weird']) & (emo_t['tokens']>=5) & (emo_t['tokens']<=40)].filter([0,1])\n",
    "emo_d =  emo_d[(~emo_d['weird']) & (emo_d['tokens']>=5) & (emo_d['tokens']<=40)].filter([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0     6733\n",
       "1    15133\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "emo_t.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/emo/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "emo_t.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "emo_d.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 1851612), ('e', 1120534), ('t', 805990), ('a', 750895), ('i', 701984), ('o', 699165), ('n', 672654), ('s', 585102), ('r', 571847), ('l', 384240), ('h', 359264), ('d', 325966), ('c', 306539), ('u', 261465), ('m', 249750), ('p', 220992), ('g', 191552), ('f', 191140), ('y', 156985), ('w', 132771), ('b', 116400), ('v', 105537), (',', 78585), ('k', 64870), ('.', 55108), ('I', 42135), ('-', 37229), ('T', 31419), ('x', 24423), ('A', 19706), ('N', 19291), ('S', 19198), ('D', 17868), ('M', 16675), ('W', 14817), ('E', 14684), ('L', 14513), ('C', 14183), (\"'\", 14040), ('R', 13380), ('O', 13050), (')', 12756), ('(', 12422), ('q', 11982), ('z', 11921), ('H', 11374), ('j', 9638), ('P', 9290), ('B', 8623), ('F', 8351), ('G', 7810), ('0', 6971), ('!', 6851), ('Y', 6648), ('?', 6600), ('1', 6359), ('U', 6141), ('2', 5297), ('\"', 4648), ('V', 3934), ('3', 3349), ('5', 2677), ('K', 2656), ('9', 2581), ('8', 2168), ('%', 2161), ('4', 2152), ('J', 2103), ('6', 2009), ('/', 1952), (':', 1845), ('$', 1819), ('7', 1624), (';', 1517), ('Q', 1331), ('\\\\', 1111), ('X', 682), ('{', 575), ('}', 564), ('*', 531), ('&', 513), ('Z', 376), ('’', 328), ('[', 284), (']', 284), ('=', 256), ('<', 229), ('+', 224), ('>', 222), ('_', 185), ('–', 156), ('”', 134), ('“', 128), ('^', 127), ('\\xa0', 104), ('\\u2009', 82), ('`', 81), ('~', 77), ('—', 73), ('×', 68), ('ı', 65), ('|', 44), ('@', 40), ('‘', 39), ('±', 39), ('#', 35), ('\\u202f', 35), ('\\u200b', 35), ('ş', 19), ('ü', 17), ('μ', 14), ('‐', 13), ('°', 13), ('Θ', 13), ('ε', 13), ('∗', 13), ('∼', 12), ('ö', 11), ('≈', 11), ('ï', 10), ('α', 10), ('©', 9), ('ç', 9), ('−', 8), ('Φ', 8), ('ã', 7), ('≥', 6), ('ğ', 6), ('δ', 6), ('θ', 6), ('η', 5), ('®', 5), ('…', 5), ('Å', 5), ('Ö', 4), ('→', 4), ('‡', 4), ('√', 4), ('\\x8c', 4), ('\\x80', 4), ('â', 4), ('é', 4), ('ä', 4), ('ŷ', 3), ('≤', 3), ('′', 3), ('σ', 3), ('•', 3), ('ˆ', 3), ('Š', 3), ('Ω', 3), ('₅', 3), ('₀', 3), ('β', 3), ('†', 3), ('ν', 3), ('Δ', 3), ('æ', 2), ('£', 2), ('à', 2), ('ω', 2), ('″', 2), ('²', 2), ('ê', 2), ('á', 2), ('⊤', 2), ('γ', 2), ('ú', 2), ('̆', 2), ('║', 2), ('ℜ', 2), ('Æ', 1), ('å', 1), ('˜', 1), ('ϵ', 1), ('§', 1), ('ξ', 1), ('ĭ', 1), ('\\u2005', 1), ('ă', 1), ('¾', 1), ('Ã', 1), ('‰', 1), ('ﬀ', 1), ('µ', 1), ('ź', 1), ('ℓ', 1), ('™', 1), ('\\x83', 1), ('\\x8a', 1), ('ó', 1), ('∞', 1), ('✩', 1), ('ø', 1), ('λ', 1), ('ë', 1), ('∈', 1), ('κ', 1), ('\\x93', 1), ('ﬁ', 1), ('ρ', 1), ('ﬃ', 1), ('Ă', 1), ('Ť', 1), ('↔', 1)])\n\n [' ', 'e', 't', 'a', 'i', 'o', 'n', 's', 'r', 'l', 'h', 'd', 'c', 'u', 'm', 'p', 'g', 'f', 'y', 'w', 'b', 'v', ',', 'k', '.', 'I', '-', 'T', 'x', 'A', 'N', 'S', 'D', 'M', 'W', 'E', 'L', 'C', \"'\", 'R', 'O', ')', '(', 'q', 'z', 'H', 'j', 'P', 'B', 'F', 'G', '0', '!', 'Y', '?', '1', 'U', '2', '\"', 'V', '3', '5', 'K', '9', '8', '%', '4', 'J', '6', '/', ':', '$', '7', ';', 'Q', '\\\\', 'X', '{', '}', '*', '&', 'Z', '’', '[', ']', '=', '<', '+', '>', '_', '–', '”', '“', '^', '\\xa0', '\\u2009', '`', '~', '—', '×', 'ı', '|', '@', '‘', '±', '#', '\\u202f', '\\u200b', 'ş', 'ü', 'μ', '‐', '°', 'Θ', 'ε', '∗', '∼', 'ö', '≈', 'ï', 'α', '©', 'ç', '−', 'Φ', 'ã', '≥', 'ğ', 'δ', 'θ', 'η', '®', '…', 'Å', 'Ö', '→', '‡', '√', '\\x8c', '\\x80', 'â', 'é', 'ä', 'ŷ', '≤', '′', 'σ', '•', 'ˆ', 'Š', 'Ω', '₅', '₀', 'β', '†', 'ν', 'Δ', 'æ', '£', 'à', 'ω', '″', '²', 'ê', 'á', '⊤', 'γ', 'ú', '̆', '║', 'ℜ', 'Æ', 'å', '˜', 'ϵ', '§', 'ξ', 'ĭ', '\\u2005', 'ă', '¾', 'Ã', '‰', 'ﬀ', 'µ', 'ź', 'ℓ', '™', '\\x83', '\\x8a', 'ó', '∞', '✩', 'ø', 'λ', 'ë', '∈', 'κ', '\\x93', 'ﬁ', 'ρ', 'ﬃ', 'Ă', 'Ť', '↔']\n"
     ]
    }
   ],
   "source": [
    "## Abstract\n",
    "data_input = '../data/processed/abstract'\n",
    "data_output = '../data/processed_filtered/abstract'\n",
    "\n",
    "abs_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "abs_d = pd.read_csv(f'{data_input}/dev.csv', header=None)\n",
    "\n",
    "p = OrderedDict(Counter(list(' '.join([str(x) for x in abs_t[0]]) + ' '.join([str(x) for x in abs_d[0]]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_t['weird']= abs_t[0].apply(lambda x: bool(sum([str(x).find(i)>=0 for i in [ '_', '–', '”', '“', '^', '\\xa0', '\\u2009', '`', '~', '—', '×', 'ı', '|', '@', '‘', '±', '#', '\\u202f', '\\u200b', 'ş', 'ü', 'μ', '‐', '°', 'Θ', 'ε', '∗', '∼', 'ö', '≈', 'ï', 'α', '©', 'ç', '−', 'Φ', 'ã', '≥', 'ğ', 'δ', 'θ', 'η', '®', '…', 'Å', 'Ö', '→', '‡', '√', '\\x8c', '\\x80', 'â', 'é', 'ä', 'ŷ', '≤', '′', 'σ', '•', 'ˆ', 'Š', 'Ω', '₅', '₀', 'β', '†', 'ν', 'Δ', 'æ', '£', 'à', 'ω', '″', '²', 'ê', 'á', '⊤', 'γ', 'ú', '̆', '║', 'ℜ', 'Æ', 'å', '˜', 'ϵ', '§', 'ξ', 'ĭ', '\\u2005', 'ă', '¾', 'Ã', '‰', 'ﬀ', 'µ', 'ź', 'ℓ', '™', '\\x83', '\\x8a', 'ó', '∞', '✩', 'ø', 'λ', 'ë', '∈', 'κ', '\\x93', 'ﬁ', 'ρ', 'ﬃ', 'Ă', 'Ť', '↔','http','www','--','!!']])) )\n",
    "abs_d['weird']= abs_d[0].apply(lambda x: bool(sum([str(x).find(i)>=0 for i in [ '_', '–', '”', '“', '^', '\\xa0', '\\u2009', '`', '~', '—', '×', 'ı', '|', '@', '‘', '±', '#', '\\u202f', '\\u200b', 'ş', 'ü', 'μ', '‐', '°', 'Θ', 'ε', '∗', '∼', 'ö', '≈', 'ï', 'α', '©', 'ç', '−', 'Φ', 'ã', '≥', 'ğ', 'δ', 'θ', 'η', '®', '…', 'Å', 'Ö', '→', '‡', '√', '\\x8c', '\\x80', 'â', 'é', 'ä', 'ŷ', '≤', '′', 'σ', '•', 'ˆ', 'Š', 'Ω', '₅', '₀', 'β', '†', 'ν', 'Δ', 'æ', '£', 'à', 'ω', '″', '²', 'ê', 'á', '⊤', 'γ', 'ú', '̆', '║', 'ℜ', 'Æ', 'å', '˜', 'ϵ', '§', 'ξ', 'ĭ', '\\u2005', 'ă', '¾', 'Ã', '‰', 'ﬀ', 'µ', 'ź', 'ℓ', '™', '\\x83', '\\x8a', 'ó', '∞', '✩', 'ø', 'λ', 'ë', '∈', 'κ', '\\x93', 'ﬁ', 'ρ', 'ﬃ', 'Ă', 'Ť', '↔','http','www','--','!!']])) )\n",
    "\n",
    "abs_t['tokens'] = abs_t[0].apply(lambda x: len(str(x).split(' ')))\n",
    "abs_d['tokens'] = abs_d[0].apply(lambda x: len(str(x).split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           weird\n",
       "tokens          \n",
       "1       1.319216\n",
       "2       0.372497\n",
       "3       0.517654\n",
       "4       0.331939\n",
       "5       1.369381\n",
       "6       6.242796\n",
       "7       6.521368\n",
       "8       6.394356\n",
       "9       5.974896\n",
       "10      5.632285\n",
       "11      5.307817\n",
       "12      4.976946\n",
       "13      4.689835\n",
       "14      4.445417\n",
       "15      4.155104\n",
       "16      3.916023\n",
       "17      3.687615\n",
       "18      3.448533\n",
       "19      3.227597\n",
       "20      2.566921\n",
       "21      2.373735\n",
       "22      2.175212\n",
       "23      2.030056\n",
       "24      1.975622\n",
       "25      1.787773\n",
       "26      1.604192\n",
       "27      1.562567\n",
       "28      1.341630\n",
       "29      1.225291\n",
       "30      1.113222\n",
       "31      0.926440\n",
       "32      0.875208\n",
       "33      0.791957\n",
       "34      0.667079\n",
       "35      0.605174\n",
       "36      0.519788\n",
       "37      0.436537\n",
       "38      0.369295\n",
       "39      0.343679\n",
       "40      0.273236"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>weird</th>\n    </tr>\n    <tr>\n      <th>tokens</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1.319216</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.372497</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.517654</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.331939</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.369381</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6.242796</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6.521368</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>6.394356</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>5.974896</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>5.632285</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>5.307817</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>4.976946</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>4.689835</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>4.445417</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>4.155104</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>3.916023</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>3.687615</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>3.448533</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>3.227597</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2.566921</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2.373735</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2.175212</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2.030056</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1.975622</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1.787773</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1.604192</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1.562567</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1.341630</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1.225291</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>1.113222</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.926440</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.875208</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.791957</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.667079</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.605174</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.519788</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.436537</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.369295</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.343679</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.273236</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "abs_t.groupby('tokens').agg({'weird':'count'}).head(40)*100/len(abs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(93692, 4) (81523, 4)\n(23424, 4) (20306, 4)\n"
     ]
    }
   ],
   "source": [
    "print(abs_t.shape, abs_t[(~abs_t['weird']) & (abs_t['tokens']>5) & (abs_t['tokens']<=30)].shape)\n",
    "print(abs_d.shape, abs_d[(~abs_d['weird']) & (abs_d['tokens']>5) & (abs_d['tokens']<=30)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0    46607\n",
       "1    34916\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "abs_t = abs_t[(~abs_t['weird']) & (abs_t['tokens']>5) & (abs_t['tokens']<=30)].filter([0,1])\n",
    "abs_d =  abs_d[(~abs_d['weird']) & (abs_d['tokens']>5) & (abs_d['tokens']<=30)].filter([0,1])\n",
    "abs_t.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/abstract/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "abs_t.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "abs_d.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([(' ', 565826), ('e', 270473), ('o', 192783), ('t', 187990), ('a', 162903), ('s', 140861), ('n', 139712), ('h', 138712), ('r', 130948), ('i', 130668), ('l', 99223), ('d', 87066), ('u', 78010), ('m', 65292), ('y', 63586), (',', 52309), ('w', 49863), ('f', 42731), ('c', 40971), ('.', 40937), ('g', 40660), ('b', 30265), ('p', 29289), ('v', 23606), ('I', 23084), ('k', 21827), ('’', 14066), ('T', 11577), ('?', 10178), ('W', 9258), ('A', 9070), ('!', 6388), ('H', 5643), ('B', 4871), ('S', 4568), ('C', 4323), (\"'\", 4262), ('O', 3952), ('M', 3947), ('Y', 3429), (';', 3160), ('N', 3064), ('G', 3028), ('—', 2815), ('L', 2790), ('D', 2627), ('F', 2430), ('P', 1937), ('j', 1919), ('x', 1639), ('-', 1509), ('q', 1476), ('E', 1334), ('R', 1226), ('z', 894), ('”', 773), (':', 713), ('K', 609), ('“', 585), ('J', 556), ('\\xa0', 497), ('U', 415), ('V', 373), ('è', 241), ('Q', 196), ('\\u2003', 183), (']', 168), ('[', 75), ('(', 48), (')', 48), ('\"', 36), ('‘', 32), ('…', 17), ('é', 15), ('–', 10), ('Z', 10), ('ç', 8), ('1', 6), ('5', 6), ('0', 5), ('à', 2), ('Æ', 2), ('&', 2), ('ï', 2), ('2', 2), ('æ', 1), ('9', 1), ('/', 1)])\n\n [' ', 'e', 'o', 't', 'a', 's', 'n', 'h', 'r', 'i', 'l', 'd', 'u', 'm', 'y', ',', 'w', 'f', 'c', '.', 'g', 'b', 'p', 'v', 'I', 'k', '’', 'T', '?', 'W', 'A', '!', 'H', 'B', 'S', 'C', \"'\", 'O', 'M', 'Y', ';', 'N', 'G', '—', 'L', 'D', 'F', 'P', 'j', 'x', '-', 'q', 'E', 'R', 'z', '”', ':', 'K', '“', 'J', '\\xa0', 'U', 'V', 'è', 'Q', '\\u2003', ']', '[', '(', ')', '\"', '‘', '…', 'é', '–', 'Z', 'ç', '1', '5', '0', 'à', 'Æ', '&', 'ï', '2', 'æ', '9', '/']\n"
     ]
    }
   ],
   "source": [
    "# Shakespeare\n",
    "## Abstract\n",
    "data_input = '../data/processed/shakespeare'\n",
    "data_output = '../data/processed_filtered/shakespeare'\n",
    "\n",
    "abs_t = pd.read_csv(f'{data_input}/train.csv', header=None)\n",
    "abs_d = pd.read_csv(f'{data_input}/dev.csv', header=None)\n",
    "\n",
    "p = OrderedDict(Counter(list(' '.join([str(x) for x in abs_t[0]]) + ' '.join([str(x) for x in abs_d[0]]))).most_common())\n",
    "print(p)\n",
    "print('\\n',[k for k in p])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_t['weird']= abs_t[0].apply(lambda x: bool(sum([str(x).find(i)>=0 for i in [ '-', '”', ':', '“', '\\xa0', 'è', '\\u2003', '…', 'é', '–', 'ç', 'à', 'Æ', 'ï','æ', '/', 'http','www','--','!!']])) )\n",
    "abs_d['weird']= abs_d[0].apply(lambda x: bool(sum([str(x).find(i)>=0 for i in ['-', '”', ':', '“', '\\xa0', 'è', '\\u2003', '…', 'é', '–', 'ç', 'à', 'Æ', 'ï','æ', '/', 'http','www','--','!!']])) )\n",
    "\n",
    "abs_t['tokens'] = abs_t[0].apply(lambda x: len(str(x).split(' ')))\n",
    "abs_d['tokens'] = abs_d[0].apply(lambda x: len(str(x).split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            weird\n",
       "tokens           \n",
       "1        2.636005\n",
       "2        5.061750\n",
       "3        8.388296\n",
       "4       12.443008\n",
       "5        9.154088\n",
       "6        8.051879\n",
       "7        6.161746\n",
       "8        6.706210\n",
       "9        5.924926\n",
       "10       4.273826\n",
       "11       3.463769\n",
       "12       2.908238\n",
       "13       2.671418\n",
       "14       2.598380\n",
       "15       2.441238\n",
       "16       2.169005\n",
       "17       1.859147\n",
       "18       1.702005\n",
       "19       1.350095\n",
       "20       1.157541\n",
       "21       1.031384\n",
       "22       0.900801\n",
       "23       0.834403\n",
       "24       0.663981\n",
       "25       0.619716\n",
       "26       0.520119\n",
       "27       0.482493\n",
       "28       0.369616\n",
       "29       0.343057\n",
       "30       0.349697\n",
       "31       0.289938\n",
       "32       0.241247\n",
       "33       0.243460\n",
       "34       0.177062\n",
       "35       0.177062\n",
       "36       0.185915\n",
       "37       0.121730\n",
       "38       0.112877\n",
       "39       0.106237\n",
       "40       0.110664"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>weird</th>\n    </tr>\n    <tr>\n      <th>tokens</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2.636005</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.061750</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8.388296</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12.443008</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>9.154088</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8.051879</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6.161746</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>6.706210</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>5.924926</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4.273826</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3.463769</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2.908238</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2.671418</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2.598380</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2.441238</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2.169005</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1.859147</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1.702005</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1.350095</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1.157541</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1.031384</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.900801</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.834403</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.663981</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.619716</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.520119</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.482493</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.369616</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.343057</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.349697</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.289938</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.241247</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.243460</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.177062</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.177062</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.185915</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.121730</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.112877</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.106237</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.110664</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "abs_t.groupby('tokens').agg({'weird':'count'}).head(40)*100/len(abs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(45182, 4) (34302, 4)\n(11296, 4) (7759, 4)\n"
     ]
    }
   ],
   "source": [
    "print(abs_t.shape, abs_t[(~abs_t['weird']) & (abs_t['tokens']>=4) & (abs_t['tokens']<=25)].shape)\n",
    "print(abs_d.shape, abs_d[(~abs_d['weird']) & (abs_d['tokens']>=4) & (abs_d['tokens']<=25)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1\n",
       "0    17345\n",
       "1    16957\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "abs_t = abs_t[(~abs_t['weird']) & (abs_t['tokens']>=4) & (abs_t['tokens']<=25)].filter([0,1])\n",
    "abs_d =  abs_d[(~abs_d['weird']) & (abs_d['tokens']>=4) & (abs_d['tokens']<=25)].filter([0,1])\n",
    "abs_t.groupby(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../data/processed_filtered/shakespeare/config.json'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "if not os.path.exists(data_output):\n",
    "    os.makedirs(data_output)\n",
    "abs_t.to_csv(f'{data_output}/train.csv', header=False, index=False)\n",
    "abs_d.to_csv(f'{data_output}/dev.csv', header=False, index=False)\n",
    "shutil.copy(f'{data_input}/config.json', f'{data_output}/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd098037a696054ba6333485ba1eda7d4b13de5ba8596b9581751e7239af6bf3f61",
   "display_name": "Python 3.8.8 64-bit ('marvin': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}