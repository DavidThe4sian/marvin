{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextGenerationPipeline, AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JointSeqClassifier(transformers.DistilBertForSequenceClassification):\n",
    "    '''\n",
    "    A class that inherits from DistilBertForSequenceClassification, but extends the model to \n",
    "    have multiple classifiers at the end to perform joint classification over multple tasks.\n",
    "    '''\n",
    "    def __init__(self, config, num_tasks=1):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self. num_tasks = num_tasks\n",
    "\n",
    "        self.distilbert = transformers.DistilBertModel(config)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        # List of classifiers\n",
    "        self.classifier = nn.ModuleList([nn.Linear(config.dim, config.num_labels) \\\n",
    "                           for i in range(num_tasks)])\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        task_ids, \n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \n",
    "        task_ids (list of ints):\n",
    "            Labels indexing which classification task the labels correspond to.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        \n",
    "        logits_list = []\n",
    "        loss = 0\n",
    "        for i in task_ids:\n",
    "            logits = self.classifier[i](pooled_output)  # (bs, num_labels)\n",
    "            logits_list.append(logits)\n",
    "            if labels != None:\n",
    "                if self.num_labels == 1:\n",
    "                    loss_fct = nn.MSELoss()\n",
    "                    loss += loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = nn.CrossEntropyLoss()\n",
    "                    loss += loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits_list,) + distilbert_output[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return transformers.modeling_outputs.SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits_list,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing JointSeqClassifier: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing JointSeqClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointSeqClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of JointSeqClassifier were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['classifier.0.weight', 'classifier.0.bias', 'classifier.1.weight', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JointSeqClassifier(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): ModuleList(\n",
       "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (1): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING JOINT MODEL\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "                                          model_max_length=64)\n",
    "\n",
    "model = JointSeqClassifier.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                                            num_tasks=2)\n",
    "\n",
    "# Try only updating final layers\n",
    "params_to_update = [[] for i in range(model.num_tasks)]\n",
    "\n",
    "for i in range(model.num_tasks):\n",
    "    for name, param in model.named_parameters():\n",
    "        if f\"classifier.{i}.\" in name or name == 'pre_classifier':\n",
    "            params_to_update[i].append(param)\n",
    "\n",
    "optims = [AdamW(params_to_update[i], lr=5e-5) for i in range(model.num_tasks)]\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", model_max_length=64)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# # Try only updating final layers\n",
    "# params_to_update = []\n",
    "\n",
    "# for name,param in model.named_parameters():\n",
    "#     if 'classifier' in name:\n",
    "#         params_to_update.append(param)\n",
    "        \n",
    "# optim = AdamW(params_to_update, lr=5e-5)\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single(input_tensor_batch, target_tensor_batch, \n",
    "                 model, model_optimizer, task_ids):\n",
    "    '''\n",
    "    A single forward and backward pass of the neural net on a single training batch.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        - input_tensor_batch  : list of tensors. \n",
    "                                A batch of encoded sentence inputs and is\n",
    "                                of size (batch_size, max_length).\n",
    "        - target_tensors_batch : list of tensors\n",
    "                                 Each tensor represents a batch of class labels.\n",
    "                                 Each tensor is of size (batch_size, 1).\n",
    "        - model                : PyTorch sequence classifier model.  \n",
    "                                 Assumed to be a JointSeqClassifier or \n",
    "                                 DistilBertForSequenceClassification model\n",
    "        - model_optimizer      : PyTorch Optimizer.\n",
    "                                 The optimizer used by the model for training.\n",
    "        - task_ids             : list of ints.\n",
    "                                 List of the indices for the tasks on which we're training. \n",
    "                                 Although it is a list, currently it only works if the list \n",
    "                                 is a single element (for the JointSeqClassifier model). TODO:\n",
    "                                 fix this. \n",
    "                           \n",
    "    Returns:\n",
    "    \n",
    "        - loss : float.\n",
    "                 The loss of this training run.\n",
    "        \n",
    "    '''\n",
    "    model_optimizer.zero_grad()\n",
    "    output = model(input_ids=input_tensor_batch.to(device),  \n",
    "                   task_ids=task_ids, \n",
    "                   labels = target_tensor_batch.to(device),\n",
    "                   output_attentions=False)\n",
    "    loss = output[0]\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train(input_tensors, target_tensors, input_val_tensors, target_val_tensors,\n",
    "          model, model_optimizer, n_epochs, task_ids):\n",
    "    '''\n",
    "    Train the classfier for a given number of epochs on the whole training set.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        - input_tensors   : list of tensors. \n",
    "                            Each entry in the list is a single batch of encoded \n",
    "                            sentence inputs and is of size (batch_size, max_length).\n",
    "        - target_tensors  : list of tensors\n",
    "                            Each tensor represents a batch of class labels.\n",
    "                            Each tensor is of size (batch_size, 1).\n",
    "        - model           : PyTorch sequence classifier model.  \n",
    "                            Assumed to be a JointSeqClassifier or \n",
    "                            DistilBertForSequenceClassification model\n",
    "        - model_optimizer : PyTorch Optimizer.\n",
    "                            The optimizer used by the model for training.\n",
    "        - n_epochs        : int.\n",
    "                            The number of epochs to train for. \n",
    "                            Each epoch is an entire pass over the data. \n",
    "        - task_ids        : list of ints.\n",
    "                            List of the indices for the tasks on which we're training. \n",
    "                            Although it is a list, currently it only works if the list \n",
    "                            is a single element (for the JointSeqClassifier model). TODO:\n",
    "                            fix this. \n",
    "                           \n",
    "    Returns:\n",
    "    \n",
    "        - loss : float.\n",
    "                 The loss of this training run.\n",
    "    '''\n",
    "    losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    # Iterate over given num of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = 0\n",
    "        # Iterate over batches\n",
    "        for i in tqdm.tqdm(range(len(input_tensors)), desc=\"batches progress\"):\n",
    "            input_tensor = input_tensors[i]\n",
    "            target_tensor = target_tensors[i]\n",
    "            loss += train_single(input_tensor, target_tensor, model, \n",
    "                                 model_optimizer, task_ids)\n",
    "        print(f\"Epoch {epoch} :\") \n",
    "        print(f\"\\tLoss {loss/len(input_tensors):.4f}\")\n",
    "        train_accuracy = get_accuracy(input_tensors, target_tensors, model)\n",
    "        val_accuracy = get_accuracy(input_val_tensors, target_val_tensors, model)\n",
    "        print(f\"\\tTraining Accuracy {train_accuracy:.4f}\")\n",
    "        print(f\"\\tValidation Accuracy {val_accuracy:.4f}\")\n",
    "        losses.append(loss/len(input_tensors))\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "    return losses, train_accs, val_accs\n",
    "\n",
    "def get_accuracy(input_tensors, target_tensors, model, task_num=0):\n",
    "    '''\n",
    "    Get model accuracy for the corresponding task. \n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        - input_tensors  : list of tensors. \n",
    "                           Each tensor represents a batch of encoded sentence inputs and is\n",
    "                           of size (batch_size, max_length).\n",
    "        - target_tensors : list of tensors\n",
    "                           Each tensor represents a batch of class labels.\n",
    "                           Each tensor is of size (batch_size, 1).\n",
    "        - model          : PyTorch sequence classifier model.  \n",
    "                           Assumed to be a JointSeqClassifier or \n",
    "                           DistilBertForSequenceClassification model\n",
    "        - task_num       : int.\n",
    "                           represents the index of the specific task we are evaluating.\n",
    "                           \n",
    "    Returns:\n",
    "    \n",
    "        - accuracy : float.\n",
    "                     Classification accuracy of the model on this data and task.\n",
    "    '''\n",
    "    accs = []\n",
    "    # Iterate over batches\n",
    "    for i in range(len(input_tensors)):\n",
    "        input_tensor_batch = input_tensors[i].to(device)\n",
    "        target_tensor_batch = target_tensors[i].to(device)\n",
    "        # Wrangle tensors into needed shapes\n",
    "        #target_tensor = torch.stack(target_tensor_batch).reshape(len(input_tensor_batch)).to(device)\n",
    "        #input_tensor = torch.stack(input_tensor_batch)\n",
    "        #input_tensor = input_tensor.reshape(len(input_tensor_batch), \n",
    "        #                                    input_tensor.shape[2]).to(device)\n",
    "        # Run the model\n",
    "        output = model(input_tensor_batch, \n",
    "                       output_attentions=False, \n",
    "                       task_ids=[task_num])\n",
    "        # Get classification prediction for the task of interest\n",
    "        preds = output.logits[task_num].argmax(axis=1)\n",
    "        # Get accuracy of given batch\n",
    "        batch_acc = ((preds == target_tensor_batch).sum() \\\n",
    "                     / target_tensor_batch.shape[0]).item()\n",
    "        # Add accuracy to list of accuracies\n",
    "        accs.append(batch_acc)\n",
    "    # Return the total accuracy, averaged over the batches\n",
    "    return np.mean(accs)\n",
    "\n",
    "def sent_pred(sent, model, tokenizer, device, batch_size):\n",
    "    '''\n",
    "    Runs the model on an input sentence.\n",
    "    \n",
    "    Arguments: \n",
    "    \n",
    "      sent  : str. \n",
    "              The input sentence.\n",
    "      model : the PyTorch sequence classifier model.  \n",
    "              Assumed to be a JointSeqClassifier or \n",
    "              DistilBertForSequenceClassification model\n",
    "    Returns:\n",
    "    \n",
    "      pred  : np array. \n",
    "              The prediction, wich is a normalized array with a value for \\\n",
    "              each class, representing the predicted probability for that class\n",
    "      attns : tuple of tensors\n",
    "              each entry in tuple is the attention matrix for an attention head.\n",
    "    '''\n",
    "    input_tensor = tokenizer.encode(sent, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model(input_tensor, output_attentions=True, task_ids=len(model.classifier))\n",
    "    \n",
    "    preds = []\n",
    "    scores = []\n",
    "    # Iterate over tasks and get class predition and scorews for each.\n",
    "    for i in range(model.num_tasks):\n",
    "        pred = output.logits[i].argmax(axis=1)\n",
    "    \n",
    "        softmax = torch.nn.Softmax(dim=1)\n",
    "        score = softmax(output.logits[i].detach())\n",
    "        \n",
    "        preds.append(pred.detach().cpu().numpy())\n",
    "        scores.append(score)\n",
    "    \n",
    "    attns = output.attentions\n",
    "    \n",
    "    return preds, scores, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stanford Politeness:\n",
    "train_polite_file = \"../../../cross_style_transfer_internal/data/xslue/StanfordPoliteness/train.tsv\"\n",
    "dev_polite_file = \"../../../cross_style_transfer_internal/data/xslue/StanfordPoliteness/dev.tsv\"\n",
    "train_polite_data = pd.read_csv(train_polite_file, names=['domain', 'id', 'text', 'score'], sep='\\t')\n",
    "val_polite_data = pd.read_csv(dev_polite_file, names=['domain', 'id', 'text', 'score'], sep='\\t')\n",
    "\n",
    "\n",
    "# Short Humor\n",
    "train_humor_file = \"../../../cross_style_transfer_internal/data/xslue/ShortHumor/train.tsv\"\n",
    "dev_humor_file = \"../../../cross_style_transfer_internal/data/xslue/ShortHumor/dev.tsv\"\n",
    "train_humor_data = pd.read_csv(train_humor_file, names=['domain', 'score', 'text'], sep='\\t', error_bad_lines=False)\n",
    "val_humor_data = pd.read_csv(dev_humor_file, names=['domain', 'score', 'text'], sep='\\t', \n",
    "                       quoting=3, error_bad_lines=False)\n",
    "\n",
    "# Dictionary for the classes for each task\n",
    "task_names = [\"Politeness\", \"Humor\"]\n",
    "class_labels_dict = [{0: \"impolite\", 1 : \"polite\"}, \n",
    "                     {0: \"humorous\", 1 : \"not humorous\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_humor(humor_df):\n",
    "    '''\n",
    "    Parse short humor dataframe into the format we need for classification.\n",
    "    \n",
    "    Returns a DataFrame with two columns, one for the input text and one for the class labels.\n",
    "    '''\n",
    "    input_df = pd.DataFrame()\n",
    "    input_df['text'] = humor_df['text']\n",
    "    input_df['label'] = humor_df['score']\n",
    "    return input_df\n",
    "\n",
    "def parse_stanford_politeness(polite_df):\n",
    "    '''\n",
    "    Parse stanford politeness dataframe into the format we need for classification.\n",
    "    \n",
    "    Returns a DataFrame with two columns, one for the input text and one for the class labels.\n",
    "    '''\n",
    "    input_df = pd.DataFrame()\n",
    "    input_df['text'] = polite_df['text']\n",
    "    # Map scores >= 0 (polite) to label 1 and scores < 0 (impolite) to label 0.\n",
    "    input_df['label'] = polite_df['score'].apply(lambda x : int(x >= 0))\n",
    "    return input_df\n",
    "\n",
    "def df_to_training_pairs(df, tokenizer, batch_size):\n",
    "    '''\n",
    "    Convert DataFrames with a 'text' and 'label' column into two lists of tensors, \n",
    "    one with texts encoded by the tokenizer and one with the class labels.\n",
    "    '''\n",
    "    input_tensors = df['text'].apply(lambda x : tokenizer.encode(x, \n",
    "                                                                 padding='max_length', \n",
    "                                                                 truncation=True, \n",
    "                                                                 return_tensors=\"pt\"))\n",
    "    target_tensors = df['label'].apply(lambda x : torch.LongTensor([x]))\n",
    "    return input_tensors.values.reshape(-1, batch_size).tolist(), target_tensors.values.reshape(-1, batch_size).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DataFrames for tasks\n",
    "train_polite_df = parse_stanford_politeness(train_polite_data)\n",
    "val_polite_df = parse_stanford_politeness(val_polite_data)\n",
    "\n",
    "train_humor_df = parse_humor(train_humor_data)\n",
    "val_humor_df = parse_humor(val_humor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "input_polite_tensors, target_polite_tensors = df_to_training_pairs(train_polite_df.head((len(train_polite_df)//batch_size)*batch_size), \n",
    "                                                                   tokenizer, batch_size)\n",
    "input_polite_val_tensors, target_polite_val_tensors = df_to_training_pairs(val_polite_df.head((len(val_polite_df)//batch_size)*batch_size), \n",
    "                                                                  tokenizer, batch_size)\n",
    "\n",
    "input_humor_tensors, target_humor_tensors = df_to_training_pairs(train_humor_df.head((len(train_humor_df)//batch_size)*batch_size), \n",
    "                                                                   tokenizer, batch_size)\n",
    "input_humor_val_tensors, target_humor_val_tensors = df_to_training_pairs(val_humor_df.head((len(val_humor_df)//batch_size)*batch_size), \n",
    "                                                                  tokenizer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(target_polite_tensors)):\n",
    "    input_polite_tensors[i] = torch.stack(input_polite_tensors[i])\n",
    "    input_polite_tensors[i] = input_polite_tensors[i].reshape(len(input_polite_tensors[i]), \n",
    "                                       input_polite_tensors[i].shape[2])\n",
    "    target_polite_tensors[i] = torch.LongTensor(target_polite_tensors[i])\n",
    "for i in range(len(target_polite_val_tensors)):\n",
    "    input_polite_val_tensors[i] = torch.stack(input_polite_val_tensors[i])\n",
    "    input_polite_val_tensors[i] = input_polite_val_tensors[i].reshape(len(input_polite_val_tensors[i]), \n",
    "                                       input_polite_val_tensors[i].shape[2])\n",
    "    target_polite_val_tensors[i] = torch.LongTensor(target_polite_val_tensors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(target_humor_tensors)):\n",
    "    input_humor_tensors[i] = torch.stack(input_humor_tensors[i])\n",
    "    input_humor_tensors[i] = input_humor_tensors[i].reshape(len(input_humor_tensors[i]), \n",
    "                                       input_humor_tensors[i].shape[2])\n",
    "    target_humor_tensors[i] = torch.LongTensor(target_humor_tensors[i])\n",
    "for i in range(len(target_humor_val_tensors)):\n",
    "    input_humor_val_tensors[i] = torch.stack(input_humor_val_tensors[i])\n",
    "    input_humor_val_tensors[i] = input_humor_val_tensors[i].reshape(len(input_humor_val_tensors[i]), \n",
    "                                       input_humor_val_tensors[i].shape[2])\n",
    "    target_humor_val_tensors[i] = torch.LongTensor(target_humor_val_tensors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_single(input_tensors[0], target_tensor_joint_test, model, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches progress: 100%|██████████| 308/308 [01:00<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "\tLoss 0.6782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches progress:   0%|          | 1/308 [00:00<00:58,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Accuracy 0.5494\n",
      "\tValidation Accuracy 0.5586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches progress: 100%|██████████| 308/308 [01:02<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :\n",
      "\tLoss 0.6759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches progress:   0%|          | 1/308 [00:00<00:59,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Accuracy 0.5503\n",
      "\tValidation Accuracy 0.5547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches progress: 100%|██████████| 308/308 [01:03<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 :\n",
      "\tLoss 0.6737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "batches progress:   0%|          | 0/1181 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Accuracy 0.5523\n",
      "\tValidation Accuracy 0.5605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches progress:   4%|▎         | 43/1181 [00:08<03:55,  4.84it/s]"
     ]
    }
   ],
   "source": [
    "train_epochs = 3\n",
    "\n",
    "train(input_polite_tensors, target_polite_tensors, \n",
    "      input_polite_val_tensors, target_polite_val_tensors, \n",
    "      model, optims[0], train_epochs, task_ids=[0])\n",
    "\n",
    "train(input_humor_tensors, target_humor_tensors, \n",
    "      input_humor_val_tensors, target_humor_val_tensors, \n",
    "      model, optims[1], train_epochs, task_ids=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Could you please help me?\"\n",
    "\n",
    "preds, scores, attns = sent_pred(sent, model, tokenizer, device, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum over attention vectors for each head and handle dimensions and move to cpu\n",
    "viz_attns = np.array([attn.sum(axis=1).cpu().detach().squeeze().numpy() for attn in attns])\n",
    "# Sum over heads\n",
    "viz_attns = viz_attns.sum(axis=0)\n",
    "# Drop cls and sep tokens\n",
    "viz_attns = viz_attns[0, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(sent))[1:-1]\n",
    "scores = [score.cpu().detach().squeeze().numpy() for score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create heatmap\n",
    "print(f\"Tokenized Input: {tokens}\\n\")\n",
    "for i, task_name in enumerate(task_names):\n",
    "    print(f\"Task: {task_name}\")\n",
    "    print(f\"Style: {class_labels_dict[i][preds[i][0]]}\")\n",
    "    print(f\"Class Scores: {class_labels_dict[i][1]} : {scores[i][1]*100:.2f}%, {class_labels_dict[i][0]} : {scores[i][0]*100:.2f}%\\n\")\n",
    "fig, ax = plt.subplots(figsize=(10, 2))\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.set_xlabel(\"Token Position\")\n",
    "plt.imshow([viz_attns], cmap='Reds');\n",
    "plt.savefig(f'Attention_humor_and_politeness_heatmap_{\"_\".join(sent.split())}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = viz_attns\n",
    "x = np.arange(len(tokens))\n",
    "labels = tokens\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x, y, 'k:')\n",
    "ax.set_ylabel(\"Attention\")\n",
    "ax.set_xlabel(\"Token Position\")\n",
    "\n",
    "for i, txt in enumerate(labels):\n",
    "    ax.annotate(txt, (x[i], y[i]), xytext=(x[i] + 0.03, y[i] + 0.03))\n",
    "plt.savefig(f'Attention_humor_and_politeness_plot_{\"_\".join(sent.split())}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True\n",
    "checkpoint_path = \"../models/BERT_Joint_EarlyTest\"\n",
    "if save_model:\n",
    "    model.save_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_marvin)",
   "language": "python",
   "name": "env_marvin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
