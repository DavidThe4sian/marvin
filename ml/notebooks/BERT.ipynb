{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextGenerationPipeline, AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JointSeqClassifier(transformers.DistilBertForSequenceClassification):\n",
    "    '''\n",
    "    A class that inherits from DistilBertForSequenceClassification, but extends the model to \n",
    "    have multiple classifiers at the end to perform joint classification over multple tasks.\n",
    "    '''\n",
    "    def __init__(self, config, num_tasks=1):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self. num_tasks = num_tasks\n",
    "\n",
    "        self.distilbert = transformers.DistilBertModel(config)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        # List of classifiers\n",
    "        self.classifier = nn.ModuleList([nn.Linear(config.dim, config.num_labels) \\\n",
    "                           for i in range(num_tasks)])\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        task_ids, \n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \n",
    "        task_ids (list of ints):\n",
    "            Labels indexing which classification task the labels correspond to.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        \n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "                \n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        \n",
    "        logits_list = []\n",
    "        loss = 0\n",
    "        for i in task_ids:\n",
    "            logits = self.classifier[i](pooled_output)  # (bs, num_labels)\n",
    "            logits_list.append(logits)\n",
    "            if labels != None:\n",
    "                if self.num_labels == 1:\n",
    "                    loss_fct = nn.MSELoss()\n",
    "                    loss += loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = nn.CrossEntropyLoss()\n",
    "                    loss += loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits_list,) + distilbert_output[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        \n",
    "        if output_hidden_states:\n",
    "            for h_state in distilbert_output.hidden_states:\n",
    "                h_state.retain_grad()\n",
    "                \n",
    "        return transformers.modeling_outputs.SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits_list,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing JointSeqClassifier: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing JointSeqClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointSeqClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of JointSeqClassifier were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['classifier.0.weight', 'classifier.0.bias', 'classifier.1.weight', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JointSeqClassifier(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): ModuleList(\n",
       "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (1): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING JOINT MODEL\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "                                          model_max_length=64)\n",
    "\n",
    "model = JointSeqClassifier.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                                            num_tasks=2)\n",
    "\n",
    "# Try only updating final layers\n",
    "params_to_update = [[] for i in range(model.num_tasks)]\n",
    "\n",
    "for i in range(model.num_tasks):\n",
    "    for name, param in model.named_parameters():\n",
    "        if f\"classifier.{i}.\" in name:\n",
    "            params_to_update[i].append(param)\n",
    "\n",
    "optims = [AdamW(params_to_update[i], lr=5e-5) for i in range(model.num_tasks)]\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", model_max_length=64)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# # Try only updating final layers\n",
    "# params_to_update = []\n",
    "\n",
    "# for name,param in model.named_parameters():\n",
    "#     if 'classifier' in name:\n",
    "#         params_to_update.append(param)\n",
    "        \n",
    "# optim = AdamW(params_to_update, lr=5e-5)\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single(input_tensor_batch, target_tensor_batch, \n",
    "                 model, model_optimizer, task_ids):\n",
    "    '''\n",
    "    A single forward and backward pass of the neural net on a single training batch.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        - input_tensor_batch  : list of tensors. \n",
    "                                A batch of encoded sentence inputs and is\n",
    "                                of size (batch_size, max_length).\n",
    "        - target_tensors_batch : list of tensors\n",
    "                                 Each tensor represents a batch of class labels.\n",
    "                                 Each tensor is of size (batch_size, 1).\n",
    "        - model                : PyTorch sequence classifier model.  \n",
    "                                 Assumed to be a JointSeqClassifier or \n",
    "                                 DistilBertForSequenceClassification model\n",
    "        - model_optimizer      : PyTorch Optimizer.\n",
    "                                 The optimizer used by the model for training.\n",
    "        - task_ids             : list of ints.\n",
    "                                 List of the indices for the tasks on which we're training. \n",
    "                                 Although it is a list, currently it only works if the list \n",
    "                                 is a single element (for the JointSeqClassifier model). TODO:\n",
    "                                 fix this. \n",
    "                           \n",
    "    Returns:\n",
    "    \n",
    "        - loss : float.\n",
    "                 The loss of this training run.\n",
    "        \n",
    "    '''\n",
    "    model_optimizer.zero_grad()\n",
    "    output = model(input_ids=input_tensor_batch.to(device),  \n",
    "                   task_ids=task_ids, \n",
    "                   labels = target_tensor_batch.to(device),\n",
    "                   output_attentions=False)\n",
    "    loss = output[0]\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train(input_tensors, target_tensors, input_val_tensors, target_val_tensors,\n",
    "          model, model_optimizer, n_epochs, task_ids):\n",
    "    '''\n",
    "    Train the classfier for a given number of epochs on the whole training set.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        - input_tensors   : list of tensors. \n",
    "                            Each entry in the list is a single batch of encoded \n",
    "                            sentence inputs and is of size (batch_size, max_length).\n",
    "        - target_tensors  : list of tensors\n",
    "                            Each tensor represents a batch of class labels.\n",
    "                            Each tensor is of size (batch_size, 1).\n",
    "        - model           : PyTorch sequence classifier model.  \n",
    "                            Assumed to be a JointSeqClassifier or \n",
    "                            DistilBertForSequenceClassification model\n",
    "        - model_optimizer : PyTorch Optimizer.\n",
    "                            The optimizer used by the model for training.\n",
    "        - n_epochs        : int.\n",
    "                            The number of epochs to train for. \n",
    "                            Each epoch is an entire pass over the data. \n",
    "        - task_ids        : list of ints.\n",
    "                            List of the indices for the tasks on which we're training. \n",
    "                            Although it is a list, currently it only works if the list \n",
    "                            is a single element (for the JointSeqClassifier model). TODO:\n",
    "                            fix this. \n",
    "                           \n",
    "    Returns:\n",
    "    \n",
    "        - loss : float.\n",
    "                 The loss of this training run.\n",
    "    '''\n",
    "    model.train()\n",
    "    losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    # Iterate over given num of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = 0\n",
    "        # Iterate over batches\n",
    "        for i in tqdm.tqdm(range(len(input_tensors)), desc=\"training batches progress\"):\n",
    "            input_tensor = input_tensors[i]\n",
    "            target_tensor = target_tensors[i]\n",
    "            loss += train_single(input_tensor, target_tensor, model, \n",
    "                                 model_optimizer, task_ids)\n",
    "        print(f\"Epoch {epoch} :\") \n",
    "        print(f\"\\tLoss {loss/len(input_tensors):.4f}\")\n",
    "        train_accuracy = get_accuracy(input_tensors, target_tensors, model)\n",
    "        val_accuracy = get_accuracy(input_val_tensors, target_val_tensors, model)\n",
    "        print(f\"\\tTraining Accuracy {train_accuracy:.4f}\")\n",
    "        print(f\"\\tValidation Accuracy {val_accuracy:.4f}\")\n",
    "        losses.append(loss/len(input_tensors))\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "    return losses, train_accs, val_accs\n",
    "\n",
    "def get_accuracy(input_tensors, target_tensors, model, task_num=0):\n",
    "    '''\n",
    "    Get model accuracy for the corresponding task. \n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        - input_tensors  : list of tensors. \n",
    "                           Each tensor represents a batch of encoded sentence inputs and is\n",
    "                           of size (batch_size, max_length).\n",
    "        - target_tensors : list of tensors\n",
    "                           Each tensor represents a batch of class labels.\n",
    "                           Each tensor is of size (batch_size, 1).\n",
    "        - model          : PyTorch sequence classifier model.  \n",
    "                           Assumed to be a JointSeqClassifier or \n",
    "                           DistilBertForSequenceClassification model\n",
    "        - task_num       : int.\n",
    "                           represents the index of the specific task we are evaluating.\n",
    "                           \n",
    "    Returns:\n",
    "    \n",
    "        - accuracy : float.\n",
    "                     Classification accuracy of the model on this data and task.\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        accs = []\n",
    "        # Iterate over batches\n",
    "        for i in tqdm.tqdm(range(len(input_tensors)), desc=\"accuracy batches progress\"):\n",
    "            input_tensor_batch = input_tensors[i].to(device)\n",
    "            target_tensor_batch = target_tensors[i].to(device)\n",
    "            # Run the model\n",
    "            output = model(input_tensor_batch, \n",
    "                           output_attentions=False, \n",
    "                           task_ids=[task_num])\n",
    "            # Get classification prediction for the task of interest\n",
    "            preds = output.logits[task_num].argmax(axis=1)\n",
    "            # Get accuracy of given batch\n",
    "            batch_acc = ((preds == target_tensor_batch).sum() \\\n",
    "                         / target_tensor_batch.shape[0]).item()\n",
    "            # Add accuracy to list of accuracies\n",
    "            accs.append(batch_acc)\n",
    "        # Return the total accuracy, averaged over the batches\n",
    "        return np.mean(accs)\n",
    "\n",
    "def sent_pred(sent, model, tokenizer, device, batch_size):\n",
    "    '''\n",
    "    Runs the model on an input sentence.\n",
    "    \n",
    "    Arguments: \n",
    "    \n",
    "      sent  : str. \n",
    "              The input sentence.\n",
    "      model : the PyTorch sequence classifier model.  \n",
    "              Assumed to be a JointSeqClassifier or \n",
    "              DistilBertForSequenceClassification model\n",
    "    Returns:\n",
    "    \n",
    "      pred  : np array. \n",
    "              The prediction, wich is a normalized array with a value for \\\n",
    "              each class, representing the predicted probability for that class\n",
    "      attns : tuple of tensors\n",
    "              each entry in tuple is the attention matrix for an attention head.\n",
    "    '''\n",
    "    model.eval()\n",
    "    input_tensor = tokenizer.encode(sent, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model(input_tensor, output_attentions=True, task_ids=range(len(model.classifier)))\n",
    "\n",
    "    preds = []\n",
    "    scores = []\n",
    "    # Iterate over tasks and get class predition and scorews for each.\n",
    "    for i in range(model.num_tasks):\n",
    "        pred = output.logits[i].argmax(axis=1)\n",
    "\n",
    "        softmax = torch.nn.Softmax(dim=1)\n",
    "        score = softmax(output.logits[i].detach())\n",
    "\n",
    "        preds.append(pred.detach().cpu().numpy())\n",
    "        scores.append(score)\n",
    "\n",
    "    attns = output.attentions\n",
    "\n",
    "    return preds, scores, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stanford Politeness:\n",
    "train_polite_file = \"../../../cross_style_transfer_internal/data/xslue/StanfordPoliteness/train.tsv\"\n",
    "dev_polite_file = \"../../../cross_style_transfer_internal/data/xslue/StanfordPoliteness/dev.tsv\"\n",
    "train_polite_data = pd.read_csv(train_polite_file, names=['domain', 'id', 'text', 'score'], sep='\\t')\n",
    "val_polite_data = pd.read_csv(dev_polite_file, names=['domain', 'id', 'text', 'score'], sep='\\t')\n",
    "\n",
    "\n",
    "# Short Humor\n",
    "train_humor_file = \"../../../cross_style_transfer_internal/data/xslue/ShortHumor/train.tsv\"\n",
    "dev_humor_file = \"../../../cross_style_transfer_internal/data/xslue/ShortHumor/dev.tsv\"\n",
    "train_humor_data = pd.read_csv(train_humor_file, names=['domain', 'score', 'text'], sep='\\t', error_bad_lines=False)\n",
    "val_humor_data = pd.read_csv(dev_humor_file, names=['domain', 'score', 'text'], sep='\\t', \n",
    "                       quoting=3, error_bad_lines=False)\n",
    "\n",
    "# Dictionary for the classes for each task\n",
    "task_names = [\"Politeness\", \"Humor\"]\n",
    "class_labels_dict = [{0: \"impolite\", 1 : \"polite\"}, \n",
    "                     {0: \"humorous\", 1 : \"not humorous\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_humor(humor_df):\n",
    "    '''\n",
    "    Parse short humor dataframe into the format we need for classification.\n",
    "    \n",
    "    Returns a DataFrame with two columns, one for the input text and one for the class labels.\n",
    "    '''\n",
    "    input_df = pd.DataFrame()\n",
    "    input_df['text'] = humor_df['text']\n",
    "    input_df['label'] = humor_df['score']\n",
    "    return input_df\n",
    "\n",
    "def parse_stanford_politeness(polite_df):\n",
    "    '''\n",
    "    Parse stanford politeness dataframe into the format we need for classification.\n",
    "    \n",
    "    Returns a DataFrame with two columns, one for the input text and one for the class labels.\n",
    "    '''\n",
    "    input_df = pd.DataFrame()\n",
    "    input_df['text'] = polite_df['text']\n",
    "    # Map scores >= 0 (polite) to label 1 and scores < 0 (impolite) to label 0.\n",
    "    input_df['label'] = polite_df['score'].apply(lambda x : int(x >= 0))\n",
    "    return input_df\n",
    "\n",
    "def df_to_training_pairs(df, tokenizer, batch_size):\n",
    "    '''\n",
    "    Convert DataFrames with a 'text' and 'label' column into two lists of tensors, \n",
    "    one with texts encoded by the tokenizer and one with the class labels.\n",
    "    '''\n",
    "    input_tensors = df['text'].apply(lambda x : tokenizer.encode(x, \n",
    "                                                                 padding='max_length', \n",
    "                                                                 truncation=True, \n",
    "                                                                 return_tensors=\"pt\"))\n",
    "    target_tensors = df['label'].apply(lambda x : torch.LongTensor([x]))\n",
    "    return input_tensors.values.reshape(-1, batch_size).tolist(), target_tensors.values.reshape(-1, batch_size).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DataFrames for tasks\n",
    "train_polite_df = parse_stanford_politeness(train_polite_data)\n",
    "val_polite_df = parse_stanford_politeness(val_polite_data)\n",
    "\n",
    "train_humor_df = parse_humor(train_humor_data)\n",
    "val_humor_df = parse_humor(val_humor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "input_polite_tensors, target_polite_tensors = df_to_training_pairs(train_polite_df.head((len(train_polite_df)//batch_size)*batch_size), \n",
    "                                                                   tokenizer, batch_size)\n",
    "input_polite_val_tensors, target_polite_val_tensors = df_to_training_pairs(val_polite_df.head((len(val_polite_df)//batch_size)*batch_size), \n",
    "                                                                  tokenizer, batch_size)\n",
    "\n",
    "input_humor_tensors, target_humor_tensors = df_to_training_pairs(train_humor_df.head((len(train_humor_df)//batch_size)*batch_size), \n",
    "                                                                   tokenizer, batch_size)\n",
    "input_humor_val_tensors, target_humor_val_tensors = df_to_training_pairs(val_humor_df.head((len(val_humor_df)//batch_size)*batch_size), \n",
    "                                                                  tokenizer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(target_polite_tensors)):\n",
    "    input_polite_tensors[i] = torch.stack(input_polite_tensors[i])\n",
    "    input_polite_tensors[i] = input_polite_tensors[i].reshape(len(input_polite_tensors[i]), \n",
    "                                       input_polite_tensors[i].shape[2])\n",
    "    target_polite_tensors[i] = torch.LongTensor(target_polite_tensors[i])\n",
    "for i in range(len(target_polite_val_tensors)):\n",
    "    input_polite_val_tensors[i] = torch.stack(input_polite_val_tensors[i])\n",
    "    input_polite_val_tensors[i] = input_polite_val_tensors[i].reshape(len(input_polite_val_tensors[i]), \n",
    "                                       input_polite_val_tensors[i].shape[2])\n",
    "    target_polite_val_tensors[i] = torch.LongTensor(target_polite_val_tensors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(target_humor_tensors)):\n",
    "    input_humor_tensors[i] = torch.stack(input_humor_tensors[i])\n",
    "    input_humor_tensors[i] = input_humor_tensors[i].reshape(len(input_humor_tensors[i]), \n",
    "                                       input_humor_tensors[i].shape[2])\n",
    "    target_humor_tensors[i] = torch.LongTensor(target_humor_tensors[i])\n",
    "for i in range(len(target_humor_val_tensors)):\n",
    "    input_humor_val_tensors[i] = torch.stack(input_humor_val_tensors[i])\n",
    "    input_humor_val_tensors[i] = input_humor_val_tensors[i].reshape(len(input_humor_val_tensors[i]), \n",
    "                                       input_humor_val_tensors[i].shape[2])\n",
    "    target_humor_val_tensors[i] = torch.LongTensor(target_humor_val_tensors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_single(input_tensors[0], target_tensor_joint_test, model, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training batches progress: 100%|██████████| 308/308 [01:03<00:00,  4.82it/s]\n",
      "accuracy batches progress:   1%|          | 2/308 [00:00<00:20, 14.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "\tLoss 0.6841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy batches progress: 100%|██████████| 308/308 [00:21<00:00, 14.09it/s]\n",
      "accuracy batches progress: 100%|██████████| 16/16 [00:01<00:00, 14.14it/s]\n",
      "training batches progress:   0%|          | 0/1181 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Accuracy 0.5496\n",
      "\tValidation Accuracy 0.5605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training batches progress: 100%|██████████| 1181/1181 [03:59<00:00,  4.93it/s]\n",
      "accuracy batches progress:   0%|          | 2/1181 [00:00<01:21, 14.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "\tLoss 0.6467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy batches progress: 100%|██████████| 1181/1181 [01:21<00:00, 14.53it/s]\n",
      "accuracy batches progress: 100%|██████████| 65/65 [00:04<00:00, 14.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining Accuracy 0.5002\n",
      "\tValidation Accuracy 0.4990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6466668239596736], [0.5001587637595258], [0.49903846153846154])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epochs = 1\n",
    "train(input_polite_tensors, target_polite_tensors, \n",
    "      input_polite_val_tensors, target_polite_val_tensors, \n",
    "      model, optims[0], train_epochs, task_ids=[0])\n",
    "\n",
    "train(input_humor_tensors, target_humor_tensors, \n",
    "      input_humor_val_tensors, target_humor_val_tensors, \n",
    "      model, optims[1], train_epochs, task_ids=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Could you please help me?\"\n",
    "\n",
    "preds, scores, attns = sent_pred(sent, model, tokenizer, device, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum over attention vectors for each head and handle dimensions and move to cpu\n",
    "viz_attns = np.array([attn.sum(axis=1).cpu().detach().squeeze().numpy() for attn in attns])\n",
    "# Sum over heads\n",
    "viz_attns = viz_attns.sum(axis=0)\n",
    "# Drop cls and sep tokens\n",
    "viz_attns = viz_attns[0, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(sent))[1:-1]\n",
    "scores = [score.cpu().detach().squeeze().numpy() for score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input: ['could', 'you', 'please', 'help', 'me', '?']\n",
      "\n",
      "Task: Politeness\n",
      "Style: polite\n",
      "Class Scores: polite : 63.15%, impolite : 36.85%\n",
      "\n",
      "Task: Humor\n",
      "Style: not humorous\n",
      "Class Scores: not humorous : 72.54%, humorous : 27.46%\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create heatmap\n",
    "print(f\"Tokenized Input: {tokens}\\n\")\n",
    "for i, task_name in enumerate(task_names):\n",
    "    print(f\"Task: {task_name}\")\n",
    "    print(f\"Style: {class_labels_dict[i][preds[i][0]]}\")\n",
    "    print(f\"Class Scores: {class_labels_dict[i][1]} : {scores[i][1]*100:.2f}%, {class_labels_dict[i][0]} : {scores[i][0]*100:.2f}%\\n\")\n",
    "fig, ax = plt.subplots(figsize=(10, 2))\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.set_xlabel(\"Token Position\")\n",
    "plt.imshow([viz_attns], cmap='Reds');\n",
    "#plt.savefig(f'Attention_humor_and_politeness_heatmap_{\"_\".join(sent.split())}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGpCAYAAADFpuEPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABYmElEQVR4nO3dd3hUVf7H8fdJoQQCggQQQyhSRCD0JoIUpSviz4ICu+gqq1jXroCKC1ZUECuiIoiogNIUxKUuFkpogoDU0KTXVFLO748ElhpCyMyZ8nk9zzzM3Llz72cYSL5zzrnnGGstIiIiIuJdIa4DiIiIiAQjFWEiIiIiDqgIExEREXFARZiIiIiIAyrCRERERBwIcx3gQpUqVcpWrFjRdQwRERGR84qLi9tnrY0623N+V4RVrFiRJUuWuI4hIiIicl7GmPhzPafuSBEREREHVISJiIiIOKAiTERERMQBFWEiIiIiDqgIExEREXFARZiIiIiIAyrCRMTjWrVqpallREROoyJMRERExAEVYSKSb7Zs2cKVV15Jjx49qFGjBrfccgtJSUmn7DNz5kyaNWtG/fr1ufXWW0lISADgpZdeolGjRtSqVYs+ffpgrQXgnXfe4aqrriI2Npbu3bsDkJiYyN13303jxo2pV68ekydP9u4bFRHJByrCRCRfrVu3jr59+7JmzRqKFSvG+++/f+K5ffv2MWjQIP7zn/+wdOlSGjZsyFtvvQXAgw8+yOLFi1m1ahXJyclMmzYNgFdffZVly5axcuVKPvzwQwAGDx5MmzZtWLRoEXPmzOHJJ58kMTHR+29WROQiqAgTkXxVvnx5mjdvDkDPnj1ZsGDBied+++03/vjjD5o3b07dunX5/PPPiY/PWtFjzpw5NGnShNq1azN79mxWr14NQGxsLD169OCLL74gLCxrpbWZM2fy6quvUrduXVq1akVKSgpbt2718jsVEbk4Hl070hizBTgKZADp1tqGpz3fCpgMbM7e9K219iVPZhIRzzLGnPOxtZbrr7+ecePGnbJPSkoKffv2ZcmSJZQvX54XX3yRlJQUAL7//nvmz5/P1KlTGTx4ML///jvWWiZOnEj16tU9/4ZERDzEGy1hra21dU8vwE7y3+zn66oAE/F/W7du5ddffwXgyy+/5JprrjnxXNOmTfn555/ZsGEDkDW2688//zxRcJUqVYqEhAQmTJgAQGZmJtu2baN169a89tprHD58mISEBNq3b8/w4cNPjBtbtmyZN9+iiEi+UHekiFywsWOhYkUICcn6c+zY/z1XvXp13nvvPWrUqMHBgwe5//77TzwXFRXFqFGjuOOOO4iNjaVZs2asXbuWSy65hHvvvZdatWrRvn17GjVqBEBGRgY9e/akdu3a1KtXj4cffphLLrmEAQMGkJaWRmxsLDVr1mTAgAHe/QsQEckH5vg3SY8c3JjNwEHAAh9Za0ec9nwrYCKwHdgJPGGtXX2W4/QB+gDExMQ0OD6GRES8b+xY6NMHsi563AtEEREBI0ZA8+Zb6NKlC6tWrXKcUkTENxhj4s7VG+jplrBrrLX1gY7AA8aYlqc9vxSoYK2tAwwHJp3tINbaEdbahtbahlFRUR4NLCI569fveAH2O1AamExSUtZ2ERHJPY8WYdbaHdl/7gG+Axqf9vwRa21C9v0fgHBjTClPZhKRi/O/ixCrA9cDESe2V6xYUa1gIiK55LEizBhTxBgTefw+0A5Yddo+ZU32pVPGmMbZefZ7KpOIXLyYmOP3CgAzySrETt4uIiK54cmWsDLAAmPMCmAR8L21doYx5j5jzH3Z+9wCrMre5x2gu/XkIDURuWiDB0PBgguAVsAG4Bjh4R8yYIAmSxURuRAemyfMWrsJqHOW7R+edP9d4F1PZRCR/NejB6xYkcCwYUc5dqwMZcsuZdeu+wkLKwz83XU8ERG/4dGrIz2hYcOGdsmSJa5jiMhJ4uLiqF+//hkTtYqIBDuXV0eKSIBJTk4mMzPzlG0NGjTAGENqaqqjVCIi/kdFmIhckFdffZVKlSqdmOX+uDlz5hATE8PatWsdJRMR8S8eXTtSRAJPkyZNyMzMpFChQqdsr1WrFs2bNyckRN/tRERyQ2PCRERERDxEY8JEJF+sWbOGgwcP5rjPgQMHePPNN88YNyYiIqdSESYiuXbPPffQrl27HPeZOnUqTz31FGqxFhHJmcaEiUiuvf322xw9ejTHfXr27Enjxo2pUaOGl1KJiPgnFWEikmuNGzc+7z6hoaEnCrCkpCQiIiI8HUtExC+pO1JEcuXzzz9n9erVud5/xIgRVK5c+bxjyEREgpWKMBE5r6SkJO677z5GjRqV69c0bdqUrl27aoC+iMg5aIoKEcmVvXv3kpGRQdmyZV1HERHxG5qiQkQuWlRUVJ4KsHXr1vHOO+94IJGIiH9TESYiOUpJSaFXr14sXrw4T68fM2YMAwYMYO/evfmcTETEv6kIE5Ec/fnnn8ycOZMDBw7k6fVPP/00f/75J1FRUfmcTETEv2mKChHJUWxsLDt37szz6yMjI4mMjATg8OHDFC9ePL+iiYj4NbWEich5hYaGEhoaelHHeOSRR2jWrBnp6en5lEpExL+pCBORc5o/fz4NGjRg7dq1F32s66+/nr/97W+askJEJJu6I0XknNLT04mMjCQ6Ovqij9WlSxe6dOmSD6lERAKDWsJE5JzatGnD3LlzKVq0aL4dc/bs2Xz66af5djwREX+lIkxEzuro0aOkpaXl+3E/+OAD3nzzTTIyMvL92CIi/kRFmIic1euvv065cuVISkrK1+O+//77xMXFXfRAfxERf6cxYSJyVm3btiUiIoKIiIh8Pe7x+cIyMjI4fPgwJUuWzNfji4j4CxVhInJWrVq1olWrVh45trWWtm3bEhkZydSpUz1yDhERX6ciTETOsHz5ckqXLk25cuU8cnxjDL169aJIkSJYazHGeOQ8IiK+zFhrXWe4IA0bNrRLlixxHUMkoF199dUcO3YM/V8TEbk4xpg4a23Dsz2nljAROcPIkSPzvFbkhcjIyGDUqFGUKlWKrl27evx8IiK+REWYiJzhqquu8sp5jDG89957VK1aVUWYiAQdFWEicooPPviAhg0b0qhRI4+fKyQkhBkzZpy4YlJEJJhonjAROSE5OZmnnnqKCRMmeO2cpUuXxhhDQkICR44c8dp5RURcU0uYiJxQuHBhdu7cSWpqqlfPm5CQwJVXXsltt93GW2+95dVzi4i4oiJMRE4RGRlJZGSkV89ZtGhR/vWvf9G8eXOvnldExCV1R4oIkNUV2a1bNxYsWODk/I8//jhNmzZ1cm4RERdUhIkIAJs3b2bFihX5vlbkhUhMTKR///4sWrTIWQYREW9Rd6SIAFnTUmzcuBGXEzhnZmYycuRIIiIiaNy4sbMcIiLe4NEizBizBTgKZADpp88Ya7LWKhkGdAKSgN7W2qWezCQiZzpeeBljnC4hFBkZyZo1ayhRooSzDCIi3uKN7sjW1tq655iyvyNQNfvWB/jAC3lE5DTz5s2jWrVqrFq1ynWUEwXY9u3bSU9Pd5xGRMRzXI8J6wqMtll+Ay4xxlzmOJNI0AkPD6d69epUqlTJdRQA1q5dS9WqVRk5cqTrKCIiHuPpIswCM40xccaYPmd5/nJg20mPt2dvExEvat68OdOmTaNIkSKuowBQvXp1+vXrR8eOHV1HERHxGE8PzL/GWrvDGFMa+MkYs9ZaO/9CD5JdwPUBiImJye+MIkHt4MGDFChQwGcKMMgam9a/f3/XMUREPMqjLWHW2h3Zf+4BvgNOv9xpB1D+pMfR2dtOP84Ia21Da21DrTEnkr/efPNNLrvsMhITE11HOcOOHTu4++672blzp+soIiL5zmNFmDGmiDEm8vh9oB1w+qjfKcDfTJamwGFr7V+eyiQiZ7rhhhv497//7VMtYcelpqby7bffsnDhQtdRRETynSe7I8sA32Vf7h4GfGmtnWGMuQ/AWvsh8ANZ01NsIGuKirs8mEdEzqJJkyY0adLEdYyzqly5Mtu3b6do0aKuo4iI5DuPFWHW2k1AnbNs//Ck+xZ4wFMZRCRnCxcupESJElSrVs11lHM6XoCtW7eO6tWrO04jIpJ/XE9RISIOPfroo9x5552uY5zXlClTuPLKK5k1a5brKCIi+UbLFokEsW+++Ybdu3e7jnFe7dq145VXXtFSRiISUIzLdeLyomHDhnbJkiWuY4iIiIiclzEm7hyrBqk7UiRYvfnmm8ybN891jAuybNkybr/9dpKTk11HERG5aCrCRIJQamoqr7zyCtOnT3cd5YIcOnSI+fPns27dOtdRREQumsaEiQShggULsnPnTpKSklxHuSCtW7dm8+bNFCpUyHUUEZGLppYwkSBVoEABLrnkEtcxLlihQoWw1rJixQrXUURELoqKMJEgk5yczPXXX8/s2bNdR8mzV199lUaNGrF582bXUURE8kzdkSJBZtu2bezZs4fMzEzXUfKsd+/elC5dmpiYGNdRRETyTFNUiAQpay3Zy4qJiIiHaIoKEQEgMzPzRAtYIBRgU6ZMoWfPnvjbl0kREVARJhJU5s6dy+WXX87y5ctdR8kXu3btYtWqVezbt891FBGRC6YiTCSIREZG0qpVK6pWreo6Sr74xz/+QVxcHFFRUa6jiIhcMBVhIkGkUaNGjBs3jiJFiriOki9CQ0MJDQ0lJSWFZcuWuY4jInJBVISJBIm9e/dy4MAB1zE84u6776Z9+/Z+N/msiAQ3FWEiQWLo0KFcfvnlJCQkuI6S755++mnGjRtHRESE6ygiIrmmecJEgsTtt99OxYoVKVq0qOso+a5OnTquI4iIXDAVYSJBIjY2ltjYWNcxPOrtt98mPj6eoUOHuo4iInJe6o4UCQLz588nLi7OdQyP27FjB/Hx8WRkZLiOIiJyXpoxXyQINGvWjIyMDBYtWuQ6ikdlZGQQGhrqOoaIyAk5zZiv7kiRIDBt2jR27tzpOobHHS/Adu/ezd69e6lVq5bjRCIi56YiTCQIXHrppVx66aWuY3iFtZb27dsTHh7OokWLAmJ5JhEJTCrCRALcv//9bxo2bEjHjh1dR/EKYwzvvPMOZcqUUQEmIj5NRZhIADt27BgjRowgOTk5aIowgJYtW7qOICJyXirCRAJYgQIFiI+PJzk52XUUr0tPT+fBBx/kiiuu4Mknn3QdR0TkDJqiQiTAhYSEBMxakRciLCyMffv2cfjwYddRRETOSkWYSIBKSkqiWbNmTJ8+3XUUZ8aPH8+gQYNcxxAROSsVYSIB6q+//iI0NJSCBQu6juLM8YH5q1atYsuWLW7DiIicRmPCRALUFVdcwYIFC1zHcC4hIYHmzZvTtWtXRo8e7TqOiMgJKsJEAlBGRgYZGRkUKFDAdRTnihYtyvjx42nQoIHrKCIip1B3pEgAmjt3LqVLl0ZLfGVp167diclq/W2pNhEJXCrCRAJQVFQUt956K1dddZXrKD5j3759tG/fnm+//dZ1FBERQN2RIgEpNjaWjz/+2HUMn1KiRAlSUlJISkpyHUVEBFARJhJw/vrrL9LT0ylfvrzrKD4lNDSUuXPnaikjEfEZ6o4UCTDvvPMOV1xxBUeOHHEdxecYY7DWMn36dE3iKiLOqQgTCTD/+Mc/GDVqFMWKFXMdxSetW7eOTp068cEHH7iOIiJBzuPdkcaYUGAJsMNa2+W053oDbwA7sje9a60d6elMIoGsSpUqVKlSxXUMn3XllVcyffp02rRp4zqKiAQ5b4wJewRYA5zra/nX1toHvZBDJOD99NNPhIeH06pVK9dRfFqHDh2ArOkqNEZMRFzxaHekMSYa6AyodUvEC1566SWefvpp1zH8wqpVq6hduzbLly93HUVEgpSnW8KGAk8BkTns83/GmJbAn8C/rLXbTt/BGNMH6AMQExPjgZgigWHGjBns2LHj/DsK0dHRFC9enKNHj7qOIiJBynhq9mhjTBegk7W2rzGmFfDEWcaEXQokWGtTjTH/BG631uY4UKNhw4ZWs4CLiIiIPzDGxFlrG57tOU92RzYHbjTGbAG+AtoYY744eQdr7X5rbWr2w5GAFncTyaPnnnuOiRMnuo7hd9LS0hgzZgwZGRmuo4hIkPFYEWatfdZaG22trQh0B2Zba3uevI8x5rKTHt5I1gB+EblAaWlpfPvttyxdutR1FL/zww8/8Le//Y0ffvjBdRQRCTJenzHfGPMSsMRaOwV42BhzI5AOHAB6ezuPSCAIDw9nzZo1HDt2zHUUv3PjjTcya9YsWrdu7TqKiAQZj40J8xSNCRMRT8nIyCA0NNR1DBEJIK7GhImIFyQmJlK7dm0mTZrkOopf++GHH6hatSp79uxxHUVEgoSKMBE/t2/fPsqXL0+JEiVcR/FrV1xxBVWrVtWUFSLiNeqOFBEREfEQdUeKBKj09HQSEhJcxwgoBw8eZNSoUa5jiEgQUBEm4sfmzp1LqVKl+O2331xHCRgfffQRd999N+vXr3cdRUQCnIowET8WHR3N/fffT506dVxHCRgPP/wwy5Yto2rVqq6jiIiXrV27lquvvpratWtz7bXXsm/fPo+eT2PCRETOIS0tjfDwcNcxRMRL1q5dS4ECBahcuTLPPvssRYoUoX///hd1TI0JEwlA27ZtY80aLTLhKcOGDaNOnTqaAFckiFx55ZVUrlwZgNTUVAoVKuTR86kIE/FT7733HrGxsRw+fNh1lIB05ZVXcvXVV5OUlOQ6ioh42Y8//sj06dO55557PHoedUeK+KkdO3awePFibrrpJtdRREQCRmZmJjExMcyZMydfxoaqO1IkAF1++eUqwLxgw4YNfPnll65jiIiX7Ny5k+LFi3vl4hwVYSJ+aNq0aUybNs11jKAwePBgHn74YXVLigSQsWOhYkUICcn6c+zY/z1XokQJ3nzzTa/kUHekiB9q06YNSUlJmh/MC3bt2gVA2bJlHScRkfwwdiz06QMnf6+KiIARI6BHj6yWsIcffpgJEybky/ly6o5UESbih9LS0vjrr7+IiYlxHSWopKSkePxqKRHxrIoVIT7+zO0VKsCWLfl/Po0JEwkw4eHhKsC87N5776Vz58742xdXETnV1q0AmcDDwOLTtnuXijARP/Poo48yZswY1zGCTpMmTWjdujWZmZmuo4jIRcj6/hoCDAWuPG27d4V5/5Qiklfp6en88ssvREZGuo4SdDw9X5CIeMe//53JP/9pSU4OBbJ+lkZEwODB3s+iIkzEj4SFhbFo0SIyMjJcRwla8+bN48iRI9xwww2uo4hIHhQrNpUiRR6hRImf+OuvqsTEZBVgPXp4P4uKMBE/Yq3FGENoaKjrKEHJWsuzzz5LRkYGXbp0wRjjOpKIXKCSJUvSpk0Txo6tRJjjKkhjwkT8RGJiIpUrV+brr792HSVoGWMYN24cc+fOVQEm4qdatGjB119/TZjrCgwVYSJ+4+DBgzRu3Jjo6GjXUYJahQoVKFy4MJmZmZrAVcTPzJs3j6NHj7qOcYKKMBE/ER0dzddff03z5s1dRwl66enpXH311TzxxBOuo4hILh09epTOnTvz+OOPu45ygvu2OBE5r7S0NA4fPkypUqVcRxGyLpC44YYbqFy5susoIpJLkZGRzJo1ixIlSriOcoJmzBfxAz/99BMdOnRg3rx5XHPNNa7jiIhILmnGfBE/V6VKFfr160eDBg1cR5GTZGRk8Pnnn/PLL7+4jiIiOZg1axbPPvusT40HAxVhIn6hUqVKvPTSSxQuXNh1FDnJsWPH6N+/P5999pnrKCKSg99++40vvvjC59Z+VXekiI/bvHkze/fupWHDhoSE6HuTr4mPj6d8+fL6bER8XFJSEhEREV4/r7ojfcCWLVuoVavWWZ9r1aoVKizlXD788EOaN2/OkSNHXEeRs6hQoQIhISEkJiaSmprqOo6InCYlJQXASQF2PirCRHzcM888ww8//MAll1ziOoqcw+7du6lWrRrvv/++6ygicpKjR48SExPDiBEjXEc5KxVhuTR69GhiY2OpU6cOvXr1YsuWLbRp04bY2Fjatm3L1q1bAejduzcTJkw48bqiRYuecazk5GS6d+9OjRo16NatG8nJyV57H+J/SpQowfXXX+86huSgTJky3HnnnTRp0sR1FBE5SUpKCrfddhv16tVzHeWsNE9YLqxevZpBgwbxyy+/UKpUKQ4cOMDf//73E7dPP/2Uhx9+mEmTJuXqeB988AERERGsWbOGlStXUr9+fc++AfFbEydOJDU1lTvvvNN1FDmPN954w3UEETlNVFQU7777rusY56SWsFyYPXs2t95664mJMkuWLMmvv/564hdjr169WLBgQa6PN3/+fHr27AlAbGwssbGx+R9aAsLIkSN9+geInCohIYGBAweyZcsW11FEgt6qVav4448/XMfIkYqwfBYWFkZmZiYAmZmZHDt2zHEi8Wfff/893333nesYkkuHDh3itddeY+rUqa6jiAS9AQMG0KZNG9LT011HOScVYbnQpk0bxo8fz/79+wE4cOAAV199NV999RUAY8eOpUWLFgBUrFiRuLg4AKZMmUJaWtoZx2vZsiVffvklkFWpr1y50htvQ/xQSEgIZcqUcR1Dcik6OpoNGzbw0EMPuY4iEvRGjBjBN998Q1iY7468UhF2mrFjoWJFCAnJ+nPsWKhZsyb9+vXj2muvpU6dOjz22GMMHz6czz77jNjYWMaMGcOwYcMAuPfee5k3bx516tTh119/pUiRImec4/777ychIYEaNWrw/PPPaxZ0Oas+ffrw0UcfuY4hF6hcuXIA7NmzB3+bh1EkkERFRdGyZUvXMXLk8clajTGhwBJgh7W2y2nPFQRGAw2A/cDt1totOR3Pk5O1jh0LffpAUtL/tkVEwIgR0KOHR04pclbp6em0b9+eFi1a8OKLL7qOIxdo0aJFtGrVim+++YYuXbqc/wUikm8SExO55557ePrpp6lbt67rOM4na30EWHOO5/4BHLTWVgHeBl7zQp5z6tcPkpIygc+ByUBWQdavn8tUEozCwsKYNWsWL7zwgusokgf16tXjn//8JzVr1nQdRSTo/PHHH8yaNYvExETXUc7Loy1hxphosiqawcBjZ2kJ+xF40Vr7qzEmDNgFRNkcQnmyJSwkBKzNBBoClYEJ2Tkhe6y9iFdkZGQQGhrqOoaIiF9KTU2lQIECGGNcR3HaEjYUeAo4VwlzObANwFqbDhwGLj19J2NMH2PMEmPMkr1793ooKsTEQNZfyXRg/GnbRbwjISGBcuXK8fnnn7uOIhdpx44dPPTQQyQkJLiOIhIUEhMTsdZSsGBBnyjAzsdjRZgxpguwx1obd7HHstaOsNY2tNY2jIqKyod0Zzd4cNYYMCgDGCCJwoWTGTzYY6cUOUNCQgI33ngj1apVcx1FLtK2bdsYOXIkv/76q+soIkGhV69edOrUyXWMXPPkdZvNgRuNMZ2AQkAxY8wX1tqeJ+2zAygPbM/ujixO1gB9J44Pvu/XD+LjDxIaWosuXfrSo4cGhYn3lC1blo8//th1DMkHTZs2Zdu2bScmehYRz+rQocNZp4byVR6/OhLAGNMKeOIsY8IeAGpba+8zxnQHbrbW3pbTsTw5Jux0zz//PO3bt6d58+ZeOZ/IsWPH+Ouvv6hQoYLrKJLPtm/fTnR0tOsYIuJlrq+OPD3MS8aYG7MffgJcaozZADwGPOPtPDl56aWXVICJV82ZM4eKFSsyd+5c11EkH40ZM4ZKlSqxevVq11FEAlJycjITJ0706dnxz8YrRZi1du7xVjBr7fPW2inZ91Ostbdaa6tYaxtbazd5I8+FSEhI4KWXXmLbtm2uo0gQqFWrFm+88QZNmzZ1HUXyUceOHXnmmWfUEibiIRMnTuSWW27hl19+cR3lgnilOzI/ebM7EiA+Pp7q1avz1ltv0bdvX6+dV0RERHInIyODWbNmcf311/vcVZE+1R3pbypUqMCGDRtUgInHrV+/nlmzZvldc7rk3tKlS+nbty+ZmnhQJF+FhobSrl07nyvAzkdFWC4c70I4dOiQ2yAS0EaOHEnHjh39YpZnyZvVq1czYcIENm3yuZEXIn7rySef5JNPPnEdI09UhOXSrFmzuPzyy1m8eLHrKBKgXnjhBebMmUPx4sVdRxEP6dGjBxs3bqRKlSquo4gEhPT0dBYuXMjatWtdR8kTT84TFlAaN25Mz5498eRksRLcIiIidDVugAsJCSEyMhJrLZs3b6Zy5cquI4n4tbCwMObPn+9Xc4OdTC1huRQZGclHH31ExYoVXUeRADRu3Dg++ugj/O1CGcmbZ555hgYNGnDgwAHXUUT81rFjx0hKSgIgPDzccZq8URF2gbZu3crAgQP1y1Ly1fjx4xk9erTfDSqVvPnb3/7Ga6+9RrFixVxHEfFbX331FeXLl2fjxo2uo+SZuiMv0Jw5c3j55Zfp1q0bsbGxruNIgJg4cSJHjhxxHUO8pGbNmtSsWdN1DBG/VrNmTXr27OnX3fqaJ+wCZWRksHPnTsqXL+8sg4gEhsmTJ/Pzzz/z+uuvu44iIh6iecLyUWho6IkC7OjRo47TSCDo1asXb7/9tusY4sCSJUuYMWOGpiURuUATJ05k165drmNcNBVheTR48GBq1qxJcnKy6yjixzIyMjhy5Ij+HQWpfv36sWzZMooUKeI6iojfOHjwIHfeeSevvvqq6ygXTWPC8qhFixYkJiaSkZHhOor4sdDQUCZPnuw6hjhSqFAhAFJTU9m2bZvmDxPJhRIlSrBq1SoiIiJcR7loGhMm4lBqaioFCxZ0HUMc69SpE5s2bWLVqlWEhem7sUgg0ZgwD1q0aBGjR492HUP80NGjRyldujQjRoxwHUUce/LJJxk+fLgKMJHzmDhxIn369AmYMdn6H3+R3n77bRYuXMgdd9zht5PFiRspKSn84x//oG7duq6jiGOtW7d2HUHEL2zevJlFixYFzDhKdUdepN27dxMREUFkZKTrKCLix6y1vPnmm1hrefLJJ13HEfFZGRkZhIaGuo6Razl1R+aqJcwYczlQ4eT9rbXz8yeefytTpgyQ9QM0OTk5IAYKiucdO3aM9evXc9VVV2mWfAHAGMOSJUvIyMjAWqt/FyKn2b9/P5deeqlfFWDnc94xYcaY14Cfgf7Ak9m3Jzycy69Ya2nfvj19+vRxHUX8xOzZs6lVqxb/+c9/XEcRH/L5558zfvx4FWAip9mzZw/R0dG8//77rqPkq9y0hN0EVLfWpno4i98yxtC+fXutAye51qBBAz744ANatGjhOor4kONXyu7bt4/ExEQqVKjgOJGIbwgLC+Ppp5+mbdu2rqPkq/OOCTPGTAdutdYmeCdSznxtTJiISH5KT0+natWq1KpVi6lTp7qOIyIX6WLHhCUBy40xs4ATrWHW2ofzKV/AsNYyYcIEYmJiaNKkies44qPWrl3LmjVr6NSpk+YIkzOEhYUxdOhQqlWr5jqKiE/49ddfSU9P55prrgm4rvrcFGFTsm9yHsnJyTzyyCO0b99eRZic0+eff85bb73F3r17VYTJWXXt2tV1BBGfMWjQINasWcP69esDalA+5HKKCmNMAeD417J11to0j6bKga93R65fv57KlSsH3D8UyT9paWmsWrWKevXquY4iPiwpKYknn3yS5s2bc+edd7qOI+JMUlISGzdupHbt2q6j5MlFdUcaY1oBnwNbAAOUN8b8XVNUnF3VqlWBrCkIQkNDVYzJGcLDw1WAyXkVKlSIuLi4E9PgiASriIgIvy3Azic3yxa9CbSz1l5rrW0JtAfe9mws/7Z161aqV6/OuHHjXEcRHzNq1CiGDBmCv02SLN4XEhLCggULeP75511HEXFi//79tGnThkWLFrmO4jG5KcLCrbXrjj+w1v4JaH2eHERHR9OyZUvKly/vOor4mFmzZjF58uSAG1wqnnF8Lck1a9awf/9+x2lEvGvz5s1s27YtoCdBz80UFZ8CmcAX2Zt6AKHW2rs9nO2sfH1MmMj5pKSkUKhQIdcxxE/s3r2bmJgYHnroIYYMGeI6johXBcLqETmNCctNS9j9wB/Aw9m3P7K3yXkkJyczbNgwEhJ8Yoo18REqwORClClThtGjR/PMM8+4jiLiNbt37yYjI8PvC7Dz0QLeHrRw4UKaNm3KmDFj6Nmzp+s44tgtt9xC3bp16d+/v+soIiI+rU2bNkDWEm/+Lk9XRxpjvrHW3maM+R04o1Kz1sbmY8aA1KRJE1auXBmwV3VI7mVmZlK4cGG1gkme7dixgz59+jBgwACaNm3qOo6IR/Xt25f09HTXMTwupykqHsn+s4s3ggSq4wVYamqqJuYMYiEhIYwZM8Z1DPFjxYsXZ9OmTcTHx6sIk4B3yy23uI7gFeccE2at/Sv7bl9rbfzJN6Cvd+IFhqlTpxITE8O2bdtcRxFHjh496jqC+LmiRYuyevVqbr/9dtdRRDzm4MGDfPjhhyQmJrqO4hW5GZh//Vm2dczvIIEsNjaWFi1akJGR4TqKOHDkyBHKlCnD8OHDXUcRPxcSkvUje86cOaSlOVu4RMRjJk+ezP3338/69etdR/GKcxZhxpj7s8eDVTfGrDzpthlY6b2I/q9ChQpMmDCBihUruo4iDqSnp/Pkk09y9dVXu44iAeDnn3+mTZs2jB492nUUkXz397//neXLl1O3bl3XUbzinFdHGmOKAyWAV4CTr40+aq094IVsZ+VPV0eebvfu3UydOpV77rnHdRQR8VPWWr7++mu6deumcaYifiBP84RZaw9ba7dYa+8AtgNpZF0lWdQYE+OZqIFtxIgR3H///WzdutV1FPGS1NRUfv31VzIzM11HkQBhjKF79+4qwCTg9OrVi2HDhrmO4VXnHRNmjHkQ2A38BHyffZuWi9cVMsYsMsasMMasNsYMPMs+vY0xe40xy7NvAd1E9Oijj/LHH38QE6MaNljMmjWLq6++mpkzZ7qOIgEmLi6Oxo0bs337dtdRRC5aWloaR44cISkpyXUUr8ppiorjHgWqW2svdOGyVKCNtTbBGBMOLDDGTLfW/nbafl9bax+8wGP7pcjISCIjI4Gsf3Dh4VqCM9A1b96cL774gtatW7uOIgGmZMmSJCUlsXPnTqKjo3Pcd8uWLXTp0oVVq1bl6tgvvvgiRYsW5YknnsiPqCLnFR4ezuTJk/G3CeQvVm6ujtwGHL7QA9ssx9frCc++Bdff7jk8//zztGzZMuj+sQWj4sWL06NHD3UdSb6rVKkSv//+O40bN3YdReSiJCYmsm/fPoCAX6bodLkpwjYBc40xzxpjHjt+y83BjTGhxpjlwB7gJ2vtwrPs9n/ZV11OMMaUP8dx+hhjlhhjluzduzc3p/Zp1atXp0mTJqSkpLiOIh60atUqPv/886BrXhfvMcaQnp7O1KlTz7tvRkYG9957LzVr1qRdu3YkJyezceNGOnToQIMGDWjRogVr164943WtWrXikUceoW7dutSqVYtFixZ54q1IEPvkk08oX7488fHxrqN4XW6KsK1kjQcrAESedDsva22GtbYuEA00NsbUOm2XqUDF7CWQfgI+P8dxRlhrG1prG0ZFReXm1D6tR48eDB06lMKFC7uOIh70zTff0KdPn6BYekPcGTVqFDfeeCO//vprjvutX7+eBx54gNWrV3PJJZcwceJE+vTpw/Dhw4mLi2PIkCH07Xv2ebiTkpJYvnw577//Pnfffbcn3oYEsXbt2vHCCy9QoUIF11G87rxjwqy1AwGMMRHW2jx9pbfWHjLGzAE6AKtO2n7yOLORwOt5Ob6/WrlyJbt27aJdu3auo4gHDBw4kJ49e1KsWDHXUSSA9erVi7Jly553KaNKlSqdmHupQYMGbNmyhV9++YVbb731xD6pqalnfe0dd9wBQMuWLTly5AiHDh3ikksuyZf8IldeeSXPPPPM+XcMQOctwowxzYBPgKJAjDGmDvBPa22OSxcZY6KAtOwCrDBZM++/dto+l520PNKNwJo8vAe/1bdvXw4fPszKlSuDrh88GBhjqFatmusYEuAKFixIly5ZS/xaa8/5s+TkcYmhoaHs3r2bSy65hOXLl5/3HKcfUz+vJL988MEHtGnThurVq7uO4kRuuiOHAu2B/QDW2hVAy1y87jJgjjFmJbCYrDFh04wxLxljbsze5+Hs6StWAA8DvS8wv1/79NNPmTdvnn6gBaARI0bw/PPP6+IL8ZrvvvuORo0akZycnKv9ixUrRqVKlRg/fjyQVcCtWLHirPt+/fXXACxYsIDixYtTvHjx/AktQW3fvn089thjjBs3znUUZ3IzRQXW2m2nFQrnXQTRWrsSqHeW7c+fdP9Z4NncZAhEJ7eSZGRkEBoa6jCN5KelS5fy559/qsAWrylZsiSJiUWpWnUfO3eWJyYGBg+GHj3O/ZqxY8dy//33M2jQINLS0ujevTt16tQ5Y79ChQpRr1490tLS+PTTTz34LiSYlCpVii1btgT1dE3nXLboxA7GTADeAt4FmgCPAA2ttd09H+9M/rxs0dmkpaXRuXNnGjduzKBBg1zHkXyUnp5OWFiuvueIXLSxY+Heey3Jyf8r/CMiYMSInAux82nVqhVDhgyhYcOzrroiIueRp2WLTnIf8ABwObADqAvkOB5Mci88PJxq1aqdd7JF8R/Hv9ioABNv6teP7ALsMDAJgKSkrO0ivubjjz/m5ptvJjEx0XUUp3LzW6K6tfaU71HGmObAz56JFHzeffdd1xEkH3Xq1Il69erx8ssvu44iQeR/S9K+TFbnxSagPBe7VO3cuXMv7gAiZ3Hs2DGSkpKIiIhwHcWp3LSEDc/lNrkI1lp++OEHNm/e7DqKXITMzEwqVqxI2bJlXUeRIPO/JWn7kfUdufxp20V8xwMPPMCMGTOCftzsOVvCsqemuBqIOm2G/GKARpDns71793LLLbfQp08fhg4d6jqO5FFISAgffPCB6xgShAYPhj59ICmpGJC1lFHhwkcYPFjz1IlvWb9+PVWrVnUdwyfk1BJWgKy5wcI4dab8I8Atno8WXEqXLs3s2bN5/fWgmq824OzZs8d1BAlSPXpkDcKvUAGMgVKlvsaYCjRvvsV1NJETNm/eTPXq1TUMJ9s5izBr7bzs2fLfttYOPOn2FlmD8yWfNW3alAIFCpCZmek6iuTB4cOHKV++PG+88YbrKBKkevSALVsgMxOWLr2a7t1vpkiRIq5jiZwQFRXFO++8w0033eQ6ik/IzZiws01FEbRze3na2rVrqVmzJv/9739dR5E8ePnll7UMlfiE8uXL88knnxAI6+1K4ChatCgPPvigZgTIltOYsI5AJ+ByY8w7Jz0VCaR5OliwiomJoVy5cmoN80PFixfn8ccfdx1D5BRbt25lwIABDBs2TOs9ilM//PADqamp3HTTTUE/IP+4nFrCdgJxQEr2n8dvm4HAmS3Vx0RERDBr1iyuvfZa11HkAqSkpDBz5kyOHTvmOorIKfbt28ekSZMIpEmuxT+9++67DBw40HUMn5LTmLAV1tpRQBVgJVALGAi0JsgW2nbh2LFjjBkzRi1ifmLWrFm0b9+e2bNnu44icor69euzbds2rrvuOtdRJMhNmTKFKVOmqBXsJOcswowx1YwxLwC/kzUv2Fayljlqba3VZQ0eNm3aNP72t78xY8YM11EkF9q0acPkyZNp06aN6ygiZyhWLGuaivnz56u1Vpyw1hIWFkaMJq47RU7dkWuBNkAXa+011trh5GLhbskfN910E7Nnz6Zjx46uo0guFC5cmBtvvJECBQq4jiJyVsuWLePaa6/lo48+ch1Fgsy2bduoXbs2v/zyi+soPienIuxm4C9gjjHmY2NMW0BtiF4SEhJC69atMcZwvkXWxa0VK1bw7rvvkpCQ4DqKyDnVq1ePL7/8knvvvdd1FAky+/btIzIykssvv9x1FJ+T05iwSdba7sCVwBzgUaC0MeYDY4yuwfeSb7/9ltq1a+sXvA+bOnUqTzzxhIpl8Xl33HEHhQoVIj09Xf9exWvq1avHr7/+SoUKFVxH8TnnnSfMWptorf3SWnsDEA0sA572eDIBoFy5cpQrV46DBw+6jiLn0L9/fzZu3EhkZKTrKCLntWvXLho1asTYsWNdR5EgsGHDBlJSUlzH8FnnnCfsbKy1B4ER2TfxgqZNmzJz5kzXMeQ81Mwu/qJ06dJcccUVmjNMPM5ay+23307RokWZN2+e6zg+6YKKMHHnwIED/Prrr3Tu3Nl1FDnJ8OHD2bRpE2+99ZYuuxa/EBISwoQJE1zHkCAxZMgQ0tI0v/u55GbZIvEBzz33HLfddhuHDh1yHUVOsnnzZtasWaMCTPyOtZZPPvlES6SJxxhjaN26tZZyy4Hxt8GZDRs2tME48/P27ds5ePAgtWvXdh1FTmOtVREmfic5OZlatWpx7bXX8umnn7qOIwFm586dfPrpp/Tt25eSJUu6juOUMSbOWtvwbM+pJcxPREdHnyjA/K1wDlTHVzNQASb+qHDhwsybN4+RI0e6jiIBaObMmbzwwgu6qOw8VIT5mX79+tGjRw/XMQS47rrreOSRR1zHEMmz6OhoQkJCOHLkCNu2bXMdRwJI7969iY+P54orrnAdxadpYL6fiYiIoGjRomRkZBAaGuo6TtCy1tKoUSMqV67sOorIRcnMzKR58+aULl2aWbNmuY4jAeD4EI3o6GjXUXyexoSJiAS5SZMmcfnll9OoUSPXUSQAtG3blrZt2/Lcc8+5juITchoTppYwP7V+/XoSEhKoV6+e6yhBacuWLVSoUEHjwSQg3HTTTSfu60ITuRgpKSlUqFCBqKgo11H8gsaE+aHMzEw6duzIv/71L9dRgtLhw4epVq0aL7/8susoIvlq0KBB3HPPPa5jiB8rVKgQn376qdYozSW1hPmhkJAQvvjiCypVquQ6SlAKDQ3l3Xff5eqrr3YdRSRfHTt2jLS0NNLT0wkL068HuTAHDhzg8OHD+t10ATQmLACo+0BE8oN+lsjF+Pe//83AgQPZunUr5cqVcx3HZ2iesAB15MgRunbtyieffOI6StBISUlh4sSJJCYmuo4iku+OF2Dx8fFa4Fsu2N13383HH3+sAuwCqAjzY5GRkSe6D8Q7Zs2axS233MKCBQtcRxHxmMGDB/Pggw9y+PBh11HEj1x++eXcddddrmP4FXVH+jl1H3hXeno6CxYs4Oqrr6ZAgQKu44h4xL59+0hMTKRChQquo4ifePHFF7nhhhto0KCB6yg+R92RAex4ATZ37lwt7u0FYWFhtGrVSgWYBLRSpUqdKMB27drlOI34ul27djF06FAtBp8HKsICwPr162ndujXvvfee6ygBLS4ujldeeUVdNBI0hgwZQo0aNVSISY7Kli3L9u3b6dOnj+sofkfXIAeAqlWrMmXKFK677jrXUQLanDlz+Pe//81DDz3kOoqIV3Tt2pUjR45wySWXuI4iPiozM5OQkBCKFi3qOopf0pgwkQtw4MABSpYs6TqGiIhPeOWVV5gxYwYzZsygcOHCruP4JCdjwowxhYwxi4wxK4wxq40xA8+yT0FjzNfGmA3GmIXGmIqeyhMMVq5cSdOmTdm0aZPrKAFLBZgEo2XLltG1a1dNzSJnKFu2LFdccYUKsDzy5JiwVKCNtbYOUBfoYIxpeto+/wAOWmurAG8Dr3kwT8ArVaoUCQkJ7Ny503WUgPPmm29y77334m8txyL5ISEhgbi4ODZs2OA6iviYu+66i08//dR1DL/lsTFhNuu3VUL2w/Ds2+m/wboCL2bfnwC8a4wxVr/p8qRcuXL8/vvvmrLCAw4ePMjevXv1dytBqUWLFmzcuJGCBQu6jiI+5JdffqFp06aEhOgav7zy6N+cMSbUGLMc2AP8ZK1deNoulwPbAKy16cBh4NKzHKePMWaJMWbJ3r17PRnZ7xljyMjI4Mcff3QdJaAMGjSISZMmuY4h4kzBggXJzMxkwoQJZGRkuI4jjv3+++80b96cDz/80HUUv+bRIsxam2GtrQtEA42NMbXyeJwR1tqG1tqGUVFR+ZoxEI0YMYIOHTqwePFi11ECglYkEMkyc+ZMbr31ViZMmOA6ijhWvXp1xo0bR/fu3V1H8WtemaLCWnvIGDMH6ACsOumpHUB5YLsxJgwoDuz3RqZA9ve//52yZcvSsOFZL8aQC9S6dWtq1KjBxx9/7DqKiFPt27dn2rRpdOzY0XUUcaxAgQIqwPKBJ6+OjDLGXJJ9vzBwPbD2tN2mAH/Pvn8LMFvjwS5eREQE3bp10/ilfGCtpUOHDjRr1sx1FBHnjDF07tyZkJAQkpKSXMcRR8aOHcvHH3+sC5XygSe7Iy8D5hhjVgKLyRoTNs0Y85Ix5sbsfT4BLjXGbAAeA57xYJ6gM378eDp06KDxGxfBGEP//v25++67XUcR8Rnr1q2jatWqGicZpMaPH88XX3yhL/r5wJNXR64E6p1l+/Mn3U8BbvVUhmCXmZlJQkIC+/bto0yZMq7j+KXVq1dTo0YNXf0jcpLKlSvTunVroqOjXUcRB7777jutVZxPNGN+ADv+2erbSt4cOnSIqKgonnvuOQYOPGOuYRGRoJOenk5YmFY8vBBOZswX94wxGGM4evQoixYtch3H7xQsWJAxY8Zwxx13uI4i4pNSUlIYMGAA+mIcHNasWUNMTAz//e9/XUcJGCpng0Dv3r35+eefiY+P12SLF6Bw4cK6+kckB6mpqXz66aeEh4frauwgkJ6eTqNGjbjyyitdRwkY6o4MAitXriQlJYXGjRu7juI3kpOT+frrr+natSslSpRwHUfEZ2lRe5GcqTsyyMXGxqoAu0Bz5szhrrvu0oS3IudxvACLj49n+/btjtOIp8TFxXHgwAHXMQKOirAgYa2lf//+DBgwwHUUv9CxY0fi4uJo1aqV6ygiPi8lJYUmTZrwr3/9y3UU8QBrLd27d+eWW25xHSXgaExYkDDG8Ndff2GMwVqrKybPwxhD/fr1XccQ8QuFChXio48+om7duq6jiAcYY5g4cSLJycmuowQcjQkLIpmZmZrvKhcWLVrEpEmTePzxx7n00jPWkxeR88jIyCA0NNR1DBGfoDFhAnCiANu6dSvx8fGO0/iuRYsWMXz4cF1JKnKBrLX07t2bhx9+2HUUySd//vknjz76KLt27XIdJSCpCAsyKSkpNGjQgKefftp1FJ/14IMPsnv3booWLeo6iohfMcZQunRpSpUqpXUFA8Rvv/3GyJEjXccIWOqODEJTp06lbt26lC9f3nUUERHxcUePHiUyMtJ1DL+l7kg5xQ033KAC7BxeeeUVunfvTmZmpusoIn5t8eLFjB492nUMuQjHjh0DUAHmQSrCgtT+/fu58847+emnn1xH8SnGGEJCQnQBg8hFeu2113jppZdIS0tzHUXywFpLgwYNeP75511HCWiaoiJIFS1alOXLl7N582bXUXzKM8884zqCSEB4//33KViwIOHh4a6jSB6kpKTQsWNH6tSp4zpKQNOYsCCmy8hPlZiYSJEiRVzHEAkomZmZbN68mSuuuMJ1FBEnNCZMzup4AbZkyRLS09Mdp3Gvbdu29OjRw3UMkYDyyCOP0KxZMw4ePOg6iuTSjh07iIuLcx0jKKgIC3ILFy6kUaNGQT+A1lrLHXfcQefOnV1HEQko99xzD6+88grFixd3HUVyadiwYTRt2pQ9e/a4jhLw1B0Z5Ky1jBw5kjvuuEPzYomICIcPH+bnn3+mU6dOrqMEBHVHyjkZY7j33nuDvgBbtGgRqamprmOIBKzp06dz3XXX6f+ZHyhevLgKMC9RESYALF++nM6dO3Po0CHXUbzu4MGDNG/enIEDB7qOIhLQ9u/fz+7du13HkHOw1vLPf/6TefPmuY4SNFSECZD1n2/FihX8+eefrqN4XZEiRZg0aRK9e/d2HUUkYHXs2JG4uDhiYmJcR5Fz2LFjB99//z2bNm1yHSVoaEyYnJCWlqY5fUTEo1JSUhgzZgz33HMPxhjXceQ0xyfX1e+C/KMxYZIr4eHhZGZmsmzZMtdRvCY5OZnhw4eri0TES8aPH0+fPn2YP3++6yhykpSUFKy1hIeHqwDzIhVhcopXXnmFxo0bB81M+v/97395+OGHWblypesoIkGhR48e/Prrr1x77bWuo8hJXnzxRerWrXtivUjxDi1bJKe46667iI6ODppxG+3atWPdunVUqlTJdRSRoBASEkLTpk0BOHDgACVLlnScSAAaNGhASEgIBQoUcB0lqGhMmIiIeN3PP/9M+/btmTx5Mm3btnUdR8RjNCZMLtj48eO5//77XcfwqF9++YUHH3xQs0KLOFC/fn169epFlSpVXEcJatZavvvuO83f5oiKMDmrTZs2sXjxYo4ePeo6isesXr2ar776Sot2izhQuHBhPvjgAypUqOA6SlBbuHAhN998M1988YXrKEFJ3ZFyVmlpaYSGhhISEth1uqblEHFr//79PP744zz55JPUrFnTdZygY61l1qxZNGvWTF9IPUTdkXLBwsPDCQkJITk5mQ0bNriO4zEqwETcyszM5Mcff2Tx4sWuowQlYwzXXXedCjBHdHWk5Khz587s27eP5cuXB1Sr2MCBA1m0aBFTpkwhNDTUdRyRoBUVFcXGjRuJiIhwHSXoDBs2jNTUVJ588klNnOuIijDJ0XPPPXeiVSyQlChRgrJly6oAE/EBxwuwxYsXU758ecqWLes4UXBYuHAhiYmJPPXUU66jBC2NCRMREef2799P+fLl6d27N++//77rOEHj2LFjmhvMw3IaE6aWMDmvjIwMXn75ZcqVK8c//vEP13Eu2v79+ylZsqSa30V8yKWXXsrEiRO5+uqrXUcJeNZaEhMTKVq0qAowxwKrj0k8IiQkhNmzZ7Nw4ULXUfJFp06duPnmm13HEJHTdOzYkeLFi5OZmanlczxo4cKFXHbZZVq/0wd4rCXMGFMeGA2UASwwwlo77LR9WgGTgeMLFX5rrX3JU5kkb4wxTJ8+nUKFCrmOctGstfzzn/8kMjLSdRQROYtjx45x/fXX07RpU1577TXXcQJSiRIl6N69O/Xr13cdJeh5sjsyHXjcWrvUGBMJxBljfrLW/nHafv+11nbxYA7JB8cLsF27dlGwYEFKlCjhOFHeGGO4++67XccQkXMoUKAAjRo1okaNGq6jBKzq1avz8ccfu44heLA70lr7l7V2afb9o8Aa4HJPnU887+DBg1SrVo1Bgwa5jpJnc+bMCehVAEQCwZAhQ+jdu7frGAHpP//5D/Hx8a5jSDavjAkzxlQE6gFnG1TUzBizwhgz3Rij6ZJ9WIkSJRgyZAj33Xef6yh5cuDAAa6//npeeeUV11FE5DystUycOJHRo0e7jhIwMjMzueuuu+jbt6/rKJLN41NUGGOKAvOAwdbab097rhiQaa1NMMZ0AoZZa6ue5Rh9gD4AMTExDVTFS15kZGTw888/Ex0dTeXKlV3HEZEcWGtp3749qampzJ07V1cz55Nt27aRkJCg7l4vymmKCo8WYcaYcGAa8KO19q1c7L8FaGit3XeufTRPmHt79uyhX79+PPbYY/qPLCIes2/fPi655BLCwjSbkvgvJ2tHmqyvLZ8Aa85VgBljymbvhzGmcXae/Z7KJPnDGMOkSZNYtGiR6yi5lpSUxODBg9m6davrKCKSS6VKlSIsLIyUlBRWr17tOo5fi4uLo2fPnuzYscN1FDmJJ79eNAd6Ab8bY5Znb3sOiAGw1n4I3ALcb4xJB5KB7tbfpvAPQlFRUcTHx/vVWm8LFy6kf//+NG3alJiYGNdxROQC3HHHHcTFxbF+/XoKFizoOo5fWrduHXPmzKFo0aKuo8hJtGyRXJQNGzZQpUoV1zFyZefOnURFRREeHu46iohcgEWLFnHkyBGuu+4611H8Wnp6urp2HXDSHSmBb8aMGVSrVo2ffvrJdZRcKVeunAowET/UuHHjEwWYvzUc+IJDhw4BqADzQSrCJM9at27NSy+9RIMGDVxHydGCBQv4+9//zl9//eU6iohchJEjR3LttdeSnp7uOorfSEtLo3bt2jz99NOuo8hZqCyWPCtYsCD9+/d3HeO8Nm3axKxZsyhevLjrKCJyEYoXL07x4sU5evSo367a4W3p6ek89NBDPv9lOVhpTJhctOXLlzN8+HA++ugjn23uzszMJCREDb8i/uz47yvNGSb+RGPCxKPi4+OZMmUKa9eudR3lDMd/aKsAE/F/xhiMMezdu5e3335b48PO488//+THH38kMzPTdRQ5B/1mkot24403snnzZmrVquU6yhn69+9Py5YtycjIcB1FRPLJl19+ydNPP+2TX/x8yXvvvcfNN9/M4cOHXUeRc1ARJhfNGEPRokWx1rJlyxbXcU5RqVIl6tSpQ2hoqOsoIpJPHnjgAX7//Xet2HEer7/+OrNnz9b4OR+mMWGSb55++mlGjhzJpk2bNAheRLxi27ZtlC9f3nUMkXPSmDDxijvvvJPBgwf7zEz6O3bsUDekSACbOHEilStXZuHCha6j+JT09HRuuOEGZsyY4TqKnIdvXsomfqlOnTrUqVPHdYwTunXrxqWXXsr06dNdRxERD2jXrh1PPvmkuiVPs2PHDrZs2UJSUpLrKHIe6o6UfDdp0iTWrVvnfHLA8ePHU7BgQW688UanOUREvM1ai7VWV4b7AHVHilf9+OOPjBs3jrS0NKc5br31VhVgIkFg06ZNdOjQgU2bNrmO4tzBgwdJS0vDGKMCzA/oE5J89/rrr7NkyRKn6zR+//337N2719n5RcR7ChQowNq1a1m/fr3rKM49++yz1KhRQ0s7+QkVYZLvIiMjCQsL49ixY+zZs8fr5z9w4ABdu3Zl6NChXj+3iHhfdHQ0GzZsoH379q6jONetWzceeughn129RE6lMWHiEdZaGjduTJkyZZg2bZrXz71ixQpKlixJTEyMV88tIu5Ya5k0aRKtWrXS3FjiMzQmTLzOGMPDDz/MAw884OTcdevWVQEmEmQ2btzIrbfeyvDhw11H8bqMjAw+/PBDDh065DqKXAAVYeIxvXr1omPHjl49Z2JiIs8++ywbN2706nlFxL0qVaowa9YsnnvuOddRvO6XX37h/vvvZ+bMma6jyAVQESYedezYMd566y2v/WBYunQpQ4YMIT4+3ivnExHfcu211xIWFkZycjLHjh1zHcdrWrRowbJly+jWrZvrKHIBVISJRxlj+OCDD5g6dapXzteiRQv27NlDy5YtvXI+EfE9Bw8eJDY2ltdff911FK+qW7eu06vS5cLp8gnxqPDwcH777TcuvfRSr51TA3JFgluJEiW46aabaNasmesoXtG/f38yMzN5+eWXXUeRC6SWMPG44wXYgQMHPNo9MHfuXLp168b27ds9dg4R8Q9vvPEGbdu2dR3DK3bv3s3u3btdx5A8UBEmXhEfH0/lypUZMWKEx86xZ88e/vjjD6+2uomI78rIyODNN99k3LhxrqN41Mcff8zIkSNdx5A8UBEmXhETE8NDDz1Eq1atPHaO2267jXXr1lG4cGGPnUNE/IcxhokTJwbsFYOZmZn89ddfQNZ7Ff+jyVolIKSnp2uGaBE5w5EjR4iMjAzIImXGjBnccMMNzJ49mxYtWriOI+egyVrFZ+zdu5fHH3+cffv25etx+/fvT926dbVemoicolixYhhj2LNnDytXrnQdJ19dddVVPPXUUzRp0sR1FMkjNR2IV+3Zs4d3332XZs2accstt+TbcWNjY8nMzFRrmIicwVrLDTfcQFJSEitXrgyYVrGYmBgGDx7sOoZcBHVHitft2bOH0qVLu44hIkHkt99+o1ixYlx11VWuo+SLr7/+murVq1O3bl3XUeQ81B0pPuV4AZZfXZKbNm0iJSUlX44lIoGpadOmJwowfx+2kJ6ezmOPPRZ0k9EGIhVh4sT48eOJjo5m7dq1F32s2267jU6dOuVDKhEJdM8//zzXX389mZmZrqPkWVhYGKtWreK1115zHUUukgbQiBOtWrWib9++lCxZ8qKPNWjQoIAZ4yEinlW5cmUOHjzIsWPHKFSokOs4eVaiRAmtDhIANCZMRETET8yePZvXX3+dkSNHEh0d7TqO5ILGhInPWrVqFS+88EKeX//NN9+wdevWfEwkIsFgzZo1vPrqq65jXLD9+/eza9cuSpUq5TqK5AMVYeLUrFmzGDp0aJ4KqUOHDnHnnXd6dCkkEQlM33zzDW+88Ybfrbl46623smzZMr/uSpX/UXekOJWamkpCQkKe13vcuHEjhQsXply5cvmcTEQCWWpqKocPH/ar6XK2bNlChQoVNAbWz6g7UnxWwYIFTxRgBw8evODXX3HFFSrAROSCFSxYkNKlS2Ot5Y8//nAd57ySkpKoX78+jz/+uOsoko9UhIlPePDBB2natGmu5+9JTEzkgQce8IsfniLiu959913q1Knj8z9LQkNDGTJkCHfeeafrKJKPPDZFhTGmPDAaKANYYIS1dthp+xhgGNAJSAJ6W2uXeiqT+K4uXbpQuXLlXM/d8/vvvzNq1ChuvfXWgJkBW0S878477yQzM5Nq1aq5jpKjggULcvfdd7uOIfnMY2PCjDGXAZdZa5caYyKBOOAma+0fJ+3TCXiIrCKsCTDMWpvjSqQaEybHJScnU6BAAUJDQ11HEZEAYK31yfFWcXFxrF27lttuu43w8HDXceQCORkTZq3963irlrX2KLAGuPy03boCo22W34BLsos3CVIzZszgyy+/zNW+hQsXVgEmIvli8eLFNGrUiJ07d7qOcoZRo0bx0EMPkZaW5jqK5DOvjAkzxlQE6gELT3vqcmDbSY+3c2ahhjGmjzFmiTFmyd69ez2WU9wbNmwYb7/9Njm10M6aNYvrrruO+Ph4LyYTkUBWokQJUlNT2bVrl+soZxg2bBgLFy4kIiLCdRTJZx5ftsgYUxSYCDxqrT2Sl2NYa0cAIyCrOzIf44mP+eyzzyhZsmSOXQIJCQkcPHjQry4tFxHfVqVKFVauXOmT3ZEhISFUrVrVdQzxAI+2hBljwskqwMZaa789yy47gPInPY7O3iZBqmzZshQoUID09HQSEhLOuk/Xrl2Ji4ujcOHCXk4nIoHMGEN6ejrvv//+OX/+eFNKSgqNGjVi8uTJrqOIh3isCMu+8vETYI219q1z7DYF+JvJ0hQ4bK39y1OZxD+kpaXRoEEDnn766TOeS05OzrGrUkTkYixbtowHHniAr776ynUUdu/eTdGiRYmMjHQdRTzEk92RzYFewO/GmOXZ254DYgCstR8CP5B1ZeQGsqaouMuDecRPhIeHc/vtt1OjRo0znnvxxRf59ttvue2224iKiuLRRx8FoF+/fpQuXZrt27czffp0jDH079+f22+/nblz5zJkyBCmTZsGZM1J1rBhQ3r37u3FdyUi/qBRo0bExcVRr14911GoUKECc+bMcR1DPMhjRZi1dgGQY+e6zWrSeMBTGcR/Pffcc2fd3qxZMwoXLkzv3r25+eabefTRR8nMzOSrr77i9ddfZ9q0aaxYsYJ9+/bRqFEjWrZs6eXkIuLv6tevD8DevXspWbKkk6uwd+zYQbFixdQKFuA0Y774rLS0ND788ENWrlx5YttNN93Eiy++SMWKFbn00ktZtmwZM2fOpF69eixYsIA77riD0NBQypQpw7XXXsvixYsdvgMR8Vfx8fFUr16dd99918n5n3jiCWrVqkVGRoaT84t3qAgTn5WQkMCzzz57Yt6wP/74gyNH/neB7T333MOoUaP47LPPcpxJOiws7JSZ+FNSUjwXWkQCQkxMDPfddx/t2rVzcv5//etfvPLKK5oLMcB5bMZ8T9GM+cFl8+bNVKxYEWMMDRo0oEiRIsyfPx+AY8eOUbt2bdLS0li/fj2TJ0/mo48+4ocffuDAgQM0bNiQhQsXkpaWRosWLVi3bh3JycnUq1ePF154QWPCRETE45zMmC+SHypVqsSXXxpiYhJYuvQd/vxzIGPHZj1XoEABWrduzW233UZoaCjdunUjNjaWOnXq0KZNG15//XXKli1L+fLlue2226hVqxa33XabTwy4FRH/kJSUxEMPPcR3333nlfOlpqYycOBAn5y5X/KfWsLEp40dC/fcs4qUlNbAZ0AXIiJgxAi4445M6tevz/jx4zWRoYh4RFpaGk2bNuWmm25iwIABHj/f3Llzadu2LT/88APt27f3+PnE83JqCVMRJj6tYkWIj08D+gCPAHUBuOyyPyhUqAvdunXjzTffdBdQRAJeamoqBQsW9Nr5tm7dSnR0NCEh6qwKBDkVYR5ftkjkYmzdChBOVivY/+zadRWZmZtcRBKRIHO8AFuzZg1paWnExsZ65DzWWowxxMTEeOT44ntUZotPO9fPIv2MEhFvysjIoHPnzicmiPaEPn368NBDD3ns+OJ7VISJTxs8GCIiTt0WEZG1XUTEW0JDQ/nqq688upxRsWLFKFq0qMeOL75HY8LE540dC/36ZXVNxsRkFWA9erhOJSLBylpLcnIyEad/QxQ5C01RIX6tRw/YsgUyM7P+VAEmIi717NmTW265hfxqxEhLS2PNmjX5cizxLyrCRERELkDLli1p27ZtvhVh3377LVdddRW//PJLvhxP/IeujhQREbkA//znP/P1eG3atGHYsGE0bdo0X48rvk8tYSIiInkwa9YsBg0adNHHiYqK4uGHH9a8YEFIn7iIiEge/PDDD4wZM4bExMQ8H+O9995j7ty5+RdK/IqujhQREcmDpKQkjDEULlw4T69PS0ujSpUqdOrUiQ8++CCf04mv0Iz5IiIi+ez4FBXp6enExcXRpEmTC3p9eHg469atIykpyRPxxA+oO1JEROQi9OvXj2uvvZYdO3bk+jXWWqy1FCpUiJIlS3ownfgyFWEiIiIX4ZFHHmHs2LGUK1cu16/57rvvaNSoEdu3b/dgMvF1KsJEREQuQrly5fi///s/jDFkZGTk6jVhYWGUKlWKyy67zMPpxJepCBMREckH33//PTVr1mT//v3n3ffGG29kxowZhIaGeiGZ+CoVYSIiIvmgfPnylCtXjqNHj+a439KlS0lPT/dSKvFlKsJERETyQWxsLLNnz6ZixYrn3OfAgQNcc801PPXUU94LJj5LRZiIiEg+OnLkCAMHDiQ1NfWM54oXL85XX31F586dufLKK+nduzfVqlWjR48e/Oc//6F58+ZUrVqVRYsWkZiYyN13303jxo2pV68ekydPdvBuxJM0WauIiEg++umnn+jQoQPTpk2jY8eOZ91ny5YtVKlShWXLllGzZk0aNWpEnTp1+OSTT5gyZQqfffYZV111FVdddRU9e/bk0KFDNG7cmGXLllGkSBEvvyO5GJqsVURExEuuv/56/vzzT6644opTts+ZM4eVK1dy3333AVCpUiVq164NQM2aNWnbti3GGGrXrs2WLVvYvn07U6ZMYciQIQCkpKSwdetWatSo4d03JB6jIkxERCSfHS/ANm7cSOXKlTHGMHnyZCZOnMgDDzwAQMGCBU/sHxIScuJxSEgI6enphIaGMnHiRKpXr+79NyBeoTFhIiIiHhAXF0eNGjX4/PPPARg6dChLly4lLCx37R/t27dn+PDhHB82tGzZMo9lFTdUhImIiHhAvXr16Nr1eQYM6IwxmVSsCDNnRuX69QMGDCAtLY3Y2Fhq1qzJgAEDPBdWnNDAfBEREQ8YOxb69IGkpMNALDCEiIhbGTECevRwnU68JaeB+WoJExER8YB+/SApCeAAUBS4gqSkrO0ioIH5IiIiHrF16/F7FYBvgeqnbZdgp5YwERERD4iJOX4vhOMF2KnbJdipCBMREfGAwYMhIuLUbRERWdtFQEWYiIiIR/ToASNGQIUKYEzWnxqULyfzWBFmjPnUGLPHGLPqHM+3MsYcNsYsz74976ksIiIiLvToAVu2QGZm1p8qwORknhyYPwp4Fxidwz7/tdZ28WAGEREREZ/ksZYwa+18sq7LFREREZHTuB4T1swYs8IYM90YU/NcOxlj+hhjlhhjluzdu9eb+UREREQ8wmURthSoYK2tAwwHJp1rR2vtCGttQ2ttw6io3C/5ICIiIuKrnBVh1toj1tqE7Ps/AOHGmFKu8oiIiIh4k7MizBhT1hhjsu83zs6y31UeEREREW/y2NWRxphxQCuglDFmO/ACEA5grf0QuAW43xiTDiQD3a2/rSYuIiIikkceK8KstXec5/l3yZrCQkRERCTouL46UkRERCQoqQgTERERcUBFmIiIiIgDKsJEREREHDD+dkGiMWYvEO+FU5UC9nnhPJJ7+kx8jz4T36TPxffoM/FN3vhcKlhrzzrTvN8VYd5ijFlirW3oOof8jz4T36PPxDfpc/E9+kx8k+vPRd2RIiIiIg6oCBMRERFxQEXYuY1wHUDOoM/E9+gz8U36XHyPPhPf5PRz0ZgwEREREQfUEiYiIiLigIowEREREQdUhJ3GGNPBGLPOGLPBGPOM6zwCxphPjTF7jDGrXGeRLMaY8saYOcaYP4wxq40xj7jOJGCMKWSMWWSMWZH9uQx0nUmyGGNCjTHLjDHTXGcRMMZsMcb8boxZboxZ4iyHxoT9jzEmFPgTuB7YDiwG7rDW/uE0WJAzxrQEEoDR1tparvMIGGMuAy6z1i41xkQCccBN+r/iljHGAEWstQnGmHBgAfCItfY3x9GCnjHmMaAhUMxa28V1nmBnjNkCNLTWOp1AVy1hp2oMbLDWbrLWHgO+Aro6zhT0rLXzgQOuc8j/WGv/stYuzb5/FFgDXO42ldgsCdkPw7Nv+qbtmDEmGugMjHSdRXyLirBTXQ5sO+nxdvSLRSRHxpiKQD1goeMowolur+XAHuAna60+F/eGAk8BmY5zyP9YYKYxJs4Y08dVCBVhIpJnxpiiwETgUWvtEdd5BKy1GdbaukA00NgYoy58h4wxXYA91to411nkFNdYa+sDHYEHsoe9eJ2KsFPtAMqf9Dg6e5uInCZ7zNFEYKy19lvXeeRU1tpDwBygg+Mowa45cGP2GKSvgDbGmC/cRhJr7Y7sP/cA35E1HMnrVISdajFQ1RhTyRhTAOgOTHGcScTnZA8A/wRYY619y3UeyWKMiTLGXJJ9vzBZFxmtdRoqyFlrn7XWRltrK5L1O2W2tban41hBzRhTJPuCIowxRYB2gJOr71WEncRamw48CPxI1kDjb6y1q92mEmPMOOBXoLoxZrsx5h+uMwnNgV5kfatfnn3r5DqUcBkwxxizkqwvlT9ZazUlgsipygALjDErgEXA99baGS6CaIoKEREREQfUEiYiIiLigIowEREREQdUhImIiIg4oCJMRERExAEVYSIiIiIOqAgTEWeMMZeeNMXFLmPMjpMeFzht3y3GmFL5fP65xph1xpgVxpifjTHV83CMH4wxl2Tf+p60vZwxZkJ+5hWRwKIpKkTEJxhjXgQSrLVDzvH8FqChtXZfPp5zLvCEtXZJ9vpxXay1N+bxWBWBadZaLRMkIrmiljAR8SnGmLbGmGXGmN+NMZ8aYwqe9nxhY8x0Y8y92TNff2qMWZT9mq7Z+/Q2xnxrjJlhjFlvjHk9F6eeD1QxWd4wxqzKznB79jEvM8bMz26lW2WMaZG9/XgL3avAFdnPv2GMqWiMWZW9TyFjzGfZx1tmjGl9ETlFJECEuQ4gInKSQsAooK219k9jzGjgfmBo9vNFyVp/b7S1drQx5mWyloG5O3u5nkXGmP9k71sXqAekAuuMMcOttdtyOPcNwO/AzdmvrQOUAhYbY+YDdwI/WmsHG2NCgYjTXv8MUCt78ezjLWPHPQBYa21tY8yVwExjTLU85hSRAKGWMBHxJaHAZmvtn9mPPwdanvT8ZOAza+3o7MftgGeMMcuBuWQVcTHZz82y1h621qYAfwAVznHOsdmvbw48AVwDjLPWZlhrdwPzgEZkLQN0V3a3aW1r7dELeF/XAF8AWGvXAvHA8SIstzlFJMCoCBMRf/Iz0CF7AXEAA/yftbZu9i3GWrsm+7nUk16Xwblb/ntkv/amnFqgrLXzySoIdwCjjDF/u7i3ckJuc4pIgFERJiK+JAOoaIypkv24F1ktUcc9DxwE3st+/CPw0PGizBhTLx8y/Be43RgTaoyJIqvwWmSMqQDsttZ+DIwE6p/2uqNAZA7H7JGdsRpZrXXr8iGriPgxFWEi4ktSgLuA8caY34FM4MPT9nkEKJw9iP3fQDiw0hizOvvxxfoOWAmsAGYDT1lrdwGtgBXGmGXA7cCwk19krd0P/Jw9aP+N0475PhCS/Z6+Bnpba1MRkaCmKSpEREREHFBLmIiIiIgDKsJEREREHFARJiIiIuKAijARERERB1SEiYiIiDigIkxERETEARVhIiIiIg78P/7q5bOmH86TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = viz_attns\n",
    "x = np.arange(len(tokens))\n",
    "labels = tokens\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x, y, 'k:')\n",
    "ax.set_ylabel(\"Attention\")\n",
    "ax.set_xlabel(\"Token Position\")\n",
    "\n",
    "for i, txt in enumerate(labels):\n",
    "    ax.annotate(txt, (x[i], y[i]), xytext=(x[i] + 0.03, y[i] + 0.03))\n",
    "#plt.savefig(f'Attention_humor_and_politeness_plot_{\"_\".join(sent.split())}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model = False\n",
    "# checkpoint_path = \"../models/BERT_Joint_EarlyTest\"\n",
    "# if save_model:\n",
    "#     model.save_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salience(sent, model, tokenizer, device, \n",
    "             batch_size, task_num, label_num):\n",
    "    '''\n",
    "    Runs the model on an input sentence and gives the salience for each token that \n",
    "    is a representation of how much a given token is contributing to the score of a \n",
    "    given class for a given classification task.\n",
    "    \n",
    "    Arguments: \n",
    "    \n",
    "      sent       : str. \n",
    "                   The input sentence.\n",
    "      model      : the PyTorch sequence classifier model.  \n",
    "                   Assumed to be a JointSeqClassifier or \n",
    "                   DistilBertForSequenceClassification model\n",
    "      tokenizer  : Transformers tokenizer.\n",
    "                   The tokenizer used to encode the input string. \n",
    "                   Assumed to be compatible with model.\n",
    "      device     : torch.device. \n",
    "                   The device to be used (cuda or cpu)\n",
    "      batch_size : int. \n",
    "                   The batch_size used by the model.\n",
    "      task_num   : int. \n",
    "                   Index for the classification task we are interested in.\n",
    "      label_num  : int.\n",
    "                   Index for the class label for the task corresponding to task_num.\n",
    "                   For example if task 0 is a politeness classificaition \n",
    "                   task and class 1 is \"polite\", then setting task_num = 0 and \n",
    "                   label_num = 1 would give us a representation of how much each \n",
    "                   token is contributing to labelling the sentence as \"polite\".\n",
    "    Returns:\n",
    "    \n",
    "      saliency : Tensor of floats with length equal to length of tokenized sent.\n",
    "                 The gradient of the particular output we are interested in (the score for \n",
    "                 class 'label_num' for task 'task_num') wrt the input tokens.\n",
    "    '''\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # Tokenize input\n",
    "    input_tensor = tokenizer.encode(sent, return_tensors=\"pt\").to(device)\n",
    "    # Run the model and get the output\n",
    "    output = model(input_tensor, output_attentions=True, output_hidden_states=True, \n",
    "                   task_ids=range(len(model.classifier)))\n",
    "    # Get the score for the desired task,class combination\n",
    "    class_score = output.logits[task_num][0][label_num]\n",
    "    # backprop\n",
    "    class_score.backward()\n",
    "    # get the gradients\n",
    "    grads = [h_state.grad for h_state in output[2]]\n",
    "    # Take the max over the heads and the embedding dim. \n",
    "    # TODO, consider a better way (perhaps use sum instead or max,sum combo)\n",
    "    saliency = torch.stack(grads).abs().max(axis=0)[0].squeeze().max(axis=1)[0]\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Could you please help me?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = tokenizer.encode(sent, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_tensor, output_attentions=True, output_hidden_states=True,\n",
    "               task_ids=range(len(model.classifier)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0291, 0.0080, 0.0049, 0.0132, 0.0154, 0.0035, 0.0044, 0.0177],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salience(sent, model, tokenizer, device, batch_size, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_marvin)",
   "language": "python",
   "name": "env_marvin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
